+---------------------------------------------------------------------+
|  Log file: pa.log                                                   |
|  Compiler version: 5.8.0 (7069cde)                                  |
|  Created on: Sun Nov 17 06:21:48 2019                               |
|  Run ID: 5264f50f8fd548e5                                           |
+---------------------------------------------------------------------+

HLIR Version: 0.10.22
PHV container sizes are: [8, 16, 32]
Parser state extraction bandwidth: 224
  8-bit: 4 extracts
  16-bit: 4 extracts
  32-bit: 4 extracts
Free containers to start for 8 bits:
  Group 4 8 bits has 16 available
  Group 5 8 bits has 16 available
  Group 6 8 bits has 16 available
  Group 7 8 bits has 16 available
  Group 16 8 bits (tagalong) has 16 available
  Group 17 8 bits (tagalong) has 16 available
Free containers to start for 16 bits:
  Group 8 16 bits has 16 available
  Group 9 16 bits has 16 available
  Group 10 16 bits has 16 available
  Group 11 16 bits has 16 available
  Group 12 16 bits has 16 available
  Group 13 16 bits has 16 available
  Group 18 16 bits (tagalong) has 16 available
  Group 19 16 bits (tagalong) has 16 available
  Group 20 16 bits (tagalong) has 16 available
Free containers to start for 32 bits:
  Group 0 32 bits has 16 available
  Group 1 32 bits has 16 available
  Group 2 32 bits has 16 available
  Group 3 32 bits has 16 available
  Group 14 32 bits (tagalong) has 16 available
  Group 15 32 bits (tagalong) has 16 available


Initializing PHV allocation...
Adding: InfoOnly Constraint: ig_intr_md_for_mb.ingress_mirror_id <10 bits ingress imeta W>
Added constraint: SolitaryMirror Constraint: eg_intr_md_for_oport._pad1 <2 bits egress imeta>
Added constraint: SolitaryMirror Constraint: eg_intr_md_for_oport.capture_tstamp_on_tx <1 bits egress imeta W>
Added constraint: SolitaryMirror Constraint: eg_intr_md_for_oport.update_delay_on_tx <1 bits egress imeta>
Added constraint: SolitaryMirror Constraint: eg_intr_md_for_oport.force_tx_error <1 bits egress imeta>
Added constraint: SolitaryMirror Constraint: eg_intr_md_for_oport.drop_ctl <3 bits egress imeta W>

-----------------------------------------------
   User added PHV constraints
-----------------------------------------------
User indicated that ingress header erspan_t3_header should never be deparsed.
User indicated that ingress header erspan_t3_header will never be parsed.
User indicated that egress header erspan_t3_header will never be parsed.
User added constraint MaxFieldSplit Constraint: ig_intr_md_from_parser_aux.ingress_parser_err <16 bits ingress parsed imeta R> -- max split: 1
User added constraint MaxFieldSplit Constraint: ig_intr_md_for_tm.ucast_egress_port <9 bits ingress imeta W> -- max split: 1
User added constraint MaxFieldSplit Constraint: ig_intr_md_for_tm.level2_mcast_hash <13 bits ingress imeta W> -- max split: 1
User added constraint MaxFieldSplit Constraint: ig_intr_md_for_tm.level2_exclusion_id <9 bits ingress imeta W> -- max split: 1
User added constraint MaxFieldSplit Constraint: ig_intr_md_for_tm.mcast_grp_b <16 bits ingress imeta W> -- max split: 1
User added constraint MaxFieldSplit Constraint: ig_intr_md_for_tm.level1_exclusion_id <16 bits ingress imeta W> -- max split: 1
User added constraint MaxFieldSplit Constraint: ig_intr_md_for_tm.rid <16 bits ingress imeta W> -- max split: 1
User added constraint MaxFieldSplit Constraint: ig_intr_md_for_tm.level1_mcast_hash <13 bits ingress imeta W> -- max split: 1
User added constraint MaxFieldSplit Constraint: ig_intr_md_for_tm.mcast_grp_a <16 bits ingress imeta> -- max split: 1
User added constraint MaxFieldSplit Constraint: ig_intr_md_for_mb.ingress_mirror_id <10 bits ingress imeta W> -- max split: 1
User added constraint MaxFieldSplit Constraint: eg_intr_md.egress_port <9 bits egress parsed imeta R> -- max split: 1
User added constraint MaxFieldSplit Constraint: eg_intr_md_from_parser_aux.egress_parser_err <16 bits egress parsed imeta> -- max split: 1
User added constraint MaxFieldSplit Constraint: eg_intr_md_for_mb.egress_mirror_id <10 bits egress imeta W> -- max split: 1
User indicated that field ipv4_option_32b.option_fields (ingress) should not be overlayed.
User indicated that field ipv4_option_32b.option_fields (egress) should not be overlayed.
User added constraint MaxFieldSplit Constraint: egress_metadata.port_type <2 bits egress meta R W> -- max split: 1
User added constraint Solitary Constraint: egress_metadata.port_type <2 bits egress meta R W>
User added constraint ContainerSize Constraint: l2_metadata.same_if_check <14 bits ingress meta R W> -- container size: 16 -- field bit: 0
User indicated that ingress fields ipv4_metadata.lkp_ipv4_sa and ipv6_metadata.lkp_ipv6_sa are mutually exclusive.
User indicated that ingress fields ipv4_metadata.lkp_ipv4_da and ipv6_metadata.lkp_ipv6_da are mutually exclusive.
User added constraint Solitary Constraint: acl_metadata.port_lag_label <16 bits ingress meta R W>
User added constraint MaxFieldSplit Constraint: acl_metadata.port_lag_label <16 bits ingress meta R W> -- max split: 1
User added constraint MaxFieldSplit Constraint: multicast_metadata.multicast_bridge_mc_index <16 bits ingress meta R W> -- max split: 1
User added constraint MaxFieldSplit Constraint: multicast_metadata.multicast_route_mc_index <16 bits ingress meta R W> -- max split: 1
User added constraint Solitary Constraint: multicast_metadata.multicast_bridge_mc_index <16 bits ingress meta R W>
User added constraint Solitary Constraint: multicast_metadata.multicast_route_mc_index <16 bits ingress meta R W>
User added constraint Solitary Constraint: fabric_metadata.reason_code <16 bits ingress meta R W>
User added constraint MaxFieldSplit Constraint: hash_metadata.hash1 <16 bits ingress meta R W> -- max split: 1
User added constraint Solitary Constraint: hash_metadata.hash1 <16 bits ingress meta R W>
User added constraint MaxFieldSplit Constraint: hash_metadata.hash2 <16 bits ingress meta R W> -- max split: 1
User added constraint Solitary Constraint: hash_metadata.hash2 <16 bits ingress meta R W>

-----------------------------------------------
  Scanning for field list calculations
-----------------------------------------------
Adding: Solitary Constraint: ipv4.hdrChecksum <16 bits egress parsed R W>
Adding: ContainerAlignment Constraint: ipv4.diffserv <8 bits ingress parsed R> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.diffserv <8 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.totalLen <16 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.totalLen <16 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.identification <16 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.identification <16 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.fragOffset <13 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.fragOffset <13 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.protocol <8 bits ingress parsed R> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.protocol <8 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.srcAddr <32 bits ingress parsed R> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.srcAddr <32 bits egress parsed R W> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.srcAddr <32 bits ingress parsed R> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.srcAddr <32 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.dstAddr <32 bits ingress parsed R> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.dstAddr <32 bits egress parsed R W> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.dstAddr <32 bits ingress parsed R> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.dstAddr <32 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4_option_32b.option_fields <32 bits ingress parsed tagalong> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4_option_32b.option_fields <32 bits egress parsed tagalong> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4_option_32b.option_fields <32 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4_option_32b.option_fields <32 bits egress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: Solitary Constraint: inner_ipv4.hdrChecksum <16 bits egress parsed R W>
Adding: ContainerAlignment Constraint: inner_ipv4.diffserv <8 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.diffserv <8 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.totalLen <16 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.totalLen <16 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.identification <16 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.identification <16 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.fragOffset <13 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.fragOffset <13 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.protocol <8 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.protocol <8 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.srcAddr <32 bits ingress parsed tagalong> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.srcAddr <32 bits egress parsed R W> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.srcAddr <32 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.srcAddr <32 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.dstAddr <32 bits ingress parsed tagalong> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.dstAddr <32 bits egress parsed R W> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.dstAddr <32 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.dstAddr <32 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]

-----------------------------------------------
   Eliminating unused metadata (184 instances)
-----------------------------------------------
Removing standard_metadata.ingress_port in ingress
Removing standard_metadata.packet_length in ingress
Removing standard_metadata.egress_spec in ingress
Removing standard_metadata.egress_port in ingress
Removing standard_metadata.egress_instance in ingress
Removing standard_metadata.instance_type in ingress
Removing standard_metadata.clone_spec in ingress
Removing standard_metadata.valid in ingress
Removing ig_prsr_ctrl.priority in ingress
Removing ig_prsr_ctrl.priority (ingress) from parse_set_prio_med's set_metadata.
Removing ig_prsr_ctrl.priority (ingress) from parse_set_prio_high's set_metadata.
Removing ig_prsr_ctrl._pad1 in ingress
Removing ig_prsr_ctrl.parser_counter in ingress
Removing ig_prsr_ctrl.valid in ingress
Removing ig_intr_md.valid in ingress
Removing ig_intr_md_from_parser_aux.ingress_global_tstamp in ingress
Removing ig_intr_md_from_parser_aux.ingress_global_ver in ingress
Removing ig_intr_md_from_parser_aux.valid in ingress
Removing ig_intr_md_for_tm._pad1 in ingress
Removing ig_intr_md_for_tm.bypass_egress in ingress
Removing ig_intr_md_for_tm.deflect_on_drop in ingress
Removing ig_intr_md_for_tm.icos_for_copy_to_cpu in ingress
Removing ig_intr_md_for_tm._pad2 in ingress
Removing ig_intr_md_for_tm.enable_mcast_cutthru in ingress
Removing ig_intr_md_for_tm.mcast_grp_a in ingress
Removing ig_intr_md_for_tm._pad3 in ingress
Removing ig_intr_md_for_tm._pad4 in ingress
Removing ig_intr_md_for_tm._pad5 in ingress
Removing ig_intr_md_for_tm.valid in ingress
Removing ig_intr_md_for_mb._pad1 in ingress
Removing ig_intr_md_for_mb.valid in ingress
Removing eg_intr_md._pad0 in ingress
Removing eg_intr_md.egress_port in ingress
Removing eg_intr_md._pad1 in ingress
Removing eg_intr_md.enq_qdepth in ingress
Removing eg_intr_md._pad2 in ingress
Removing eg_intr_md.enq_congest_stat in ingress
Removing eg_intr_md.enq_tstamp in ingress
Removing eg_intr_md._pad3 in ingress
Removing eg_intr_md.deq_qdepth in ingress
Removing eg_intr_md._pad4 in ingress
Removing eg_intr_md.deq_congest_stat in ingress
Removing eg_intr_md.app_pool_congest_stat in ingress
Removing eg_intr_md.deq_timedelta in ingress
Removing eg_intr_md.egress_rid in ingress
Removing eg_intr_md._pad5 in ingress
Removing eg_intr_md.egress_rid_first in ingress
Removing eg_intr_md._pad6 in ingress
Removing eg_intr_md.egress_qid in ingress
Removing eg_intr_md._pad7 in ingress
Removing eg_intr_md.egress_cos in ingress
Removing eg_intr_md._pad8 in ingress
Removing eg_intr_md.deflection_flag in ingress
Removing eg_intr_md.pkt_length in ingress
Removing eg_intr_md.valid in ingress
Removing eg_intr_md_from_parser_aux.egress_global_tstamp in ingress
Removing eg_intr_md_from_parser_aux.egress_global_ver in ingress
Removing eg_intr_md_from_parser_aux.egress_parser_err in ingress
Removing eg_intr_md_from_parser_aux.clone_digest_id in ingress
Removing eg_intr_md_from_parser_aux.clone_src in ingress
Removing eg_intr_md_from_parser_aux.coalesce_sample_count in ingress
Removing eg_intr_md_from_parser_aux.valid in ingress
Removing eg_intr_md_for_mb._pad1 in ingress
Removing eg_intr_md_for_mb.egress_mirror_id in ingress
Removing eg_intr_md_for_mb.coalesce_flush in ingress
Removing eg_intr_md_for_mb.coalesce_length in ingress
Removing eg_intr_md_for_mb.valid in ingress
Removing eg_intr_md_for_oport._pad1 in ingress
Removing eg_intr_md_for_oport.capture_tstamp_on_tx in ingress
Removing eg_intr_md_for_oport.update_delay_on_tx in ingress
Removing eg_intr_md_for_oport.force_tx_error in ingress
Removing eg_intr_md_for_oport.drop_ctl in ingress
Removing eg_intr_md_for_oport.valid in ingress
Removing ingress_metadata.outer_bd in ingress
Removing ingress_metadata.control_frame in ingress
Removing ingress_metadata.valid in ingress
Removing egress_metadata.port_type in ingress
Removing egress_metadata.payload_length in ingress
Removing egress_metadata.smac_idx in ingress
Removing egress_metadata.bd in ingress
Removing egress_metadata.outer_bd in ingress
Removing egress_metadata.mac_da in ingress
Removing egress_metadata.routed in ingress
Removing egress_metadata.same_bd_check in ingress
Removing egress_metadata.drop_reason in ingress
Removing egress_metadata.ifindex in ingress
Removing egress_metadata.egress_port in ingress
Removing egress_metadata.valid in ingress
Removing intrinsic_metadata.mcast_grp in ingress
Removing intrinsic_metadata.lf_field_list in ingress
Removing intrinsic_metadata.egress_rid in ingress
Removing intrinsic_metadata.ingress_global_timestamp in ingress
Removing intrinsic_metadata.valid in ingress
Removing global_config_metadata.enable_dod in ingress
Removing global_config_metadata.valid in ingress
Removing l2_metadata.valid in ingress
Removing l3_metadata.fib_partition_index in ingress
Removing l3_metadata.outer_routed in ingress
Removing l3_metadata.mtu_index in ingress
Removing l3_metadata.l3_mtu_check in ingress
Removing l3_metadata.egress_l4_sport in ingress
Removing l3_metadata.egress_l4_dport in ingress
Removing l3_metadata.valid in ingress
Removing ipv4_metadata.valid in ingress
Removing ipv6_metadata.valid in ingress
Removing tunnel_metadata.mpls_enabled in ingress
Removing tunnel_metadata.mpls_ttl in ingress
Removing tunnel_metadata.mpls_in_udp in ingress
Removing tunnel_metadata.egress_tunnel_type in ingress
Removing tunnel_metadata.tunnel_index in ingress
Removing tunnel_metadata.tunnel_src_index in ingress
Removing tunnel_metadata.tunnel_smac_index in ingress
Removing tunnel_metadata.tunnel_dmac_index in ingress
Removing tunnel_metadata.vnid in ingress
Removing tunnel_metadata.egress_header_count in ingress
Removing tunnel_metadata.inner_ip_proto in ingress
Removing tunnel_metadata.valid in ingress
Removing acl_metadata.egress_acl_deny in ingress
Removing acl_metadata.mirror_acl_stats_index in ingress
Removing acl_metadata.egress_acl_stats_index in ingress
Removing acl_metadata.acl_partition_index in ingress
Removing acl_metadata.egress_port_lag_label in ingress
Removing acl_metadata.egress_bd_label in ingress
Removing acl_metadata.egress_src_port_range_id in ingress
Removing acl_metadata.egress_dst_port_range_id in ingress
Removing acl_metadata.valid in ingress
Removing i2e_metadata.valid in ingress
Removing nat_metadata.egress_nat_mode in ingress
Removing nat_metadata.nat_nexthop in ingress
Removing nat_metadata.nat_nexthop_type in ingress
Removing nat_metadata.nat_hit in ingress
Removing nat_metadata.nat_rewrite_index in ingress
Removing nat_metadata.update_checksum in ingress
Removing nat_metadata.update_udp_checksum in ingress
Removing nat_metadata.update_tcp_checksum in ingress
Removing nat_metadata.update_inner_udp_checksum in ingress
Removing nat_metadata.update_inner_tcp_checksum in ingress
Removing nat_metadata.l4_len in ingress
Removing nat_metadata.valid in ingress
Removing multicast_metadata.ipv4_mcast_key_type in ingress
Removing multicast_metadata.ipv4_mcast_key in ingress
Removing multicast_metadata.ipv6_mcast_key_type in ingress
Removing multicast_metadata.ipv6_mcast_key in ingress
Removing multicast_metadata.outer_mcast_route_hit in ingress
Removing multicast_metadata.outer_mcast_mode in ingress
Removing multicast_metadata.inner_replica in ingress
Removing multicast_metadata.replica in ingress
Removing multicast_metadata.valid in ingress
Removing nexthop_metadata.valid in ingress
Removing fabric_metadata.packetType in ingress
Removing fabric_metadata.fabric_header_present in ingress
Removing fabric_metadata.valid in ingress
Removing hash_metadata.valid in ingress
Removing meter_metadata.qos_meter_color in ingress
Removing meter_metadata.meter_index in ingress
Removing meter_metadata.valid in ingress
Removing qos_metadata.tc_qos_group in ingress
Removing qos_metadata.egress_qos_group in ingress
Removing qos_metadata.valid in ingress
Removing eg_intr_md._pad1 in egress
Removing eg_intr_md.enq_qdepth in egress
Removing eg_intr_md._pad2 in egress
Removing eg_intr_md.enq_congest_stat in egress
Removing eg_intr_md.enq_tstamp in egress
Removing eg_intr_md._pad3 in egress
Removing eg_intr_md.deq_qdepth in egress
Removing eg_intr_md._pad4 in egress
Removing eg_intr_md.deq_congest_stat in egress
Removing eg_intr_md.app_pool_congest_stat in egress
Removing eg_intr_md.deq_timedelta in egress
Removing eg_intr_md._pad5 in egress
Removing eg_intr_md.egress_rid_first in egress
Removing eg_intr_md._pad6 in egress
Removing eg_intr_md.egress_qid in egress
Removing eg_intr_md_from_parser_aux.egress_global_tstamp in egress
Removing eg_intr_md_from_parser_aux.egress_global_ver in egress
Removing eg_intr_md_from_parser_aux.egress_parser_err in egress
Removing eg_intr_md_from_parser_aux.coalesce_sample_count in egress
Removing ig_prsr_ctrl.priority in egress
Removing ig_prsr_ctrl._pad1 in egress
Removing ig_prsr_ctrl.parser_counter in egress
Removing eg_intr_md_for_oport._pad1 in egress
Removing eg_intr_md_for_oport.update_delay_on_tx in egress
Removing eg_intr_md_for_oport.force_tx_error in egress
Removing eg_intr_md_for_mb.coalesce_flush in egress
Removing eg_intr_md_for_mb.coalesce_length in egress

-----------------------------------------------
   Eliminating unused packet fields (30 instances)
-----------------------------------------------
Removing ethernet.valid in ingress
Removing llc_header.valid in ingress
Removing snap_header.valid in ingress
Removing vlan_tag_[0].valid in ingress
Removing vlan_tag_[1].valid in ingress
Removing mpls[0].valid in ingress
Removing mpls[1].valid in ingress
Removing mpls[2].valid in ingress
Removing ipv4.valid in ingress
Removing ipv4_option_32b.valid in ingress
Removing ipv6.valid in ingress
Removing icmp.valid in ingress
Removing igmp.valid in ingress
Removing tcp.valid in ingress
Removing udp.valid in ingress
Removing gre.valid in ingress
Removing nvgre.valid in ingress
Removing inner_ethernet.valid in ingress
Removing inner_ipv4.valid in ingress
Removing inner_ipv6.valid in ingress
Removing erspan_t3_header.valid in ingress
Removing vxlan.valid in ingress
Removing genv.valid in ingress
Removing inner_icmp.valid in ingress
Removing inner_tcp.valid in ingress
Removing inner_udp.valid in ingress
Removing fabric_header.valid in ingress
Removing fabric_header_cpu.valid in ingress
Removing fabric_payload_header.valid in ingress
Removing fabric_header_timestamp.valid in ingress

-----------------------------------------------
   Eliminating metadata written but never read (18 instances)
-----------------------------------------------
Removing global_config_metadata.switch_id in ingress
Eliminating constraint from i2e_metadata.ingress_tstamp (ingress): DifferentContainer Constraint: i2e_metadata.ingress_tstamp <32 bits ingress meta R W> -- other field instance: global_config_metadata.switch_id <32 bits ingress meta W>
Eliminating constraint from ingress_metadata.ingress_port (ingress): DifferentContainer Constraint: ingress_metadata.ingress_port <9 bits ingress meta W> -- other field instance: global_config_metadata.switch_id <32 bits ingress meta W>
Removing l2_metadata.lkp_pcp in ingress
Eliminating constraint from l2_metadata.lkp_pkt_type (ingress): DifferentContainer Constraint: l2_metadata.lkp_pkt_type <3 bits ingress meta R W> -- other field instance: l2_metadata.lkp_pcp <3 bits ingress meta W>
Eliminating constraint from vlan_tag_[0].pcp (ingress): MauGroup Constraint: vlan_tag_[0].pcp <3 bits ingress parsed R> -- other field instance: l2_metadata.lkp_pcp <3 bits ingress meta W>
Removing l3_metadata.lkp_dscp in ingress
Eliminating constraint from l3_metadata.lkp_ip_type (ingress): DifferentContainer Constraint: l3_metadata.lkp_ip_type <2 bits ingress meta R W> -- other field instance: l3_metadata.lkp_dscp <8 bits ingress meta W>
Eliminating constraint from ipv4.diffserv (ingress): MauGroup Constraint: ipv4.diffserv <8 bits ingress parsed R> -- other field instance: l3_metadata.lkp_dscp <8 bits ingress meta W>
Eliminating constraint from l3_metadata.lkp_ip_llmc (ingress): DifferentContainer Constraint: l3_metadata.lkp_ip_llmc <1 bits ingress meta R W> -- other field instance: l3_metadata.lkp_dscp <8 bits ingress meta W>
Eliminating constraint from l3_metadata.lkp_ip_mc (ingress): DifferentContainer Constraint: l3_metadata.lkp_ip_mc <1 bits ingress meta R W> -- other field instance: l3_metadata.lkp_dscp <8 bits ingress meta W>
Eliminating constraint from ipv6.trafficClass (ingress): MauGroup Constraint: ipv6.trafficClass <8 bits ingress parsed R> -- other field instance: l3_metadata.lkp_dscp <8 bits ingress meta W>
Removing tunnel_metadata.mpls_exp in ingress
Eliminating constraint from tunnel_metadata.tunnel_vni (ingress): DifferentContainer Constraint: tunnel_metadata.tunnel_vni <24 bits ingress parsed meta R W> -- other field instance: tunnel_metadata.mpls_exp <3 bits ingress meta W>
Eliminating constraint from tunnel_metadata.tunnel_lookup (ingress): DifferentContainer Constraint: tunnel_metadata.tunnel_lookup <1 bits ingress meta R W> -- other field instance: tunnel_metadata.mpls_exp <3 bits ingress meta W>
Eliminating constraint from mpls[0].exp (ingress): MauGroup Constraint: mpls[0].exp <3 bits ingress parsed R> -- other field instance: tunnel_metadata.mpls_exp <3 bits ingress meta W>
Removing acl_metadata.copp_meter_id in ingress
Eliminating constraint from ig_intr_md_for_tm.packet_color (ingress): DifferentContainer Constraint: ig_intr_md_for_tm.packet_color <2 bits ingress imeta W> -- other field instance: acl_metadata.copp_meter_id <8 bits ingress meta W>
Removing nat_metadata.ingress_nat_mode in ingress
Removing meter_metadata.packet_color in ingress
Eliminating constraint from i2e_metadata.ingress_tstamp_hi (ingress): DifferentContainer Constraint: i2e_metadata.ingress_tstamp_hi <16 bits ingress meta W> -- other field instance: meter_metadata.packet_color <2 bits ingress meta W>
Removing qos_metadata.ingress_qos_group in ingress
Eliminating constraint from i2e_metadata.ingress_tstamp_hi (ingress): DifferentContainer Constraint: i2e_metadata.ingress_tstamp_hi <16 bits ingress meta W> -- other field instance: qos_metadata.ingress_qos_group <5 bits ingress meta W>
Removing qos_metadata.lkp_tc in ingress
Eliminating constraint from i2e_metadata.ingress_tstamp_hi (ingress): DifferentContainer Constraint: i2e_metadata.ingress_tstamp_hi <16 bits ingress meta W> -- other field instance: qos_metadata.lkp_tc <8 bits ingress meta W>
Removing qos_metadata.trust_dscp in ingress
Eliminating constraint from i2e_metadata.ingress_tstamp_hi (ingress): DifferentContainer Constraint: i2e_metadata.ingress_tstamp_hi <16 bits ingress meta W> -- other field instance: qos_metadata.trust_dscp <1 bits ingress meta W>
Removing qos_metadata.trust_pcp in ingress
Eliminating constraint from i2e_metadata.ingress_tstamp_hi (ingress): DifferentContainer Constraint: i2e_metadata.ingress_tstamp_hi <16 bits ingress meta W> -- other field instance: qos_metadata.trust_pcp <1 bits ingress meta W>
Removing qos_metadata.egress_qos_group in egress
Removing acl_metadata.egress_port_lag_label in egress
Removing nat_metadata.egress_nat_mode in egress
Removing acl_metadata.egress_bd_label in egress
Removing tunnel_metadata.tunnel_src_index in egress
Removing l3_metadata.egress_l4_sport in egress
Eliminating constraint from --validity_check--inner_ethernet (egress): DifferentContainer Constraint: --validity_check--inner_ethernet <1 bits egress parsed pov R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from --validity_check--udp (egress): DifferentContainer Constraint: --validity_check--udp <1 bits egress parsed pov R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from --validity_check--vxlan (egress): DifferentContainer Constraint: --validity_check--vxlan <1 bits egress parsed pov W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from udp.dstPort (egress): DifferentContainer Constraint: udp.dstPort <16 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from l3_metadata.egress_l4_dport (egress): DifferentContainer Constraint: l3_metadata.egress_l4_dport <16 bits egress meta W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from udp.checksum (egress): DifferentContainer Constraint: udp.checksum <16 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from udp.length_ (egress): DifferentContainer Constraint: udp.length_ <16 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from vxlan.flags (egress): DifferentContainer Constraint: vxlan.flags <8 bits egress parsed W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from vxlan.reserved (egress): DifferentContainer Constraint: vxlan.reserved <24 bits egress parsed W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from vxlan.reserved2 (egress): DifferentContainer Constraint: vxlan.reserved2 <8 bits egress parsed W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from --validity_check--ipv4 (egress): DifferentContainer Constraint: --validity_check--ipv4 <1 bits egress parsed pov R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from ipv4.protocol (egress): DifferentContainer Constraint: ipv4.protocol <8 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from ipv4.ttl (egress): DifferentContainer Constraint: ipv4.ttl <8 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from ipv4.version (egress): DifferentContainer Constraint: ipv4.version <4 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from ipv4.ihl (egress): DifferentContainer Constraint: ipv4.ihl <4 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from ipv4.diffserv (egress): DifferentContainer Constraint: ipv4.diffserv <8 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from ipv4.identification (egress): DifferentContainer Constraint: ipv4.identification <16 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from ipv4.flags (egress): DifferentContainer Constraint: ipv4.flags <3 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from ipv4.totalLen (egress): DifferentContainer Constraint: ipv4.totalLen <16 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from ethernet.etherType (egress): DifferentContainer Constraint: ethernet.etherType <16 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from hash_metadata.entropy_hash (egress): MauGroup Constraint: hash_metadata.entropy_hash <16 bits egress meta R> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from --validity_check--genv (egress): DifferentContainer Constraint: --validity_check--genv <1 bits egress parsed pov W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from genv.ver (egress): DifferentContainer Constraint: genv.ver <2 bits egress parsed W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from genv.oam (egress): DifferentContainer Constraint: genv.oam <1 bits egress parsed W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from genv.critical (egress): DifferentContainer Constraint: genv.critical <1 bits egress parsed W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from genv.optLen (egress): DifferentContainer Constraint: genv.optLen <6 bits egress parsed W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from genv.protoType (egress): DifferentContainer Constraint: genv.protoType <16 bits egress parsed W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from genv.reserved (egress): DifferentContainer Constraint: genv.reserved <6 bits egress parsed W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from genv.reserved2 (egress): DifferentContainer Constraint: genv.reserved2 <8 bits egress parsed W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from --validity_check--ipv6 (egress): DifferentContainer Constraint: --validity_check--ipv6 <1 bits egress parsed pov R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from ipv6.version (egress): DifferentContainer Constraint: ipv6.version <4 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from ipv6.nextHdr (egress): DifferentContainer Constraint: ipv6.nextHdr <8 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from ipv6.hopLimit (egress): DifferentContainer Constraint: ipv6.hopLimit <8 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from ipv6.trafficClass (egress): DifferentContainer Constraint: ipv6.trafficClass <8 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from ipv6.flowLabel (egress): DifferentContainer Constraint: ipv6.flowLabel <20 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Eliminating constraint from ipv6.payloadLen (egress): DifferentContainer Constraint: ipv6.payloadLen <16 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_sport <16 bits egress meta W>
Removing l3_metadata.egress_l4_dport in egress
Eliminating constraint from inner_ethernet.srcAddr (egress): DifferentContainer Constraint: inner_ethernet.srcAddr <48 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_dport <16 bits egress meta W>
Eliminating constraint from inner_ethernet.etherType (egress): DifferentContainer Constraint: inner_ethernet.etherType <16 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_dport <16 bits egress meta W>
Eliminating constraint from inner_ethernet.dstAddr (egress): DifferentContainer Constraint: inner_ethernet.dstAddr <48 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_dport <16 bits egress meta W>
Eliminating constraint from udp.srcPort (egress): DifferentContainer Constraint: udp.srcPort <16 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_dport <16 bits egress meta W>
Eliminating constraint from udp.length_ (egress): DifferentContainer Constraint: udp.length_ <16 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_dport <16 bits egress meta W>
Eliminating constraint from vxlan.vni (egress): DifferentContainer Constraint: vxlan.vni <24 bits egress parsed W> -- other field instance: l3_metadata.egress_l4_dport <16 bits egress meta W>
Eliminating constraint from ipv4.totalLen (egress): DifferentContainer Constraint: ipv4.totalLen <16 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_dport <16 bits egress meta W>
Eliminating constraint from genv.vni (egress): DifferentContainer Constraint: genv.vni <24 bits egress parsed W> -- other field instance: l3_metadata.egress_l4_dport <16 bits egress meta W>
Eliminating constraint from ipv6.payloadLen (egress): DifferentContainer Constraint: ipv6.payloadLen <16 bits egress parsed R W> -- other field instance: l3_metadata.egress_l4_dport <16 bits egress meta W>

--------------------------------------------
  ingress field instance bit width histogram
--------------------------------------------
   Total fields: 321
   Max value: 98

    1 : xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx (98)
    2 : xxxxxxxxxxxxx (13)
    3 : xxxxxxxxxxxxxxxx (16)
    4 : xxxxxxxxxxxx (12)
    5 : xxxxx (5)
    6 : xx (2)
    8 : xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx (33)
    9 : xxxx (4)
   10 : xxxxx (5)
   12 : xxxxx (5)
   13 : xxxx (4)
   14 : xxxxxxxxxxxx (12)
   16 : xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx (74)
   20 : xxxxx (5)
   24 : xxxxxx (6)
   32 : xxxxxxxxxxxxxx (14)
   48 : xxxxxxx (7)
  128 : xxxxxx (6)

--------------------------------------------
  egress field instance bit width histogram
--------------------------------------------
   Total fields: 238
   Max value: 59

    1 : xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx (56)
    2 : xxxxx (5)
    3 : xxxxxxxxxxxxxx (14)
    4 : xxxxxxxxxxxxxx (14)
    5 : xxxxxx (6)
    6 : xxx (3)
    7 : xx (2)
    8 : xxxxxxxxxxxxxxxxxxxxxxxxxxxx (28)
    9 : xxx (3)
   10 : x (1)
   12 : xxxx (4)
   13 : xx (2)
   14 : xxxxxxxxx (9)
   16 : xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx (59)
   20 : xxxxx (5)
   24 : xxxxxx (6)
   32 : xxxxxxxxxxxx (12)
   48 : xxxxx (5)
  128 : xxxx (4)

HLIR Version: 0.10.22
PHV container sizes are: [8, 16, 32]
Parser state extraction bandwidth: 224
  8-bit: 4 extracts
  16-bit: 4 extracts
  32-bit: 4 extracts
Free containers to start for 8 bits:
  Group 4 8 bits has 16 available
  Group 5 8 bits has 16 available
  Group 6 8 bits has 16 available
  Group 7 8 bits has 16 available
  Group 16 8 bits (tagalong) has 16 available
  Group 17 8 bits (tagalong) has 16 available
Free containers to start for 16 bits:
  Group 8 16 bits has 16 available
  Group 9 16 bits has 16 available
  Group 10 16 bits has 16 available
  Group 11 16 bits has 16 available
  Group 12 16 bits has 16 available
  Group 13 16 bits has 16 available
  Group 18 16 bits (tagalong) has 16 available
  Group 19 16 bits (tagalong) has 16 available
  Group 20 16 bits (tagalong) has 16 available
Free containers to start for 32 bits:
  Group 0 32 bits has 16 available
  Group 1 32 bits has 16 available
  Group 2 32 bits has 16 available
  Group 3 32 bits has 16 available
  Group 14 32 bits (tagalong) has 16 available
  Group 15 32 bits (tagalong) has 16 available


Initializing PHV allocation...
Adding: InfoOnly Constraint: ig_intr_md_for_mb.ingress_mirror_id <10 bits ingress imeta W>
Added constraint: SolitaryMirror Constraint: eg_intr_md_for_oport._pad1 <2 bits egress imeta>
Added constraint: SolitaryMirror Constraint: eg_intr_md_for_oport.capture_tstamp_on_tx <1 bits egress imeta W>
Added constraint: SolitaryMirror Constraint: eg_intr_md_for_oport.update_delay_on_tx <1 bits egress imeta>
Added constraint: SolitaryMirror Constraint: eg_intr_md_for_oport.force_tx_error <1 bits egress imeta>
Added constraint: SolitaryMirror Constraint: eg_intr_md_for_oport.drop_ctl <3 bits egress imeta W>

-----------------------------------------------
   User added PHV constraints
-----------------------------------------------
User indicated that ingress header erspan_t3_header should never be deparsed.
User indicated that ingress header erspan_t3_header will never be parsed.
User indicated that egress header erspan_t3_header will never be parsed.
User added constraint MaxFieldSplit Constraint: ig_intr_md_from_parser_aux.ingress_parser_err <16 bits ingress parsed imeta R> -- max split: 1
User added constraint MaxFieldSplit Constraint: ig_intr_md_for_tm.ucast_egress_port <9 bits ingress imeta W> -- max split: 1
User added constraint MaxFieldSplit Constraint: ig_intr_md_for_tm.level2_mcast_hash <13 bits ingress imeta W> -- max split: 1
User added constraint MaxFieldSplit Constraint: ig_intr_md_for_tm.level2_exclusion_id <9 bits ingress imeta W> -- max split: 1
User added constraint MaxFieldSplit Constraint: ig_intr_md_for_tm.mcast_grp_b <16 bits ingress imeta W> -- max split: 1
User added constraint MaxFieldSplit Constraint: ig_intr_md_for_tm.level1_exclusion_id <16 bits ingress imeta W> -- max split: 1
User added constraint MaxFieldSplit Constraint: ig_intr_md_for_tm.rid <16 bits ingress imeta W> -- max split: 1
User added constraint MaxFieldSplit Constraint: ig_intr_md_for_tm.level1_mcast_hash <13 bits ingress imeta W> -- max split: 1
User added constraint MaxFieldSplit Constraint: ig_intr_md_for_tm.mcast_grp_a <16 bits ingress imeta> -- max split: 1
User added constraint MaxFieldSplit Constraint: ig_intr_md_for_mb.ingress_mirror_id <10 bits ingress imeta W> -- max split: 1
User added constraint MaxFieldSplit Constraint: eg_intr_md.egress_port <9 bits egress parsed imeta R> -- max split: 1
User added constraint MaxFieldSplit Constraint: eg_intr_md_from_parser_aux.egress_parser_err <16 bits egress parsed imeta> -- max split: 1
User added constraint MaxFieldSplit Constraint: eg_intr_md_for_mb.egress_mirror_id <10 bits egress imeta W> -- max split: 1
User indicated that field ipv4_option_32b.option_fields (ingress) should not be overlayed.
User indicated that field ipv4_option_32b.option_fields (egress) should not be overlayed.
User added constraint MaxFieldSplit Constraint: egress_metadata.port_type <2 bits egress meta R W> -- max split: 1
User added constraint Solitary Constraint: egress_metadata.port_type <2 bits egress meta R W>
User added constraint ContainerSize Constraint: l2_metadata.same_if_check <14 bits ingress meta R W> -- container size: 16 -- field bit: 0
User indicated that ingress fields ipv4_metadata.lkp_ipv4_sa and ipv6_metadata.lkp_ipv6_sa are mutually exclusive.
User indicated that ingress fields ipv4_metadata.lkp_ipv4_da and ipv6_metadata.lkp_ipv6_da are mutually exclusive.
User added constraint Solitary Constraint: acl_metadata.port_lag_label <16 bits ingress meta R W>
User added constraint MaxFieldSplit Constraint: acl_metadata.port_lag_label <16 bits ingress meta R W> -- max split: 1
User added constraint MaxFieldSplit Constraint: multicast_metadata.multicast_bridge_mc_index <16 bits ingress meta R W> -- max split: 1
User added constraint MaxFieldSplit Constraint: multicast_metadata.multicast_route_mc_index <16 bits ingress meta R W> -- max split: 1
User added constraint Solitary Constraint: multicast_metadata.multicast_bridge_mc_index <16 bits ingress meta R W>
User added constraint Solitary Constraint: multicast_metadata.multicast_route_mc_index <16 bits ingress meta R W>
User added constraint Solitary Constraint: fabric_metadata.reason_code <16 bits ingress meta R W>
User added constraint MaxFieldSplit Constraint: hash_metadata.hash1 <16 bits ingress meta R W> -- max split: 1
User added constraint Solitary Constraint: hash_metadata.hash1 <16 bits ingress meta R W>
User added constraint MaxFieldSplit Constraint: hash_metadata.hash2 <16 bits ingress meta R W> -- max split: 1
User added constraint Solitary Constraint: hash_metadata.hash2 <16 bits ingress meta R W>

-----------------------------------------------
  Scanning for field list calculations
-----------------------------------------------
Adding: Solitary Constraint: ipv4.hdrChecksum <16 bits egress parsed R W>
Adding: ContainerAlignment Constraint: ipv4.diffserv <8 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.diffserv <8 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.totalLen <16 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.totalLen <16 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.identification <16 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.identification <16 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.fragOffset <13 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.fragOffset <13 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.protocol <8 bits ingress parsed R> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.protocol <8 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.srcAddr <32 bits ingress parsed R> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.srcAddr <32 bits egress parsed R W> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.srcAddr <32 bits ingress parsed R> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.srcAddr <32 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.dstAddr <32 bits ingress parsed R> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.dstAddr <32 bits egress parsed R W> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.dstAddr <32 bits ingress parsed R> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4.dstAddr <32 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4_option_32b.option_fields <32 bits ingress parsed tagalong> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4_option_32b.option_fields <32 bits egress parsed tagalong> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4_option_32b.option_fields <32 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: ipv4_option_32b.option_fields <32 bits egress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: Solitary Constraint: inner_ipv4.hdrChecksum <16 bits egress parsed R W>
Adding: ContainerAlignment Constraint: inner_ipv4.diffserv <8 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.diffserv <8 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.totalLen <16 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.totalLen <16 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.identification <16 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.identification <16 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.fragOffset <13 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.fragOffset <13 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.protocol <8 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.protocol <8 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.srcAddr <32 bits ingress parsed tagalong> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.srcAddr <32 bits egress parsed R W> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.srcAddr <32 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.srcAddr <32 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.dstAddr <32 bits ingress parsed tagalong> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.dstAddr <32 bits egress parsed R W> -- field_bit: 16 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.dstAddr <32 bits ingress parsed tagalong> -- field_bit: 0 -- bits_list: [0, 16]
Adding: ContainerAlignment Constraint: inner_ipv4.dstAddr <32 bits egress parsed R W> -- field_bit: 0 -- bits_list: [0, 16]

-----------------------------------------------
   Eliminating unused metadata (195 instances)
-----------------------------------------------
Removing standard_metadata.ingress_port in ingress
Removing standard_metadata.packet_length in ingress
Removing standard_metadata.egress_spec in ingress
Removing standard_metadata.egress_port in ingress
Removing standard_metadata.egress_instance in ingress
Removing standard_metadata.instance_type in ingress
Removing standard_metadata.clone_spec in ingress
Removing standard_metadata.valid in ingress
Removing ig_prsr_ctrl.priority in ingress
Removing ig_prsr_ctrl.priority (ingress) from parse_set_prio_med's set_metadata.
Removing ig_prsr_ctrl.priority (ingress) from parse_set_prio_high's set_metadata.
Removing ig_prsr_ctrl._pad1 in ingress
Removing ig_prsr_ctrl.parser_counter in ingress
Removing ig_prsr_ctrl.valid in ingress
Removing ig_intr_md.valid in ingress
Removing ig_intr_md_from_parser_aux.ingress_global_tstamp in ingress
Removing ig_intr_md_from_parser_aux.ingress_global_ver in ingress
Removing ig_intr_md_from_parser_aux.valid in ingress
Removing ig_intr_md_for_tm._pad1 in ingress
Removing ig_intr_md_for_tm.bypass_egress in ingress
Removing ig_intr_md_for_tm.deflect_on_drop in ingress
Removing ig_intr_md_for_tm.icos_for_copy_to_cpu in ingress
Removing ig_intr_md_for_tm._pad2 in ingress
Removing ig_intr_md_for_tm.enable_mcast_cutthru in ingress
Removing ig_intr_md_for_tm.mcast_grp_a in ingress
Removing ig_intr_md_for_tm._pad3 in ingress
Removing ig_intr_md_for_tm._pad4 in ingress
Removing ig_intr_md_for_tm._pad5 in ingress
Removing ig_intr_md_for_tm.valid in ingress
Removing ig_intr_md_for_mb._pad1 in ingress
Removing ig_intr_md_for_mb.valid in ingress
Removing eg_intr_md._pad0 in ingress
Removing eg_intr_md.egress_port in ingress
Removing eg_intr_md._pad1 in ingress
Removing eg_intr_md.enq_qdepth in ingress
Removing eg_intr_md._pad2 in ingress
Removing eg_intr_md.enq_congest_stat in ingress
Removing eg_intr_md.enq_tstamp in ingress
Removing eg_intr_md._pad3 in ingress
Removing eg_intr_md.deq_qdepth in ingress
Removing eg_intr_md._pad4 in ingress
Removing eg_intr_md.deq_congest_stat in ingress
Removing eg_intr_md.app_pool_congest_stat in ingress
Removing eg_intr_md.deq_timedelta in ingress
Removing eg_intr_md.egress_rid in ingress
Removing eg_intr_md._pad5 in ingress
Removing eg_intr_md.egress_rid_first in ingress
Removing eg_intr_md._pad6 in ingress
Removing eg_intr_md.egress_qid in ingress
Removing eg_intr_md._pad7 in ingress
Removing eg_intr_md.egress_cos in ingress
Removing eg_intr_md._pad8 in ingress
Removing eg_intr_md.deflection_flag in ingress
Removing eg_intr_md.pkt_length in ingress
Removing eg_intr_md.valid in ingress
Removing eg_intr_md_from_parser_aux.egress_global_tstamp in ingress
Removing eg_intr_md_from_parser_aux.egress_global_ver in ingress
Removing eg_intr_md_from_parser_aux.egress_parser_err in ingress
Removing eg_intr_md_from_parser_aux.clone_digest_id in ingress
Removing eg_intr_md_from_parser_aux.clone_src in ingress
Removing eg_intr_md_from_parser_aux.coalesce_sample_count in ingress
Removing eg_intr_md_from_parser_aux.valid in ingress
Removing eg_intr_md_for_mb._pad1 in ingress
Removing eg_intr_md_for_mb.egress_mirror_id in ingress
Removing eg_intr_md_for_mb.coalesce_flush in ingress
Removing eg_intr_md_for_mb.coalesce_length in ingress
Removing eg_intr_md_for_mb.valid in ingress
Removing eg_intr_md_for_oport._pad1 in ingress
Removing eg_intr_md_for_oport.capture_tstamp_on_tx in ingress
Removing eg_intr_md_for_oport.update_delay_on_tx in ingress
Removing eg_intr_md_for_oport.force_tx_error in ingress
Removing eg_intr_md_for_oport.drop_ctl in ingress
Removing eg_intr_md_for_oport.valid in ingress
Removing ingress_metadata.outer_bd in ingress
Removing ingress_metadata.control_frame in ingress
Removing ingress_metadata.valid in ingress
Removing egress_metadata.port_type in ingress
Removing egress_metadata.payload_length in ingress
Removing egress_metadata.smac_idx in ingress
Removing egress_metadata.bd in ingress
Removing egress_metadata.outer_bd in ingress
Removing egress_metadata.mac_da in ingress
Removing egress_metadata.routed in ingress
Removing egress_metadata.same_bd_check in ingress
Removing egress_metadata.drop_reason in ingress
Removing egress_metadata.ifindex in ingress
Removing egress_metadata.egress_port in ingress
Removing egress_metadata.valid in ingress
Removing intrinsic_metadata.mcast_grp in ingress
Removing intrinsic_metadata.lf_field_list in ingress
Removing intrinsic_metadata.egress_rid in ingress
Removing intrinsic_metadata.ingress_global_timestamp in ingress
Removing intrinsic_metadata.valid in ingress
Removing global_config_metadata.enable_dod in ingress
Removing global_config_metadata.switch_id in ingress
Removing global_config_metadata.valid in ingress
Removing l2_metadata.lkp_pcp in ingress
Removing l2_metadata.valid in ingress
Removing l3_metadata.lkp_dscp in ingress
Removing l3_metadata.fib_partition_index in ingress
Removing l3_metadata.outer_routed in ingress
Removing l3_metadata.mtu_index in ingress
Removing l3_metadata.l3_mtu_check in ingress
Removing l3_metadata.egress_l4_sport in ingress
Removing l3_metadata.egress_l4_dport in ingress
Removing l3_metadata.valid in ingress
Removing ipv4_metadata.valid in ingress
Removing ipv6_metadata.valid in ingress
Removing tunnel_metadata.mpls_enabled in ingress
Removing tunnel_metadata.mpls_ttl in ingress
Removing tunnel_metadata.mpls_exp in ingress
Removing tunnel_metadata.mpls_in_udp in ingress
Removing tunnel_metadata.egress_tunnel_type in ingress
Removing tunnel_metadata.tunnel_index in ingress
Removing tunnel_metadata.tunnel_src_index in ingress
Removing tunnel_metadata.tunnel_smac_index in ingress
Removing tunnel_metadata.tunnel_dmac_index in ingress
Removing tunnel_metadata.vnid in ingress
Removing tunnel_metadata.egress_header_count in ingress
Removing tunnel_metadata.inner_ip_proto in ingress
Removing tunnel_metadata.valid in ingress
Removing acl_metadata.egress_acl_deny in ingress
Removing acl_metadata.mirror_acl_stats_index in ingress
Removing acl_metadata.egress_acl_stats_index in ingress
Removing acl_metadata.acl_partition_index in ingress
Removing acl_metadata.egress_port_lag_label in ingress
Removing acl_metadata.egress_bd_label in ingress
Removing acl_metadata.egress_src_port_range_id in ingress
Removing acl_metadata.egress_dst_port_range_id in ingress
Removing acl_metadata.copp_meter_id in ingress
Removing acl_metadata.valid in ingress
Removing i2e_metadata.valid in ingress
Removing nat_metadata.ingress_nat_mode in ingress
Removing nat_metadata.egress_nat_mode in ingress
Removing nat_metadata.nat_nexthop in ingress
Removing nat_metadata.nat_nexthop_type in ingress
Removing nat_metadata.nat_hit in ingress
Removing nat_metadata.nat_rewrite_index in ingress
Removing nat_metadata.update_checksum in ingress
Removing nat_metadata.update_udp_checksum in ingress
Removing nat_metadata.update_tcp_checksum in ingress
Removing nat_metadata.update_inner_udp_checksum in ingress
Removing nat_metadata.update_inner_tcp_checksum in ingress
Removing nat_metadata.l4_len in ingress
Removing nat_metadata.valid in ingress
Removing multicast_metadata.ipv4_mcast_key_type in ingress
Removing multicast_metadata.ipv4_mcast_key in ingress
Removing multicast_metadata.ipv6_mcast_key_type in ingress
Removing multicast_metadata.ipv6_mcast_key in ingress
Removing multicast_metadata.outer_mcast_route_hit in ingress
Removing multicast_metadata.outer_mcast_mode in ingress
Removing multicast_metadata.inner_replica in ingress
Removing multicast_metadata.replica in ingress
Removing multicast_metadata.valid in ingress
Removing nexthop_metadata.valid in ingress
Removing fabric_metadata.packetType in ingress
Removing fabric_metadata.fabric_header_present in ingress
Removing fabric_metadata.valid in ingress
Removing hash_metadata.valid in ingress
Removing meter_metadata.qos_meter_color in ingress
Removing meter_metadata.packet_color in ingress
Removing meter_metadata.meter_index in ingress
Removing meter_metadata.valid in ingress
Removing qos_metadata.ingress_qos_group in ingress
Removing qos_metadata.tc_qos_group in ingress
Removing qos_metadata.egress_qos_group in ingress
Removing qos_metadata.lkp_tc in ingress
Removing qos_metadata.trust_dscp in ingress
Removing qos_metadata.trust_pcp in ingress
Removing qos_metadata.valid in ingress
Removing eg_intr_md._pad1 in egress
Removing eg_intr_md.enq_qdepth in egress
Removing eg_intr_md._pad2 in egress
Removing eg_intr_md.enq_congest_stat in egress
Removing eg_intr_md.enq_tstamp in egress
Removing eg_intr_md._pad3 in egress
Removing eg_intr_md.deq_qdepth in egress
Removing eg_intr_md._pad4 in egress
Removing eg_intr_md.deq_congest_stat in egress
Removing eg_intr_md.app_pool_congest_stat in egress
Removing eg_intr_md.deq_timedelta in egress
Removing eg_intr_md._pad5 in egress
Removing eg_intr_md.egress_rid_first in egress
Removing eg_intr_md._pad6 in egress
Removing eg_intr_md.egress_qid in egress
Removing eg_intr_md_from_parser_aux.egress_global_tstamp in egress
Removing eg_intr_md_from_parser_aux.egress_global_ver in egress
Removing eg_intr_md_from_parser_aux.egress_parser_err in egress
Removing eg_intr_md_from_parser_aux.coalesce_sample_count in egress
Removing ig_prsr_ctrl.priority in egress
Removing ig_prsr_ctrl._pad1 in egress
Removing ig_prsr_ctrl.parser_counter in egress
Removing eg_intr_md_for_oport._pad1 in egress
Removing eg_intr_md_for_oport.update_delay_on_tx in egress
Removing eg_intr_md_for_oport.force_tx_error in egress
Removing eg_intr_md_for_mb.coalesce_flush in egress
Removing eg_intr_md_for_mb.coalesce_length in egress

-----------------------------------------------
   Eliminating unused packet fields (30 instances)
-----------------------------------------------
Removing ethernet.valid in ingress
Removing llc_header.valid in ingress
Removing snap_header.valid in ingress
Removing vlan_tag_[0].valid in ingress
Removing vlan_tag_[1].valid in ingress
Removing mpls[0].valid in ingress
Removing mpls[1].valid in ingress
Removing mpls[2].valid in ingress
Removing ipv4.valid in ingress
Removing ipv4_option_32b.valid in ingress
Removing ipv6.valid in ingress
Removing icmp.valid in ingress
Removing igmp.valid in ingress
Removing tcp.valid in ingress
Removing udp.valid in ingress
Removing gre.valid in ingress
Removing nvgre.valid in ingress
Removing inner_ethernet.valid in ingress
Removing inner_ipv4.valid in ingress
Removing inner_ipv6.valid in ingress
Removing erspan_t3_header.valid in ingress
Removing vxlan.valid in ingress
Removing genv.valid in ingress
Removing inner_icmp.valid in ingress
Removing inner_tcp.valid in ingress
Removing inner_udp.valid in ingress
Removing fabric_header.valid in ingress
Removing fabric_header_cpu.valid in ingress
Removing fabric_payload_header.valid in ingress
Removing fabric_header_timestamp.valid in ingress

--------------------------------------------
  ingress field instance bit width histogram
--------------------------------------------
   Total fields: 321
   Max value: 98

    1 : xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx (98)
    2 : xxxxxxxxxxxxx (13)
    3 : xxxxxxxxxxxxxxxx (16)
    4 : xxxxxxxxxxxx (12)
    5 : xxxxx (5)
    6 : xx (2)
    8 : xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx (33)
    9 : xxxx (4)
   10 : xxxxx (5)
   12 : xxxxx (5)
   13 : xxxx (4)
   14 : xxxxxxxxxxxx (12)
   16 : xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx (74)
   20 : xxxxx (5)
   24 : xxxxxx (6)
   32 : xxxxxxxxxxxxxx (14)
   48 : xxxxxxx (7)
  128 : xxxxxx (6)

--------------------------------------------
  egress field instance bit width histogram
--------------------------------------------
   Total fields: 238
   Max value: 59

    1 : xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx (56)
    2 : xxxxx (5)
    3 : xxxxxxxxxxxxxx (14)
    4 : xxxxxxxxxxxxxx (14)
    5 : xxxxxx (6)
    6 : xxx (3)
    7 : xx (2)
    8 : xxxxxxxxxxxxxxxxxxxxxxxxxxxx (28)
    9 : xxx (3)
   10 : x (1)
   12 : xxxx (4)
   13 : xx (2)
   14 : xxxxxxxxx (9)
   16 : xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx (59)
   20 : xxxxx (5)
   24 : xxxxxx (6)
   32 : xxxxxxxxxxxx (12)
   48 : xxxxx (5)
  128 : xxxx (4)

-------------------------------------------------------------------------------------------------------------------------------------------
|                   Field Name                  | Bit Width | Direction | Parsed? | Deparsed? | Metadata? | Read in MAU? | Write in MAU? |
-------------------------------------------------------------------------------------------------------------------------------------------
|       --validity_check--erspan_t3_header      |     1     |   egress  |    x    |     x     |           |              |       x       |
|           --validity_check--ethernet          |     1     |   egress  |    x    |     x     |           |      x       |       x       |
|        --validity_check--fabric_header        |     1     |   egress  |    x    |     x     |           |              |       x       |
|      --validity_check--fabric_header_cpu      |     1     |   egress  |    x    |     x     |           |              |       x       |
|   --validity_check--fabric_header_timestamp   |     1     |   egress  |    x    |     x     |           |              |       x       |
|    --validity_check--fabric_payload_header    |     1     |   egress  |    x    |     x     |           |              |       x       |
|             --validity_check--genv            |     1     |   egress  |    x    |     x     |           |              |       x       |
|             --validity_check--gre             |     1     |   egress  |    x    |     x     |           |              |       x       |
|             --validity_check--icmp            |     1     |   egress  |    x    |     x     |           |      x       |       x       |
|             --validity_check--igmp            |     1     |   egress  |    x    |     x     |           |              |               |
|        --validity_check--inner_ethernet       |     1     |   egress  |    x    |     x     |           |      x       |       x       |
|          --validity_check--inner_icmp         |     1     |   egress  |    x    |     x     |           |      x       |       x       |
|          --validity_check--inner_ipv4         |     1     |   egress  |    x    |     x     |           |      x       |       x       |
|          --validity_check--inner_ipv6         |     1     |   egress  |    x    |     x     |           |      x       |       x       |
|          --validity_check--inner_tcp          |     1     |   egress  |    x    |     x     |           |      x       |       x       |
|          --validity_check--inner_udp          |     1     |   egress  |    x    |     x     |           |      x       |       x       |
|             --validity_check--ipv4            |     1     |   egress  |    x    |     x     |           |      x       |       x       |
|       --validity_check--ipv4_option_32b       |     1     |   egress  |    x    |     x     |           |              |               |
|             --validity_check--ipv6            |     1     |   egress  |    x    |     x     |           |      x       |       x       |
|          --validity_check--llc_header         |     1     |   egress  |    x    |     x     |           |              |               |
|           --validity_check--mpls[0]           |     1     |   egress  |    x    |     x     |           |      x       |       x       |
|           --validity_check--mpls[1]           |     1     |   egress  |    x    |     x     |           |      x       |       x       |
|           --validity_check--mpls[2]           |     1     |   egress  |    x    |     x     |           |              |       x       |
|            --validity_check--nvgre            |     1     |   egress  |    x    |     x     |           |              |       x       |
|         --validity_check--snap_header         |     1     |   egress  |    x    |     x     |           |              |               |
|             --validity_check--tcp             |     1     |   egress  |    x    |     x     |           |      x       |       x       |
|             --validity_check--udp             |     1     |   egress  |    x    |     x     |           |      x       |       x       |
|         --validity_check--vlan_tag_[0]        |     1     |   egress  |    x    |     x     |           |      x       |       x       |
|         --validity_check--vlan_tag_[1]        |     1     |   egress  |    x    |     x     |           |              |               |
|            --validity_check--vxlan            |     1     |   egress  |    x    |     x     |           |              |       x       |
|                eg_intr_md._pad0               |     7     |   egress  |    x    |           |     x     |              |               |
|                eg_intr_md._pad7               |     5     |   egress  |    x    |           |     x     |              |               |
|                eg_intr_md._pad8               |     7     |   egress  |    x    |           |     x     |              |               |
|           eg_intr_md.deflection_flag          |     1     |   egress  |    x    |           |     x     |      x       |               |
|             eg_intr_md.egress_cos             |     3     |   egress  |    x    |     x     |     x     |              |               |
|             eg_intr_md.egress_port            |     9     |   egress  |    x    |     x     |     x     |      x       |               |
|             eg_intr_md.egress_rid             |     16    |   egress  |    x    |           |     x     |      x       |               |
|             eg_intr_md.pkt_length             |     16    |   egress  |    x    |           |     x     |      x       |       x       |
|            eg_intr_md_for_mb._pad1            |     6     |   egress  |         |     x     |     x     |              |               |
|       eg_intr_md_for_mb.egress_mirror_id      |     10    |   egress  |         |     x     |     x     |              |       x       |
|   eg_intr_md_for_oport.capture_tstamp_on_tx   |     1     |   egress  |         |     x     |     x     |              |       x       |
|         eg_intr_md_for_oport.drop_ctl         |     3     |   egress  |         |     x     |     x     |              |       x       |
|   eg_intr_md_from_parser_aux.clone_digest_id  |     4     |   egress  |    x    |           |     x     |      x       |               |
|      eg_intr_md_from_parser_aux.clone_src     |     4     |   egress  |    x    |           |     x     |      x       |               |
|               egress_metadata.bd              |     14    |   egress  |         |           |     x     |      x       |       x       |
|             egress_metadata.bypass            |     1     |   egress  |    x    |           |     x     |      x       |               |
|      egress_metadata.capture_tstamp_on_tx     |     1     |   egress  |    x    |           |     x     |      x       |               |
|             egress_metadata.mac_da            |     48    |   egress  |         |           |     x     |      x       |       x       |
|            egress_metadata.outer_bd           |     14    |   egress  |         |           |     x     |      x       |       x       |
|         egress_metadata.payload_length        |     16    |   egress  |         |           |     x     |      x       |       x       |
|           egress_metadata.port_type           |     2     |   egress  |         |           |     x     |      x       |       x       |
|             egress_metadata.routed            |     1     |   egress  |         |           |     x     |      x       |       x       |
|         egress_metadata.same_bd_check         |     14    |   egress  |         |           |     x     |      x       |       x       |
|            egress_metadata.smac_idx           |     9     |   egress  |         |           |     x     |      x       |       x       |
|          erspan_t3_header.ft_d_other          |     16    |   egress  |         |     x     |           |              |       x       |
|       erspan_t3_header.priority_span_id       |     16    |   egress  |         |     x     |           |              |       x       |
|              erspan_t3_header.sgt             |     16    |   egress  |         |     x     |           |              |       x       |
|           erspan_t3_header.timestamp          |     32    |   egress  |         |     x     |           |              |       x       |
|            erspan_t3_header.version           |     4     |   egress  |         |     x     |           |              |       x       |
|             erspan_t3_header.vlan             |     12    |   egress  |         |     x     |           |              |       x       |
|                ethernet.dstAddr               |     48    |   egress  |    x    |     x     |           |      x       |       x       |
|               ethernet.etherType              |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                ethernet.srcAddr               |     48    |   egress  |    x    |     x     |           |      x       |       x       |
|            fabric_header.dstDevice            |     8     |   egress  |    x    |     x     |           |              |               |
|          fabric_header.dstPortOrGroup         |     16    |   egress  |    x    |     x     |           |              |               |
|           fabric_header.fabricColor           |     3     |   egress  |    x    |     x     |           |              |               |
|            fabric_header.fabricQos            |     5     |   egress  |    x    |     x     |           |              |               |
|          fabric_header.headerVersion          |     2     |   egress  |    x    |     x     |           |              |       x       |
|            fabric_header.packetType           |     3     |   egress  |    x    |     x     |           |              |       x       |
|          fabric_header.packetVersion          |     2     |   egress  |    x    |     x     |           |              |       x       |
|               fabric_header.pad1              |     1     |   egress  |    x    |     x     |           |              |       x       |
|     fabric_header_cpu.capture_tstamp_on_tx    |     1     |   egress  |    x    |     x     |           |              |               |
|         fabric_header_cpu.egressQueue         |     5     |   egress  |    x    |     x     |           |              |               |
|          fabric_header_cpu.ingressBd          |     16    |   egress  |    x    |     x     |           |              |       x       |
|        fabric_header_cpu.ingressIfindex       |     16    |   egress  |    x    |     x     |           |              |       x       |
|         fabric_header_cpu.ingressPort         |     16    |   egress  |    x    |     x     |           |              |       x       |
|          fabric_header_cpu.reasonCode         |     16    |   egress  |    x    |     x     |           |              |       x       |
|           fabric_header_cpu.reserved          |     1     |   egress  |    x    |     x     |           |              |               |
|           fabric_header_cpu.txBypass          |     1     |   egress  |    x    |     x     |           |              |               |
|      fabric_header_timestamp.arrival_time     |     32    |   egress  |    x    |     x     |           |              |       x       |
|    fabric_header_timestamp.arrival_time_hi    |     16    |   egress  |    x    |     x     |           |              |       x       |
|          fabric_metadata.reason_code          |     16    |   egress  |    x    |     x     |     x     |      x       |       x       |
|        fabric_payload_header.etherType        |     16    |   egress  |    x    |     x     |           |              |       x       |
|                 genv.critical                 |     1     |   egress  |    x    |     x     |           |              |       x       |
|                    genv.oam                   |     1     |   egress  |    x    |     x     |           |              |       x       |
|                  genv.optLen                  |     6     |   egress  |    x    |     x     |           |              |       x       |
|                 genv.protoType                |     16    |   egress  |    x    |     x     |           |              |       x       |
|                 genv.reserved                 |     6     |   egress  |    x    |     x     |           |              |       x       |
|                 genv.reserved2                |     8     |   egress  |    x    |     x     |           |              |       x       |
|                    genv.ver                   |     2     |   egress  |    x    |     x     |           |              |       x       |
|                    genv.vni                   |     24    |   egress  |    x    |     x     |           |              |       x       |
|                     gre.C                     |     1     |   egress  |    x    |     x     |           |              |       x       |
|                     gre.K                     |     1     |   egress  |    x    |     x     |           |              |       x       |
|                     gre.R                     |     1     |   egress  |    x    |     x     |           |              |       x       |
|                     gre.S                     |     1     |   egress  |    x    |     x     |           |              |       x       |
|                   gre.flags                   |     5     |   egress  |    x    |     x     |           |              |       x       |
|                   gre.proto                   |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                  gre.recurse                  |     3     |   egress  |    x    |     x     |           |              |       x       |
|                     gre.s                     |     1     |   egress  |    x    |     x     |           |              |       x       |
|                    gre.ver                    |     3     |   egress  |    x    |     x     |           |              |       x       |
|           hash_metadata.entropy_hash          |     16    |   egress  |    x    |           |     x     |      x       |               |
|          i2e_metadata.ingress_tstamp          |     32    |   egress  |    x    |     x     |     x     |      x       |               |
|         i2e_metadata.ingress_tstamp_hi        |     16    |   egress  |    x    |           |     x     |      x       |               |
|         i2e_metadata.mirror_session_id        |     16    |   egress  |    x    |     x     |     x     |      x       |       x       |
|                icmp.hdrChecksum               |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                 icmp.typeCode                 |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|         ig_intr_md_for_tm.packet_color        |     2     |   egress  |    x    |           |     x     |      x       |               |
|                igmp.hdrChecksum               |     16    |   egress  |    x    |     x     |           |              |               |
|                 igmp.typeCode                 |     16    |   egress  |    x    |     x     |           |              |               |
|              ingress_metadata.bd              |     14    |   egress  |    x    |     x     |     x     |      x       |               |
|        ingress_metadata.egress_ifindex        |     14    |   egress  |    x    |           |     x     |      x       |       x       |
|            ingress_metadata.ifindex           |     14    |   egress  |    x    |     x     |     x     |      x       |               |
|         ingress_metadata.ingress_port         |     9     |   egress  |    x    |     x     |     x     |      x       |               |
|           ingress_metadata.outer_bd           |     14    |   egress  |         |           |     x     |      x       |               |
|             inner_ethernet.dstAddr            |     48    |   egress  |    x    |     x     |           |      x       |       x       |
|            inner_ethernet.etherType           |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|             inner_ethernet.srcAddr            |     48    |   egress  |    x    |     x     |           |      x       |       x       |
|             inner_icmp.hdrChecksum            |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|              inner_icmp.typeCode              |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|              inner_ipv4.diffserv              |     8     |   egress  |    x    |     x     |           |      x       |       x       |
|               inner_ipv4.dstAddr              |     32    |   egress  |    x    |     x     |           |      x       |       x       |
|                inner_ipv4.flags               |     3     |   egress  |    x    |     x     |           |      x       |       x       |
|             inner_ipv4.fragOffset             |     13    |   egress  |    x    |     x     |           |      x       |       x       |
|             inner_ipv4.hdrChecksum            |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|           inner_ipv4.identification           |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                 inner_ipv4.ihl                |     4     |   egress  |    x    |     x     |           |      x       |       x       |
|              inner_ipv4.protocol              |     8     |   egress  |    x    |     x     |           |      x       |       x       |
|               inner_ipv4.srcAddr              |     32    |   egress  |    x    |     x     |           |      x       |       x       |
|              inner_ipv4.totalLen              |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                 inner_ipv4.ttl                |     8     |   egress  |    x    |     x     |           |      x       |       x       |
|               inner_ipv4.version              |     4     |   egress  |    x    |     x     |           |      x       |       x       |
|               inner_ipv6.dstAddr              |    128    |   egress  |    x    |     x     |           |      x       |       x       |
|              inner_ipv6.flowLabel             |     20    |   egress  |    x    |     x     |           |      x       |       x       |
|              inner_ipv6.hopLimit              |     8     |   egress  |    x    |     x     |           |      x       |       x       |
|               inner_ipv6.nextHdr              |     8     |   egress  |    x    |     x     |           |      x       |       x       |
|             inner_ipv6.payloadLen             |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|               inner_ipv6.srcAddr              |    128    |   egress  |    x    |     x     |           |      x       |       x       |
|            inner_ipv6.trafficClass            |     8     |   egress  |    x    |     x     |           |      x       |       x       |
|               inner_ipv6.version              |     4     |   egress  |    x    |     x     |           |      x       |       x       |
|                inner_tcp.ackNo                |     32    |   egress  |    x    |     x     |           |      x       |       x       |
|               inner_tcp.checksum              |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|              inner_tcp.dataOffset             |     4     |   egress  |    x    |     x     |           |      x       |       x       |
|               inner_tcp.dstPort               |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                inner_tcp.flags                |     8     |   egress  |    x    |     x     |           |      x       |       x       |
|                 inner_tcp.res                 |     4     |   egress  |    x    |     x     |           |      x       |       x       |
|                inner_tcp.seqNo                |     32    |   egress  |    x    |     x     |           |      x       |       x       |
|               inner_tcp.srcPort               |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|              inner_tcp.urgentPtr              |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                inner_tcp.window               |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|               inner_udp.checksum              |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|               inner_udp.dstPort               |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|               inner_udp.length_               |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|               inner_udp.srcPort               |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                 ipv4.diffserv                 |     8     |   egress  |    x    |     x     |           |      x       |       x       |
|                  ipv4.dstAddr                 |     32    |   egress  |    x    |     x     |           |      x       |       x       |
|                   ipv4.flags                  |     3     |   egress  |    x    |     x     |           |      x       |       x       |
|                ipv4.fragOffset                |     13    |   egress  |    x    |     x     |           |      x       |       x       |
|                ipv4.hdrChecksum               |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|              ipv4.identification              |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                    ipv4.ihl                   |     4     |   egress  |    x    |     x     |           |      x       |       x       |
|                 ipv4.protocol                 |     8     |   egress  |    x    |     x     |           |      x       |       x       |
|                  ipv4.srcAddr                 |     32    |   egress  |    x    |     x     |           |      x       |       x       |
|                 ipv4.totalLen                 |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                    ipv4.ttl                   |     8     |   egress  |    x    |     x     |           |      x       |       x       |
|                  ipv4.version                 |     4     |   egress  |    x    |     x     |           |      x       |       x       |
|         ipv4_option_32b.option_fields         |     32    |   egress  |    x    |     x     |           |              |               |
|                  ipv6.dstAddr                 |    128    |   egress  |    x    |     x     |           |      x       |       x       |
|                 ipv6.flowLabel                |     20    |   egress  |    x    |     x     |           |      x       |       x       |
|                 ipv6.hopLimit                 |     8     |   egress  |    x    |     x     |           |      x       |       x       |
|                  ipv6.nextHdr                 |     8     |   egress  |    x    |     x     |           |      x       |       x       |
|                ipv6.payloadLen                |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                  ipv6.srcAddr                 |    128    |   egress  |    x    |     x     |           |      x       |       x       |
|               ipv6.trafficClass               |     8     |   egress  |    x    |     x     |           |      x       |       x       |
|                  ipv6.version                 |     4     |   egress  |    x    |     x     |           |      x       |       x       |
|            l2_metadata.lkp_pkt_type           |     3     |   egress  |    x    |           |     x     |      x       |               |
|            l3_metadata.l3_mtu_check           |     16    |   egress  |         |           |     x     |      x       |       x       |
|             l3_metadata.mtu_index             |     8     |   egress  |         |           |     x     |      x       |       x       |
|           l3_metadata.nexthop_index           |     16    |   egress  |    x    |           |     x     |      x       |               |
|            l3_metadata.outer_routed           |     1     |   egress  |         |           |     x     |      x       |               |
|               l3_metadata.routed              |     1     |   egress  |    x    |           |     x     |      x       |               |
|                l3_metadata.vrf                |     14    |   egress  |    x    |           |     x     |      x       |               |
|              llc_header.control_              |     8     |   egress  |    x    |     x     |           |              |               |
|                llc_header.dsap                |     8     |   egress  |    x    |     x     |           |              |               |
|                llc_header.ssap                |     8     |   egress  |    x    |     x     |           |              |               |
|                  mpls[0].bos                  |     1     |   egress  |    x    |     x     |           |      x       |       x       |
|                  mpls[0].exp                  |     3     |   egress  |    x    |     x     |           |      x       |       x       |
|                 mpls[0].label                 |     20    |   egress  |    x    |     x     |           |      x       |       x       |
|                  mpls[0].ttl                  |     8     |   egress  |    x    |     x     |           |      x       |       x       |
|                  mpls[1].bos                  |     1     |   egress  |    x    |     x     |           |      x       |       x       |
|                  mpls[1].exp                  |     3     |   egress  |    x    |     x     |           |      x       |       x       |
|                 mpls[1].label                 |     20    |   egress  |    x    |     x     |           |      x       |       x       |
|                  mpls[1].ttl                  |     8     |   egress  |    x    |     x     |           |      x       |       x       |
|                  mpls[2].bos                  |     1     |   egress  |    x    |     x     |           |              |       x       |
|                  mpls[2].exp                  |     3     |   egress  |    x    |     x     |           |              |       x       |
|                 mpls[2].label                 |     20    |   egress  |    x    |     x     |           |              |       x       |
|                  mpls[2].ttl                  |     8     |   egress  |    x    |     x     |           |              |       x       |
|        multicast_metadata.inner_replica       |     1     |   egress  |         |           |     x     |      x       |       x       |
|           multicast_metadata.replica          |     1     |   egress  |         |           |     x     |      x       |       x       |
|                 nvgre.flow_id                 |     8     |   egress  |    x    |     x     |           |              |       x       |
|                   nvgre.tni                   |     24    |   egress  |    x    |     x     |           |              |       x       |
|                snap_header.oui                |     24    |   egress  |    x    |     x     |           |              |               |
|               snap_header.type_               |     16    |   egress  |    x    |     x     |           |              |               |
|                   tcp.ackNo                   |     32    |   egress  |    x    |     x     |           |      x       |       x       |
|                  tcp.checksum                 |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                 tcp.dataOffset                |     4     |   egress  |    x    |     x     |           |      x       |       x       |
|                  tcp.dstPort                  |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                   tcp.flags                   |     8     |   egress  |    x    |     x     |           |      x       |       x       |
|                    tcp.res                    |     4     |   egress  |    x    |     x     |           |      x       |       x       |
|                   tcp.seqNo                   |     32    |   egress  |    x    |     x     |           |      x       |       x       |
|                  tcp.srcPort                  |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                 tcp.urgentPtr                 |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                   tcp.window                  |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|      tunnel_metadata.egress_header_count      |     4     |   egress  |         |           |     x     |      x       |       x       |
|       tunnel_metadata.egress_tunnel_type      |     5     |   egress  |         |           |     x     |      x       |       x       |
|      tunnel_metadata.ingress_tunnel_type      |     5     |   egress  |    x    |           |     x     |      x       |               |
|         tunnel_metadata.inner_ip_proto        |     8     |   egress  |         |           |     x     |      x       |       x       |
|       tunnel_metadata.tunnel_dmac_index       |     12    |   egress  |         |           |     x     |      x       |       x       |
|        tunnel_metadata.tunnel_dst_index       |     16    |   egress  |    x    |           |     x     |      x       |               |
|          tunnel_metadata.tunnel_index         |     14    |   egress  |         |           |     x     |      x       |       x       |
|       tunnel_metadata.tunnel_smac_index       |     8     |   egress  |         |           |     x     |      x       |       x       |
|        tunnel_metadata.tunnel_terminate       |     1     |   egress  |    x    |           |     x     |      x       |               |
|              tunnel_metadata.vnid             |     24    |   egress  |         |           |     x     |      x       |       x       |
|                  udp.checksum                 |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                  udp.dstPort                  |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                  udp.length_                  |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                  udp.srcPort                  |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                vlan_tag_[0].cfi               |     1     |   egress  |    x    |     x     |           |              |               |
|             vlan_tag_[0].etherType            |     16    |   egress  |    x    |     x     |           |      x       |       x       |
|                vlan_tag_[0].pcp               |     3     |   egress  |    x    |     x     |           |              |       x       |
|                vlan_tag_[0].vid               |     12    |   egress  |    x    |     x     |           |              |       x       |
|                vlan_tag_[1].cfi               |     1     |   egress  |    x    |     x     |           |              |               |
|             vlan_tag_[1].etherType            |     16    |   egress  |    x    |     x     |           |              |               |
|                vlan_tag_[1].pcp               |     3     |   egress  |    x    |     x     |           |              |               |
|                vlan_tag_[1].vid               |     12    |   egress  |    x    |     x     |           |              |               |
|                  vxlan.flags                  |     8     |   egress  |    x    |     x     |           |              |       x       |
|                 vxlan.reserved                |     24    |   egress  |    x    |     x     |           |              |       x       |
|                vxlan.reserved2                |     8     |   egress  |    x    |     x     |           |              |       x       |
|                   vxlan.vni                   |     24    |   egress  |    x    |     x     |           |              |       x       |
|       --validity_check--erspan_t3_header      |     1     |  ingress  |    x    |     x     |           |              |               |
|           --validity_check--ethernet          |     1     |  ingress  |    x    |     x     |           |      x       |               |
|        --validity_check--fabric_header        |     1     |  ingress  |    x    |     x     |           |              |       x       |
|      --validity_check--fabric_header_cpu      |     1     |  ingress  |    x    |     x     |           |      x       |       x       |
|   --validity_check--fabric_header_timestamp   |     1     |  ingress  |    x    |     x     |           |              |               |
|    --validity_check--fabric_payload_header    |     1     |  ingress  |    x    |     x     |           |              |       x       |
|             --validity_check--genv            |     1     |  ingress  |    x    |     x     |           |              |               |
|             --validity_check--gre             |     1     |  ingress  |    x    |     x     |           |              |               |
|             --validity_check--icmp            |     1     |  ingress  |    x    |     x     |           |              |               |
|             --validity_check--igmp            |     1     |  ingress  |    x    |     x     |           |              |               |
|        --validity_check--inner_ethernet       |     1     |  ingress  |    x    |     x     |           |              |               |
|          --validity_check--inner_icmp         |     1     |  ingress  |    x    |     x     |           |              |               |
|          --validity_check--inner_ipv4         |     1     |  ingress  |    x    |     x     |           |      x       |               |
|          --validity_check--inner_ipv6         |     1     |  ingress  |    x    |     x     |           |      x       |               |
|          --validity_check--inner_tcp          |     1     |  ingress  |    x    |     x     |           |              |               |
|          --validity_check--inner_udp          |     1     |  ingress  |    x    |     x     |           |              |               |
|             --validity_check--ipv4            |     1     |  ingress  |    x    |     x     |           |      x       |               |
|       --validity_check--ipv4_option_32b       |     1     |  ingress  |    x    |     x     |           |              |               |
|             --validity_check--ipv6            |     1     |  ingress  |    x    |     x     |           |      x       |               |
|          --validity_check--llc_header         |     1     |  ingress  |    x    |     x     |           |              |               |
|       --validity_check--metadata_bridge       |     1     |  ingress  |    x    |     x     |           |              |               |
|           --validity_check--mpls[0]           |     1     |  ingress  |    x    |     x     |           |      x       |               |
|           --validity_check--mpls[1]           |     1     |  ingress  |    x    |     x     |           |              |               |
|           --validity_check--mpls[2]           |     1     |  ingress  |    x    |     x     |           |              |               |
|            --validity_check--nvgre            |     1     |  ingress  |    x    |     x     |           |              |               |
|         --validity_check--snap_header         |     1     |  ingress  |    x    |     x     |           |              |               |
|             --validity_check--tcp             |     1     |  ingress  |    x    |     x     |           |              |               |
|             --validity_check--udp             |     1     |  ingress  |    x    |     x     |           |              |               |
|         --validity_check--vlan_tag_[0]        |     1     |  ingress  |    x    |     x     |           |      x       |               |
|         --validity_check--vlan_tag_[1]        |     1     |  ingress  |    x    |     x     |           |              |               |
|            --validity_check--vxlan            |     1     |  ingress  |    x    |     x     |           |              |               |
|             acl_metadata.acl_deny             |     1     |  ingress  |         |           |     x     |      x       |       x       |
|            acl_metadata.acl_nexthop           |     16    |  ingress  |         |           |     x     |      x       |       x       |
|         acl_metadata.acl_nexthop_type         |     1     |  ingress  |         |           |     x     |      x       |       x       |
|           acl_metadata.acl_redirect           |     1     |  ingress  |         |           |     x     |      x       |       x       |
|          acl_metadata.acl_stats_index         |     12    |  ingress  |         |           |     x     |      x       |       x       |
|             acl_metadata.bd_label             |     16    |  ingress  |         |           |     x     |      x       |       x       |
|     acl_metadata.ingress_dst_port_range_id    |     8     |  ingress  |         |           |     x     |      x       |       x       |
|     acl_metadata.ingress_src_port_range_id    |     8     |  ingress  |         |           |     x     |      x       |       x       |
|          acl_metadata.port_lag_label          |     16    |  ingress  |         |           |     x     |      x       |       x       |
|             acl_metadata.racl_deny            |     1     |  ingress  |         |           |     x     |      x       |       x       |
|           acl_metadata.racl_nexthop           |     16    |  ingress  |         |           |     x     |      x       |       x       |
|         acl_metadata.racl_nexthop_type        |     1     |  ingress  |         |           |     x     |      x       |       x       |
|           acl_metadata.racl_redirect          |     1     |  ingress  |         |           |     x     |      x       |       x       |
|         acl_metadata.racl_stats_index         |     12    |  ingress  |         |           |     x     |      x       |       x       |
|             egress_metadata.bypass            |     1     |  ingress  |         |     x     |     x     |              |       x       |
|      egress_metadata.capture_tstamp_on_tx     |     1     |  ingress  |         |     x     |     x     |              |       x       |
|          erspan_t3_header.ft_d_other          |     16    |  ingress  |         |           |           |              |               |
|       erspan_t3_header.priority_span_id       |     16    |  ingress  |         |           |           |              |               |
|              erspan_t3_header.sgt             |     16    |  ingress  |         |           |           |              |               |
|           erspan_t3_header.timestamp          |     32    |  ingress  |         |           |           |              |               |
|            erspan_t3_header.version           |     4     |  ingress  |         |           |           |              |               |
|             erspan_t3_header.vlan             |     12    |  ingress  |         |           |           |              |               |
|                ethernet.dstAddr               |     48    |  ingress  |    x    |     x     |           |      x       |               |
|               ethernet.etherType              |     16    |  ingress  |    x    |     x     |           |      x       |       x       |
|                ethernet.srcAddr               |     48    |  ingress  |    x    |     x     |           |      x       |               |
|            fabric_header.dstDevice            |     8     |  ingress  |    x    |     x     |           |      x       |               |
|          fabric_header.dstPortOrGroup         |     16    |  ingress  |    x    |     x     |           |      x       |               |
|           fabric_header.fabricColor           |     3     |  ingress  |    x    |     x     |           |              |               |
|            fabric_header.fabricQos            |     5     |  ingress  |    x    |     x     |           |              |               |
|          fabric_header.headerVersion          |     2     |  ingress  |    x    |     x     |           |              |               |
|            fabric_header.packetType           |     3     |  ingress  |    x    |     x     |           |              |               |
|          fabric_header.packetVersion          |     2     |  ingress  |    x    |     x     |           |              |               |
|               fabric_header.pad1              |     1     |  ingress  |    x    |     x     |           |              |               |
|     fabric_header_cpu.capture_tstamp_on_tx    |     1     |  ingress  |    x    |     x     |           |      x       |               |
|         fabric_header_cpu.egressQueue         |     5     |  ingress  |    x    |     x     |           |              |               |
|          fabric_header_cpu.ingressBd          |     16    |  ingress  |    x    |     x     |           |      x       |               |
|        fabric_header_cpu.ingressIfindex       |     16    |  ingress  |    x    |     x     |           |              |               |
|         fabric_header_cpu.ingressPort         |     16    |  ingress  |    x    |     x     |           |              |               |
|          fabric_header_cpu.reasonCode         |     16    |  ingress  |    x    |     x     |           |              |               |
|           fabric_header_cpu.reserved          |     1     |  ingress  |    x    |     x     |           |              |               |
|           fabric_header_cpu.txBypass          |     1     |  ingress  |    x    |     x     |           |      x       |               |
|      fabric_header_timestamp.arrival_time     |     32    |  ingress  |    x    |     x     |           |              |               |
|    fabric_header_timestamp.arrival_time_hi    |     16    |  ingress  |    x    |     x     |           |              |               |
|          fabric_metadata.reason_code          |     16    |  ingress  |         |     x     |     x     |      x       |       x       |
|        fabric_payload_header.etherType        |     16    |  ingress  |    x    |     x     |           |      x       |               |
|                 genv.critical                 |     1     |  ingress  |    x    |     x     |           |              |               |
|                    genv.oam                   |     1     |  ingress  |    x    |     x     |           |              |               |
|                  genv.optLen                  |     6     |  ingress  |    x    |     x     |           |              |               |
|                 genv.protoType                |     16    |  ingress  |    x    |     x     |           |              |               |
|                 genv.reserved                 |     6     |  ingress  |    x    |     x     |           |              |               |
|                 genv.reserved2                |     8     |  ingress  |    x    |     x     |           |              |               |
|                    genv.ver                   |     2     |  ingress  |    x    |     x     |           |              |               |
|                    genv.vni                   |     24    |  ingress  |    x    |     x     |           |              |               |
|                     gre.C                     |     1     |  ingress  |    x    |     x     |           |              |               |
|                     gre.K                     |     1     |  ingress  |    x    |     x     |           |              |               |
|                     gre.R                     |     1     |  ingress  |    x    |     x     |           |              |               |
|                     gre.S                     |     1     |  ingress  |    x    |     x     |           |              |               |
|                   gre.flags                   |     5     |  ingress  |    x    |     x     |           |              |               |
|                   gre.proto                   |     16    |  ingress  |    x    |     x     |           |              |               |
|                  gre.recurse                  |     3     |  ingress  |    x    |     x     |           |              |               |
|                     gre.s                     |     1     |  ingress  |    x    |     x     |           |              |               |
|                    gre.ver                    |     3     |  ingress  |    x    |     x     |           |              |               |
|           hash_metadata.entropy_hash          |     16    |  ingress  |         |     x     |     x     |              |       x       |
|              hash_metadata.hash1              |     16    |  ingress  |         |           |     x     |      x       |       x       |
|              hash_metadata.hash2              |     16    |  ingress  |         |           |     x     |      x       |       x       |
|          i2e_metadata.ingress_tstamp          |     32    |  ingress  |         |     x     |     x     |      x       |       x       |
|         i2e_metadata.ingress_tstamp_hi        |     16    |  ingress  |         |     x     |     x     |              |       x       |
|         i2e_metadata.mirror_session_id        |     16    |  ingress  |         |     x     |     x     |      x       |       x       |
|                icmp.hdrChecksum               |     16    |  ingress  |    x    |     x     |           |              |               |
|                 icmp.typeCode                 |     16    |  ingress  |    x    |     x     |           |              |               |
|                ig_intr_md._pad1               |     1     |  ingress  |    x    |           |     x     |              |               |
|                ig_intr_md._pad2               |     2     |  ingress  |    x    |           |     x     |              |               |
|                ig_intr_md._pad3               |     3     |  ingress  |    x    |           |     x     |              |               |
|         ig_intr_md.ingress_mac_tstamp         |     48    |  ingress  |    x    |           |     x     |      x       |               |
|            ig_intr_md.ingress_port            |     9     |  ingress  |    x    |           |     x     |      x       |               |
|            ig_intr_md.resubmit_flag           |     1     |  ingress  |    x    |           |     x     |      x       |               |
|      ig_intr_md_for_mb.ingress_mirror_id      |     10    |  ingress  |         |     x     |     x     |              |       x       |
|         ig_intr_md_for_tm.copy_to_cpu         |     1     |  ingress  |         |     x     |     x     |              |       x       |
|    ig_intr_md_for_tm.disable_ucast_cutthru    |     1     |  ingress  |         |     x     |     x     |              |       x       |
|           ig_intr_md_for_tm.drop_ctl          |     3     |  ingress  |         |     x     |     x     |              |       x       |
|         ig_intr_md_for_tm.ingress_cos         |     3     |  ingress  |         |     x     |     x     |              |       x       |
|     ig_intr_md_for_tm.level1_exclusion_id     |     16    |  ingress  |         |     x     |     x     |              |       x       |
|      ig_intr_md_for_tm.level1_mcast_hash      |     13    |  ingress  |         |     x     |     x     |              |       x       |
|     ig_intr_md_for_tm.level2_exclusion_id     |     9     |  ingress  |         |     x     |     x     |              |       x       |
|      ig_intr_md_for_tm.level2_mcast_hash      |     13    |  ingress  |         |     x     |     x     |              |       x       |
|         ig_intr_md_for_tm.mcast_grp_b         |     16    |  ingress  |         |     x     |     x     |              |       x       |
|         ig_intr_md_for_tm.packet_color        |     2     |  ingress  |         |     x     |     x     |              |       x       |
|             ig_intr_md_for_tm.qid             |     5     |  ingress  |         |     x     |     x     |              |       x       |
|             ig_intr_md_for_tm.rid             |     16    |  ingress  |         |     x     |     x     |              |       x       |
|      ig_intr_md_for_tm.ucast_egress_port      |     9     |  ingress  |         |     x     |     x     |              |       x       |
| ig_intr_md_from_parser_aux.ingress_parser_err |     16    |  ingress  |    x    |           |     x     |      x       |               |
|                igmp.hdrChecksum               |     16    |  ingress  |    x    |     x     |           |              |               |
|                 igmp.typeCode                 |     16    |  ingress  |    x    |     x     |           |              |               |
|              ingress_metadata.bd              |     14    |  ingress  |         |     x     |     x     |      x       |       x       |
|        ingress_metadata.bypass_lookups        |     8     |  ingress  |    x    |           |     x     |      x       |               |
|           ingress_metadata.drop_flag          |     1     |  ingress  |         |           |     x     |      x       |       x       |
|          ingress_metadata.drop_reason         |     8     |  ingress  |         |           |     x     |      x       |       x       |
|        ingress_metadata.egress_ifindex        |     14    |  ingress  |         |     x     |     x     |      x       |       x       |
|     ingress_metadata.egress_port_lag_index    |     10    |  ingress  |         |           |     x     |      x       |       x       |
|            ingress_metadata.ifindex           |     14    |  ingress  |         |     x     |     x     |      x       |       x       |
|         ingress_metadata.ingress_port         |     9     |  ingress  |         |     x     |     x     |              |       x       |
|        ingress_metadata.port_lag_index        |     10    |  ingress  |         |           |     x     |      x       |       x       |
|           ingress_metadata.port_type          |     2     |  ingress  |         |           |     x     |      x       |       x       |
|             inner_ethernet.dstAddr            |     48    |  ingress  |    x    |     x     |           |              |               |
|            inner_ethernet.etherType           |     16    |  ingress  |    x    |     x     |           |      x       |               |
|             inner_ethernet.srcAddr            |     48    |  ingress  |    x    |     x     |           |              |               |
|             inner_icmp.hdrChecksum            |     16    |  ingress  |    x    |     x     |           |              |               |
|              inner_icmp.typeCode              |     16    |  ingress  |    x    |     x     |           |              |               |
|              inner_ipv4.diffserv              |     8     |  ingress  |    x    |     x     |           |              |               |
|               inner_ipv4.dstAddr              |     32    |  ingress  |    x    |     x     |           |              |               |
|                inner_ipv4.flags               |     3     |  ingress  |    x    |     x     |           |              |               |
|             inner_ipv4.fragOffset             |     13    |  ingress  |    x    |     x     |           |              |               |
|             inner_ipv4.hdrChecksum            |     16    |  ingress  |    x    |     x     |           |              |               |
|           inner_ipv4.identification           |     16    |  ingress  |    x    |     x     |           |              |               |
|                 inner_ipv4.ihl                |     4     |  ingress  |    x    |     x     |           |      x       |               |
|              inner_ipv4.protocol              |     8     |  ingress  |    x    |     x     |           |              |               |
|               inner_ipv4.srcAddr              |     32    |  ingress  |    x    |     x     |           |              |               |
|              inner_ipv4.totalLen              |     16    |  ingress  |    x    |     x     |           |              |               |
|                 inner_ipv4.ttl                |     8     |  ingress  |    x    |     x     |           |              |               |
|               inner_ipv4.version              |     4     |  ingress  |    x    |     x     |           |      x       |               |
|               inner_ipv6.dstAddr              |    128    |  ingress  |    x    |     x     |           |              |               |
|              inner_ipv6.flowLabel             |     20    |  ingress  |    x    |     x     |           |              |               |
|              inner_ipv6.hopLimit              |     8     |  ingress  |    x    |     x     |           |              |               |
|               inner_ipv6.nextHdr              |     8     |  ingress  |    x    |     x     |           |              |               |
|             inner_ipv6.payloadLen             |     16    |  ingress  |    x    |     x     |           |              |               |
|               inner_ipv6.srcAddr              |    128    |  ingress  |    x    |     x     |           |              |               |
|            inner_ipv6.trafficClass            |     8     |  ingress  |    x    |     x     |           |              |               |
|               inner_ipv6.version              |     4     |  ingress  |    x    |     x     |           |      x       |               |
|                inner_tcp.ackNo                |     32    |  ingress  |    x    |     x     |           |              |               |
|               inner_tcp.checksum              |     16    |  ingress  |    x    |     x     |           |              |               |
|              inner_tcp.dataOffset             |     4     |  ingress  |    x    |     x     |           |              |               |
|               inner_tcp.dstPort               |     16    |  ingress  |    x    |     x     |           |              |               |
|                inner_tcp.flags                |     8     |  ingress  |    x    |     x     |           |              |               |
|                 inner_tcp.res                 |     4     |  ingress  |    x    |     x     |           |              |               |
|                inner_tcp.seqNo                |     32    |  ingress  |    x    |     x     |           |              |               |
|               inner_tcp.srcPort               |     16    |  ingress  |    x    |     x     |           |              |               |
|              inner_tcp.urgentPtr              |     16    |  ingress  |    x    |     x     |           |              |               |
|                inner_tcp.window               |     16    |  ingress  |    x    |     x     |           |              |               |
|               inner_udp.checksum              |     16    |  ingress  |    x    |     x     |           |              |               |
|               inner_udp.dstPort               |     16    |  ingress  |    x    |     x     |           |              |               |
|               inner_udp.length_               |     16    |  ingress  |    x    |     x     |           |              |               |
|               inner_udp.srcPort               |     16    |  ingress  |    x    |     x     |           |              |               |
|                 ipv4.diffserv                 |     8     |  ingress  |    x    |     x     |           |              |               |
|                  ipv4.dstAddr                 |     32    |  ingress  |    x    |     x     |           |      x       |               |
|                   ipv4.flags                  |     3     |  ingress  |    x    |     x     |           |              |               |
|                ipv4.fragOffset                |     13    |  ingress  |    x    |     x     |           |              |               |
|                ipv4.hdrChecksum               |     16    |  ingress  |    x    |     x     |           |              |               |
|              ipv4.identification              |     16    |  ingress  |    x    |     x     |           |              |               |
|                    ipv4.ihl                   |     4     |  ingress  |    x    |     x     |           |      x       |               |
|                 ipv4.protocol                 |     8     |  ingress  |    x    |     x     |           |      x       |               |
|                  ipv4.srcAddr                 |     32    |  ingress  |    x    |     x     |           |      x       |               |
|                 ipv4.totalLen                 |     16    |  ingress  |    x    |     x     |           |              |               |
|                    ipv4.ttl                   |     8     |  ingress  |    x    |     x     |           |      x       |               |
|                  ipv4.version                 |     4     |  ingress  |    x    |     x     |           |      x       |               |
|       ipv4_metadata.ipv4_unicast_enabled      |     1     |  ingress  |         |           |     x     |      x       |       x       |
|          ipv4_metadata.ipv4_urpf_mode         |     2     |  ingress  |         |           |     x     |      x       |       x       |
|           ipv4_metadata.lkp_ipv4_da           |     32    |  ingress  |    x    |           |     x     |      x       |       x       |
|           ipv4_metadata.lkp_ipv4_sa           |     32    |  ingress  |    x    |           |     x     |      x       |       x       |
|         ipv4_option_32b.option_fields         |     32    |  ingress  |    x    |     x     |           |              |               |
|                  ipv6.dstAddr                 |    128    |  ingress  |    x    |     x     |           |      x       |               |
|                 ipv6.flowLabel                |     20    |  ingress  |    x    |     x     |           |              |               |
|                 ipv6.hopLimit                 |     8     |  ingress  |    x    |     x     |           |      x       |               |
|                  ipv6.nextHdr                 |     8     |  ingress  |    x    |     x     |           |      x       |               |
|                ipv6.payloadLen                |     16    |  ingress  |    x    |     x     |           |              |               |
|                  ipv6.srcAddr                 |    128    |  ingress  |    x    |     x     |           |      x       |               |
|               ipv6.trafficClass               |     8     |  ingress  |    x    |     x     |           |              |               |
|                  ipv6.version                 |     4     |  ingress  |    x    |     x     |           |      x       |               |
|      ipv6_metadata.ipv6_src_is_link_local     |     1     |  ingress  |         |           |     x     |      x       |       x       |
|       ipv6_metadata.ipv6_unicast_enabled      |     1     |  ingress  |         |           |     x     |      x       |       x       |
|          ipv6_metadata.ipv6_urpf_mode         |     2     |  ingress  |         |           |     x     |      x       |       x       |
|           ipv6_metadata.lkp_ipv6_da           |    128    |  ingress  |    x    |           |     x     |      x       |       x       |
|           ipv6_metadata.lkp_ipv6_sa           |    128    |  ingress  |    x    |           |     x     |      x       |       x       |
|             l2_metadata.arp_opcode            |     2     |  ingress  |    x    |           |     x     |      x       |               |
|            l2_metadata.bd_stats_idx           |     14    |  ingress  |         |           |     x     |      x       |       x       |
|            l2_metadata.l2_dst_miss            |     1     |  ingress  |         |           |     x     |      x       |       x       |
|             l2_metadata.l2_nexthop            |     16    |  ingress  |         |           |     x     |      x       |       x       |
|          l2_metadata.l2_nexthop_type          |     1     |  ingress  |         |           |     x     |      x       |       x       |
|            l2_metadata.l2_redirect            |     1     |  ingress  |         |           |     x     |      x       |       x       |
|            l2_metadata.l2_src_miss            |     1     |  ingress  |         |           |     x     |      x       |       x       |
|            l2_metadata.l2_src_move            |     14    |  ingress  |         |           |     x     |      x       |       x       |
|          l2_metadata.learning_enabled         |     1     |  ingress  |         |           |     x     |      x       |       x       |
|             l2_metadata.lkp_mac_da            |     48    |  ingress  |    x    |           |     x     |      x       |       x       |
|             l2_metadata.lkp_mac_sa            |     48    |  ingress  |    x    |     x     |     x     |      x       |       x       |
|            l2_metadata.lkp_mac_type           |     16    |  ingress  |         |           |     x     |      x       |       x       |
|            l2_metadata.lkp_pkt_type           |     3     |  ingress  |         |     x     |     x     |      x       |       x       |
|           l2_metadata.non_ip_packet           |     1     |  ingress  |         |           |     x     |      x       |       x       |
|       l2_metadata.port_learning_enabled       |     1     |  ingress  |         |           |     x     |      x       |       x       |
|       l2_metadata.port_vlan_mapping_miss      |     1     |  ingress  |         |           |     x     |      x       |       x       |
|           l2_metadata.same_if_check           |     14    |  ingress  |         |           |     x     |      x       |       x       |
|             l2_metadata.stp_group             |     10    |  ingress  |         |           |     x     |      x       |       x       |
|             l2_metadata.stp_state             |     3     |  ingress  |         |           |     x     |      x       |       x       |
|              l3_metadata.fib_hit              |     1     |  ingress  |         |           |     x     |      x       |       x       |
|            l3_metadata.fib_hit_myip           |     1     |  ingress  |         |           |     x     |      x       |       x       |
|            l3_metadata.fib_nexthop            |     16    |  ingress  |         |           |     x     |      x       |       x       |
|          l3_metadata.fib_nexthop_type         |     1     |  ingress  |         |           |     x     |      x       |       x       |
|              l3_metadata.l3_copy              |     1     |  ingress  |         |           |     x     |      x       |               |
|            l3_metadata.lkp_ip_llmc            |     1     |  ingress  |         |           |     x     |      x       |       x       |
|             l3_metadata.lkp_ip_mc             |     1     |  ingress  |         |           |     x     |      x       |       x       |
|            l3_metadata.lkp_ip_proto           |     8     |  ingress  |    x    |           |     x     |      x       |       x       |
|             l3_metadata.lkp_ip_ttl            |     8     |  ingress  |    x    |           |     x     |      x       |       x       |
|            l3_metadata.lkp_ip_type            |     2     |  ingress  |         |           |     x     |      x       |       x       |
|           l3_metadata.lkp_ip_version          |     4     |  ingress  |         |           |     x     |      x       |       x       |
|            l3_metadata.lkp_l4_dport           |     16    |  ingress  |    x    |           |     x     |      x       |       x       |
|            l3_metadata.lkp_l4_sport           |     16    |  ingress  |    x    |           |     x     |      x       |       x       |
|         l3_metadata.lkp_outer_l4_dport        |     16    |  ingress  |    x    |           |     x     |      x       |               |
|         l3_metadata.lkp_outer_l4_sport        |     16    |  ingress  |    x    |           |     x     |      x       |               |
|        l3_metadata.lkp_outer_tcp_flags        |     8     |  ingress  |    x    |           |     x     |      x       |               |
|           l3_metadata.lkp_tcp_flags           |     8     |  ingress  |    x    |           |     x     |      x       |       x       |
|           l3_metadata.nexthop_index           |     16    |  ingress  |         |     x     |     x     |      x       |       x       |
|             l3_metadata.rmac_group            |     10    |  ingress  |         |           |     x     |      x       |       x       |
|              l3_metadata.rmac_hit             |     1     |  ingress  |         |           |     x     |      x       |       x       |
|               l3_metadata.routed              |     1     |  ingress  |         |     x     |     x     |      x       |       x       |
|           l3_metadata.same_bd_check           |     14    |  ingress  |         |           |     x     |      x       |       x       |
|           l3_metadata.urpf_bd_group           |     14    |  ingress  |         |           |     x     |      x       |       x       |
|          l3_metadata.urpf_check_fail          |     1     |  ingress  |         |           |     x     |      x       |       x       |
|              l3_metadata.urpf_hit             |     1     |  ingress  |         |           |     x     |      x       |       x       |
|             l3_metadata.urpf_mode             |     2     |  ingress  |         |           |     x     |      x       |       x       |
|                l3_metadata.vrf                |     14    |  ingress  |         |     x     |     x     |      x       |       x       |
|              llc_header.control_              |     8     |  ingress  |    x    |     x     |           |              |               |
|                llc_header.dsap                |     8     |  ingress  |    x    |     x     |           |              |               |
|                llc_header.ssap                |     8     |  ingress  |    x    |     x     |           |              |               |
|       meter_metadata.storm_control_color      |     2     |  ingress  |         |           |     x     |      x       |       x       |
|                  mpls[0].bos                  |     1     |  ingress  |    x    |     x     |           |              |               |
|                  mpls[0].exp                  |     3     |  ingress  |    x    |     x     |           |              |               |
|                 mpls[0].label                 |     20    |  ingress  |    x    |     x     |           |      x       |               |
|                  mpls[0].ttl                  |     8     |  ingress  |    x    |     x     |           |              |               |
|                  mpls[1].bos                  |     1     |  ingress  |    x    |     x     |           |              |               |
|                  mpls[1].exp                  |     3     |  ingress  |    x    |     x     |           |              |               |
|                 mpls[1].label                 |     20    |  ingress  |    x    |     x     |           |              |               |
|                  mpls[1].ttl                  |     8     |  ingress  |    x    |     x     |           |              |               |
|                  mpls[2].bos                  |     1     |  ingress  |    x    |     x     |           |              |               |
|                  mpls[2].exp                  |     3     |  ingress  |    x    |     x     |           |              |               |
|                 mpls[2].label                 |     20    |  ingress  |    x    |     x     |           |              |               |
|                  mpls[2].ttl                  |     8     |  ingress  |    x    |     x     |           |              |               |
|        multicast_metadata.bd_mrpf_group       |     14    |  ingress  |         |           |     x     |      x       |       x       |
|      multicast_metadata.flood_to_mrouters     |     1     |  ingress  |         |           |     x     |      x       |       x       |
|    multicast_metadata.igmp_snooping_enabled   |     1     |  ingress  |         |           |     x     |      x       |       x       |
|   multicast_metadata.ipv4_multicast_enabled   |     1     |  ingress  |         |           |     x     |      x       |       x       |
|   multicast_metadata.ipv6_multicast_enabled   |     1     |  ingress  |         |           |     x     |      x       |       x       |
|      multicast_metadata.mcast_bridge_hit      |     1     |  ingress  |         |           |     x     |      x       |       x       |
|      multicast_metadata.mcast_copy_to_cpu     |     1     |  ingress  |         |           |     x     |      x       |       x       |
|         multicast_metadata.mcast_mode         |     2     |  ingress  |         |           |     x     |      x       |       x       |
|       multicast_metadata.mcast_route_hit      |     1     |  ingress  |         |           |     x     |      x       |       x       |
|     multicast_metadata.mcast_route_s_g_hit    |     1     |  ingress  |         |           |     x     |      x       |       x       |
|       multicast_metadata.mcast_rpf_fail       |     1     |  ingress  |         |           |     x     |      x       |       x       |
|       multicast_metadata.mcast_rpf_group      |     14    |  ingress  |         |           |     x     |      x       |       x       |
|    multicast_metadata.mld_snooping_enabled    |     1     |  ingress  |         |           |     x     |      x       |       x       |
|  multicast_metadata.multicast_bridge_mc_index |     16    |  ingress  |         |           |     x     |      x       |       x       |
|  multicast_metadata.multicast_route_mc_index  |     16    |  ingress  |         |           |     x     |      x       |       x       |
|         nexthop_metadata.nexthop_glean        |     1     |  ingress  |         |           |     x     |      x       |       x       |
|         nexthop_metadata.nexthop_type         |     1     |  ingress  |         |           |     x     |      x       |       x       |
|                 nvgre.flow_id                 |     8     |  ingress  |    x    |     x     |           |              |               |
|                   nvgre.tni                   |     24    |  ingress  |    x    |     x     |           |              |               |
|                snap_header.oui                |     24    |  ingress  |    x    |     x     |           |              |               |
|               snap_header.type_               |     16    |  ingress  |    x    |     x     |           |              |               |
|                   tcp.ackNo                   |     32    |  ingress  |    x    |     x     |           |              |               |
|                  tcp.checksum                 |     16    |  ingress  |    x    |     x     |           |              |               |
|                 tcp.dataOffset                |     4     |  ingress  |    x    |     x     |           |              |               |
|                  tcp.dstPort                  |     16    |  ingress  |    x    |     x     |           |              |               |
|                   tcp.flags                   |     8     |  ingress  |    x    |     x     |           |      x       |               |
|                    tcp.res                    |     4     |  ingress  |    x    |     x     |           |              |               |
|                   tcp.seqNo                   |     32    |  ingress  |    x    |     x     |           |              |               |
|                  tcp.srcPort                  |     16    |  ingress  |    x    |     x     |           |              |               |
|                 tcp.urgentPtr                 |     16    |  ingress  |    x    |     x     |           |              |               |
|                   tcp.window                  |     16    |  ingress  |    x    |     x     |           |              |               |
|      tunnel_metadata.ingress_tunnel_type      |     5     |  ingress  |    x    |     x     |     x     |      x       |       x       |
|          tunnel_metadata.src_vtep_hit         |     1     |  ingress  |         |           |     x     |      x       |       x       |
|        tunnel_metadata.tunnel_dst_index       |     16    |  ingress  |         |     x     |     x     |      x       |       x       |
|        tunnel_metadata.tunnel_if_check        |     1     |  ingress  |         |           |     x     |      x       |       x       |
|         tunnel_metadata.tunnel_lookup         |     1     |  ingress  |         |           |     x     |      x       |       x       |
|        tunnel_metadata.tunnel_term_type       |     1     |  ingress  |         |           |     x     |      x       |       x       |
|        tunnel_metadata.tunnel_terminate       |     1     |  ingress  |         |     x     |     x     |      x       |       x       |
|           tunnel_metadata.tunnel_vni          |     24    |  ingress  |    x    |           |     x     |      x       |       x       |
|          tunnel_metadata.vtep_ifindex         |     14    |  ingress  |         |           |     x     |      x       |       x       |
|                  udp.checksum                 |     16    |  ingress  |    x    |     x     |           |              |               |
|                  udp.dstPort                  |     16    |  ingress  |    x    |     x     |           |              |               |
|                  udp.length_                  |     16    |  ingress  |    x    |     x     |           |              |               |
|                  udp.srcPort                  |     16    |  ingress  |    x    |     x     |           |              |               |
|                vlan_tag_[0].cfi               |     1     |  ingress  |    x    |     x     |           |              |               |
|             vlan_tag_[0].etherType            |     16    |  ingress  |    x    |     x     |           |      x       |               |
|                vlan_tag_[0].pcp               |     3     |  ingress  |    x    |     x     |           |              |               |
|                vlan_tag_[0].vid               |     12    |  ingress  |    x    |     x     |           |      x       |               |
|                vlan_tag_[1].cfi               |     1     |  ingress  |    x    |     x     |           |              |               |
|             vlan_tag_[1].etherType            |     16    |  ingress  |    x    |     x     |           |              |               |
|                vlan_tag_[1].pcp               |     3     |  ingress  |    x    |     x     |           |              |               |
|                vlan_tag_[1].vid               |     12    |  ingress  |    x    |     x     |           |              |               |
|                  vxlan.flags                  |     8     |  ingress  |    x    |     x     |           |              |               |
|                 vxlan.reserved                |     24    |  ingress  |    x    |     x     |           |              |               |
|                vxlan.reserved2                |     8     |  ingress  |    x    |     x     |           |              |               |
|                   vxlan.vni                   |     24    |  ingress  |    x    |     x     |           |              |               |
-------------------------------------------------------------------------------------------------------------------------------------------

Performing PHV allocation...
ingress_parser critical path: 2013 bits
  start of 0 bits
  ingress_intrinsic_metadata of 64 bits
  ingress_parse_aux of 16 bits
  ingress_parser_control of 0 bits
  parse_ethernet of 112 bits
  parse_fabric_header of 40 bits
  parse_fabric_header_cpu of 72 bits
  parse_fabric_timestamp_header of 48 bits
  parse_fabric_payload_header of 16 bits
  parse_llc_header of 24 bits
  parse_snap_header of 40 bits
  parse_qinq of 32 bits
  parse_qinq_vlan of 32 bits
  parse_ipv6 of 320 bits
  parse_udp of 64 bits
  parse_vxlan of 64 bits
  parse_inner_ethernet of 112 bits
  parse_inner_ipv6 of 320 bits
  parse_inner_tcp of 160 bits
  --ingress-- of 0 bits

--------------------------------------
   Exclusive parse states in ingress_parser
--------------------------------------
  start and start_i2e_mirrored are exclusive parse states
  start and start_e2e_mirrored are exclusive parse states
  start_i2e_mirrored and start_e2e_mirrored are exclusive parse states
  start_i2e_mirrored and parse_ethernet are exclusive parse states
  start_i2e_mirrored and parse_llc_header are exclusive parse states
  start_i2e_mirrored and parse_snap_header are exclusive parse states
  start_i2e_mirrored and parse_vlan are exclusive parse states
  start_i2e_mirrored and parse_qinq are exclusive parse states
  start_i2e_mirrored and parse_qinq_vlan are exclusive parse states
  start_i2e_mirrored and parse_mpls are exclusive parse states
  start_i2e_mirrored and parse_mpls_bos are exclusive parse states
  start_i2e_mirrored and parse_mpls_inner_ipv4 are exclusive parse states
  start_i2e_mirrored and parse_mpls_inner_ipv6 are exclusive parse states
  start_i2e_mirrored and parse_ipv4 are exclusive parse states
  start_i2e_mirrored and parse_ipv4_other are exclusive parse states
  start_i2e_mirrored and parse_ipv4_option_32b are exclusive parse states
  start_i2e_mirrored and parse_ipv4_no_options are exclusive parse states
  start_i2e_mirrored and parse_ipv4_in_ip are exclusive parse states
  start_i2e_mirrored and parse_ipv6_in_ip are exclusive parse states
  start_i2e_mirrored and parse_ipv6 are exclusive parse states
  start_i2e_mirrored and parse_icmp are exclusive parse states
  start_i2e_mirrored and parse_igmp are exclusive parse states
  start_i2e_mirrored and parse_tcp are exclusive parse states
  start_i2e_mirrored and parse_udp are exclusive parse states
  start_i2e_mirrored and parse_gre are exclusive parse states
  start_i2e_mirrored and parse_gre_ipv4 are exclusive parse states
  start_i2e_mirrored and parse_gre_ipv6 are exclusive parse states
  start_i2e_mirrored and parse_nvgre are exclusive parse states
  start_i2e_mirrored and parse_erspan_t3 are exclusive parse states
  start_i2e_mirrored and parse_arp_rarp_req are exclusive parse states
  start_i2e_mirrored and parse_arp_rarp_res are exclusive parse states
  start_i2e_mirrored and parse_arp_rarp are exclusive parse states
  start_i2e_mirrored and parse_eompls are exclusive parse states
  start_i2e_mirrored and parse_vxlan are exclusive parse states
  start_i2e_mirrored and parse_geneve are exclusive parse states
  start_i2e_mirrored and parse_inner_ipv4 are exclusive parse states
  start_i2e_mirrored and parse_inner_icmp are exclusive parse states
  start_i2e_mirrored and parse_inner_tcp are exclusive parse states
  start_i2e_mirrored and parse_inner_udp are exclusive parse states
  start_i2e_mirrored and parse_inner_ipv6 are exclusive parse states
  start_i2e_mirrored and parse_inner_ethernet are exclusive parse states
  start_i2e_mirrored and parse_sflow are exclusive parse states
  start_i2e_mirrored and parse_fabric_header are exclusive parse states
  start_i2e_mirrored and parse_fabric_header_cpu are exclusive parse states
  start_i2e_mirrored and parse_fabric_timestamp_header are exclusive parse states
  start_i2e_mirrored and parse_fabric_payload_header are exclusive parse states
  start_i2e_mirrored and parse_set_prio_med are exclusive parse states
  start_i2e_mirrored and parse_set_prio_high are exclusive parse states
  start_i2e_mirrored and ingress_intrinsic_metadata are exclusive parse states
  start_i2e_mirrored and ingress_parse_aux are exclusive parse states
  start_i2e_mirrored and ingress_parser_control are exclusive parse states
  start_e2e_mirrored and parse_ethernet are exclusive parse states
  start_e2e_mirrored and parse_llc_header are exclusive parse states
  start_e2e_mirrored and parse_snap_header are exclusive parse states
  start_e2e_mirrored and parse_vlan are exclusive parse states
  start_e2e_mirrored and parse_qinq are exclusive parse states
  start_e2e_mirrored and parse_qinq_vlan are exclusive parse states
  start_e2e_mirrored and parse_mpls are exclusive parse states
  start_e2e_mirrored and parse_mpls_bos are exclusive parse states
  start_e2e_mirrored and parse_mpls_inner_ipv4 are exclusive parse states
  start_e2e_mirrored and parse_mpls_inner_ipv6 are exclusive parse states
  start_e2e_mirrored and parse_ipv4 are exclusive parse states
  start_e2e_mirrored and parse_ipv4_other are exclusive parse states
  start_e2e_mirrored and parse_ipv4_option_32b are exclusive parse states
  start_e2e_mirrored and parse_ipv4_no_options are exclusive parse states
  start_e2e_mirrored and parse_ipv4_in_ip are exclusive parse states
  start_e2e_mirrored and parse_ipv6_in_ip are exclusive parse states
  start_e2e_mirrored and parse_ipv6 are exclusive parse states
  start_e2e_mirrored and parse_icmp are exclusive parse states
  start_e2e_mirrored and parse_igmp are exclusive parse states
  start_e2e_mirrored and parse_tcp are exclusive parse states
  start_e2e_mirrored and parse_udp are exclusive parse states
  start_e2e_mirrored and parse_gre are exclusive parse states
  start_e2e_mirrored and parse_gre_ipv4 are exclusive parse states
  start_e2e_mirrored and parse_gre_ipv6 are exclusive parse states
  start_e2e_mirrored and parse_nvgre are exclusive parse states
  start_e2e_mirrored and parse_erspan_t3 are exclusive parse states
  start_e2e_mirrored and parse_arp_rarp_req are exclusive parse states
  start_e2e_mirrored and parse_arp_rarp_res are exclusive parse states
  start_e2e_mirrored and parse_arp_rarp are exclusive parse states
  start_e2e_mirrored and parse_eompls are exclusive parse states
  start_e2e_mirrored and parse_vxlan are exclusive parse states
  start_e2e_mirrored and parse_geneve are exclusive parse states
  start_e2e_mirrored and parse_inner_ipv4 are exclusive parse states
  start_e2e_mirrored and parse_inner_icmp are exclusive parse states
  start_e2e_mirrored and parse_inner_tcp are exclusive parse states
  start_e2e_mirrored and parse_inner_udp are exclusive parse states
  start_e2e_mirrored and parse_inner_ipv6 are exclusive parse states
  start_e2e_mirrored and parse_inner_ethernet are exclusive parse states
  start_e2e_mirrored and parse_sflow are exclusive parse states
  start_e2e_mirrored and parse_fabric_header are exclusive parse states
  start_e2e_mirrored and parse_fabric_header_cpu are exclusive parse states
  start_e2e_mirrored and parse_fabric_timestamp_header are exclusive parse states
  start_e2e_mirrored and parse_fabric_payload_header are exclusive parse states
  start_e2e_mirrored and parse_set_prio_med are exclusive parse states
  start_e2e_mirrored and parse_set_prio_high are exclusive parse states
  start_e2e_mirrored and ingress_intrinsic_metadata are exclusive parse states
  start_e2e_mirrored and ingress_parse_aux are exclusive parse states
  start_e2e_mirrored and ingress_parser_control are exclusive parse states
  parse_vlan and parse_qinq are exclusive parse states
  parse_vlan and parse_qinq_vlan are exclusive parse states
  parse_mpls and parse_ipv4 are exclusive parse states
  parse_mpls and parse_ipv4_other are exclusive parse states
  parse_mpls and parse_ipv4_option_32b are exclusive parse states
  parse_mpls and parse_ipv4_no_options are exclusive parse states
  parse_mpls and parse_ipv4_in_ip are exclusive parse states
  parse_mpls and parse_ipv6_in_ip are exclusive parse states
  parse_mpls and parse_ipv6 are exclusive parse states
  parse_mpls and parse_icmp are exclusive parse states
  parse_mpls and parse_igmp are exclusive parse states
  parse_mpls and parse_tcp are exclusive parse states
  parse_mpls and parse_udp are exclusive parse states
  parse_mpls and parse_gre are exclusive parse states
  parse_mpls and parse_gre_ipv4 are exclusive parse states
  parse_mpls and parse_gre_ipv6 are exclusive parse states
  parse_mpls and parse_nvgre are exclusive parse states
  parse_mpls and parse_erspan_t3 are exclusive parse states
  parse_mpls and parse_arp_rarp_req are exclusive parse states
  parse_mpls and parse_arp_rarp_res are exclusive parse states
  parse_mpls and parse_arp_rarp are exclusive parse states
  parse_mpls and parse_vxlan are exclusive parse states
  parse_mpls and parse_geneve are exclusive parse states
  parse_mpls and parse_sflow are exclusive parse states
  parse_mpls and parse_set_prio_med are exclusive parse states
  parse_mpls and parse_set_prio_high are exclusive parse states
  parse_mpls_bos and parse_ipv4 are exclusive parse states
  parse_mpls_bos and parse_ipv4_other are exclusive parse states
  parse_mpls_bos and parse_ipv4_option_32b are exclusive parse states
  parse_mpls_bos and parse_ipv4_no_options are exclusive parse states
  parse_mpls_bos and parse_ipv4_in_ip are exclusive parse states
  parse_mpls_bos and parse_ipv6_in_ip are exclusive parse states
  parse_mpls_bos and parse_ipv6 are exclusive parse states
  parse_mpls_bos and parse_icmp are exclusive parse states
  parse_mpls_bos and parse_igmp are exclusive parse states
  parse_mpls_bos and parse_tcp are exclusive parse states
  parse_mpls_bos and parse_udp are exclusive parse states
  parse_mpls_bos and parse_gre are exclusive parse states
  parse_mpls_bos and parse_gre_ipv4 are exclusive parse states
  parse_mpls_bos and parse_gre_ipv6 are exclusive parse states
  parse_mpls_bos and parse_nvgre are exclusive parse states
  parse_mpls_bos and parse_erspan_t3 are exclusive parse states
  parse_mpls_bos and parse_arp_rarp_req are exclusive parse states
  parse_mpls_bos and parse_arp_rarp_res are exclusive parse states
  parse_mpls_bos and parse_arp_rarp are exclusive parse states
  parse_mpls_bos and parse_vxlan are exclusive parse states
  parse_mpls_bos and parse_geneve are exclusive parse states
  parse_mpls_bos and parse_sflow are exclusive parse states
  parse_mpls_bos and parse_set_prio_med are exclusive parse states
  parse_mpls_bos and parse_set_prio_high are exclusive parse states
  parse_mpls_inner_ipv4 and parse_mpls_inner_ipv6 are exclusive parse states
  parse_mpls_inner_ipv4 and parse_ipv4 are exclusive parse states
  parse_mpls_inner_ipv4 and parse_ipv4_other are exclusive parse states
  parse_mpls_inner_ipv4 and parse_ipv4_option_32b are exclusive parse states
  parse_mpls_inner_ipv4 and parse_ipv4_no_options are exclusive parse states
  parse_mpls_inner_ipv4 and parse_ipv4_in_ip are exclusive parse states
  parse_mpls_inner_ipv4 and parse_ipv6_in_ip are exclusive parse states
  parse_mpls_inner_ipv4 and parse_ipv6 are exclusive parse states
  parse_mpls_inner_ipv4 and parse_icmp are exclusive parse states
  parse_mpls_inner_ipv4 and parse_igmp are exclusive parse states
  parse_mpls_inner_ipv4 and parse_tcp are exclusive parse states
  parse_mpls_inner_ipv4 and parse_udp are exclusive parse states
  parse_mpls_inner_ipv4 and parse_gre are exclusive parse states
  parse_mpls_inner_ipv4 and parse_gre_ipv4 are exclusive parse states
  parse_mpls_inner_ipv4 and parse_gre_ipv6 are exclusive parse states
  parse_mpls_inner_ipv4 and parse_nvgre are exclusive parse states
  parse_mpls_inner_ipv4 and parse_erspan_t3 are exclusive parse states
  parse_mpls_inner_ipv4 and parse_arp_rarp_req are exclusive parse states
  parse_mpls_inner_ipv4 and parse_arp_rarp_res are exclusive parse states
  parse_mpls_inner_ipv4 and parse_arp_rarp are exclusive parse states
  parse_mpls_inner_ipv4 and parse_eompls are exclusive parse states
  parse_mpls_inner_ipv4 and parse_vxlan are exclusive parse states
  parse_mpls_inner_ipv4 and parse_geneve are exclusive parse states
  parse_mpls_inner_ipv4 and parse_inner_ipv6 are exclusive parse states
  parse_mpls_inner_ipv4 and parse_inner_ethernet are exclusive parse states
  parse_mpls_inner_ipv4 and parse_sflow are exclusive parse states
  parse_mpls_inner_ipv4 and parse_set_prio_med are exclusive parse states
  parse_mpls_inner_ipv4 and parse_set_prio_high are exclusive parse states
  parse_mpls_inner_ipv6 and parse_ipv4 are exclusive parse states
  parse_mpls_inner_ipv6 and parse_ipv4_other are exclusive parse states
  parse_mpls_inner_ipv6 and parse_ipv4_option_32b are exclusive parse states
  parse_mpls_inner_ipv6 and parse_ipv4_no_options are exclusive parse states
  parse_mpls_inner_ipv6 and parse_ipv4_in_ip are exclusive parse states
  parse_mpls_inner_ipv6 and parse_ipv6_in_ip are exclusive parse states
  parse_mpls_inner_ipv6 and parse_ipv6 are exclusive parse states
  parse_mpls_inner_ipv6 and parse_icmp are exclusive parse states
  parse_mpls_inner_ipv6 and parse_igmp are exclusive parse states
  parse_mpls_inner_ipv6 and parse_tcp are exclusive parse states
  parse_mpls_inner_ipv6 and parse_udp are exclusive parse states
  parse_mpls_inner_ipv6 and parse_gre are exclusive parse states
  parse_mpls_inner_ipv6 and parse_gre_ipv4 are exclusive parse states
  parse_mpls_inner_ipv6 and parse_gre_ipv6 are exclusive parse states
  parse_mpls_inner_ipv6 and parse_nvgre are exclusive parse states
  parse_mpls_inner_ipv6 and parse_erspan_t3 are exclusive parse states
  parse_mpls_inner_ipv6 and parse_arp_rarp_req are exclusive parse states
  parse_mpls_inner_ipv6 and parse_arp_rarp_res are exclusive parse states
  parse_mpls_inner_ipv6 and parse_arp_rarp are exclusive parse states
  parse_mpls_inner_ipv6 and parse_eompls are exclusive parse states
  parse_mpls_inner_ipv6 and parse_vxlan are exclusive parse states
  parse_mpls_inner_ipv6 and parse_geneve are exclusive parse states
  parse_mpls_inner_ipv6 and parse_inner_ipv4 are exclusive parse states
  parse_mpls_inner_ipv6 and parse_inner_ethernet are exclusive parse states
  parse_mpls_inner_ipv6 and parse_sflow are exclusive parse states
  parse_mpls_inner_ipv6 and parse_set_prio_med are exclusive parse states
  parse_mpls_inner_ipv6 and parse_set_prio_high are exclusive parse states
  parse_ipv4 and parse_ipv6 are exclusive parse states
  parse_ipv4 and parse_arp_rarp_req are exclusive parse states
  parse_ipv4 and parse_arp_rarp_res are exclusive parse states
  parse_ipv4 and parse_arp_rarp are exclusive parse states
  parse_ipv4 and parse_eompls are exclusive parse states
  parse_ipv4 and parse_set_prio_high are exclusive parse states
  parse_ipv4_other and parse_ipv4_option_32b are exclusive parse states
  parse_ipv4_other and parse_ipv4_no_options are exclusive parse states
  parse_ipv4_other and parse_ipv4_in_ip are exclusive parse states
  parse_ipv4_other and parse_ipv6_in_ip are exclusive parse states
  parse_ipv4_other and parse_ipv6 are exclusive parse states
  parse_ipv4_other and parse_icmp are exclusive parse states
  parse_ipv4_other and parse_igmp are exclusive parse states
  parse_ipv4_other and parse_tcp are exclusive parse states
  parse_ipv4_other and parse_udp are exclusive parse states
  parse_ipv4_other and parse_gre are exclusive parse states
  parse_ipv4_other and parse_gre_ipv4 are exclusive parse states
  parse_ipv4_other and parse_gre_ipv6 are exclusive parse states
  parse_ipv4_other and parse_nvgre are exclusive parse states
  parse_ipv4_other and parse_erspan_t3 are exclusive parse states
  parse_ipv4_other and parse_arp_rarp_req are exclusive parse states
  parse_ipv4_other and parse_arp_rarp_res are exclusive parse states
  parse_ipv4_other and parse_arp_rarp are exclusive parse states
  parse_ipv4_other and parse_eompls are exclusive parse states
  parse_ipv4_other and parse_vxlan are exclusive parse states
  parse_ipv4_other and parse_geneve are exclusive parse states
  parse_ipv4_other and parse_inner_ipv4 are exclusive parse states
  parse_ipv4_other and parse_inner_icmp are exclusive parse states
  parse_ipv4_other and parse_inner_tcp are exclusive parse states
  parse_ipv4_other and parse_inner_udp are exclusive parse states
  parse_ipv4_other and parse_inner_ipv6 are exclusive parse states
  parse_ipv4_other and parse_inner_ethernet are exclusive parse states
  parse_ipv4_other and parse_sflow are exclusive parse states
  parse_ipv4_other and parse_set_prio_med are exclusive parse states
  parse_ipv4_other and parse_set_prio_high are exclusive parse states
  parse_ipv4_option_32b and parse_ipv4_no_options are exclusive parse states
  parse_ipv4_option_32b and parse_ipv6 are exclusive parse states
  parse_ipv4_option_32b and parse_arp_rarp_req are exclusive parse states
  parse_ipv4_option_32b and parse_arp_rarp_res are exclusive parse states
  parse_ipv4_option_32b and parse_arp_rarp are exclusive parse states
  parse_ipv4_option_32b and parse_eompls are exclusive parse states
  parse_ipv4_option_32b and parse_set_prio_high are exclusive parse states
  parse_ipv4_no_options and parse_ipv6 are exclusive parse states
  parse_ipv4_no_options and parse_arp_rarp_req are exclusive parse states
  parse_ipv4_no_options and parse_arp_rarp_res are exclusive parse states
  parse_ipv4_no_options and parse_arp_rarp are exclusive parse states
  parse_ipv4_no_options and parse_eompls are exclusive parse states
  parse_ipv4_no_options and parse_set_prio_high are exclusive parse states
  parse_ipv4_in_ip and parse_ipv6_in_ip are exclusive parse states
  parse_ipv4_in_ip and parse_icmp are exclusive parse states
  parse_ipv4_in_ip and parse_igmp are exclusive parse states
  parse_ipv4_in_ip and parse_tcp are exclusive parse states
  parse_ipv4_in_ip and parse_udp are exclusive parse states
  parse_ipv4_in_ip and parse_gre are exclusive parse states
  parse_ipv4_in_ip and parse_gre_ipv4 are exclusive parse states
  parse_ipv4_in_ip and parse_gre_ipv6 are exclusive parse states
  parse_ipv4_in_ip and parse_nvgre are exclusive parse states
  parse_ipv4_in_ip and parse_erspan_t3 are exclusive parse states
  parse_ipv4_in_ip and parse_arp_rarp_req are exclusive parse states
  parse_ipv4_in_ip and parse_arp_rarp_res are exclusive parse states
  parse_ipv4_in_ip and parse_arp_rarp are exclusive parse states
  parse_ipv4_in_ip and parse_eompls are exclusive parse states
  parse_ipv4_in_ip and parse_vxlan are exclusive parse states
  parse_ipv4_in_ip and parse_geneve are exclusive parse states
  parse_ipv4_in_ip and parse_inner_ipv6 are exclusive parse states
  parse_ipv4_in_ip and parse_inner_ethernet are exclusive parse states
  parse_ipv4_in_ip and parse_sflow are exclusive parse states
  parse_ipv4_in_ip and parse_set_prio_med are exclusive parse states
  parse_ipv4_in_ip and parse_set_prio_high are exclusive parse states
  parse_ipv6_in_ip and parse_icmp are exclusive parse states
  parse_ipv6_in_ip and parse_igmp are exclusive parse states
  parse_ipv6_in_ip and parse_tcp are exclusive parse states
  parse_ipv6_in_ip and parse_udp are exclusive parse states
  parse_ipv6_in_ip and parse_gre are exclusive parse states
  parse_ipv6_in_ip and parse_gre_ipv4 are exclusive parse states
  parse_ipv6_in_ip and parse_gre_ipv6 are exclusive parse states
  parse_ipv6_in_ip and parse_nvgre are exclusive parse states
  parse_ipv6_in_ip and parse_erspan_t3 are exclusive parse states
  parse_ipv6_in_ip and parse_arp_rarp_req are exclusive parse states
  parse_ipv6_in_ip and parse_arp_rarp_res are exclusive parse states
  parse_ipv6_in_ip and parse_arp_rarp are exclusive parse states
  parse_ipv6_in_ip and parse_eompls are exclusive parse states
  parse_ipv6_in_ip and parse_vxlan are exclusive parse states
  parse_ipv6_in_ip and parse_geneve are exclusive parse states
  parse_ipv6_in_ip and parse_inner_ipv4 are exclusive parse states
  parse_ipv6_in_ip and parse_inner_ethernet are exclusive parse states
  parse_ipv6_in_ip and parse_sflow are exclusive parse states
  parse_ipv6_in_ip and parse_set_prio_med are exclusive parse states
  parse_ipv6_in_ip and parse_set_prio_high are exclusive parse states
  parse_ipv6 and parse_igmp are exclusive parse states
  parse_ipv6 and parse_arp_rarp_req are exclusive parse states
  parse_ipv6 and parse_arp_rarp_res are exclusive parse states
  parse_ipv6 and parse_arp_rarp are exclusive parse states
  parse_ipv6 and parse_eompls are exclusive parse states
  parse_ipv6 and parse_set_prio_high are exclusive parse states
  parse_icmp and parse_igmp are exclusive parse states
  parse_icmp and parse_tcp are exclusive parse states
  parse_icmp and parse_udp are exclusive parse states
  parse_icmp and parse_gre are exclusive parse states
  parse_icmp and parse_gre_ipv4 are exclusive parse states
  parse_icmp and parse_gre_ipv6 are exclusive parse states
  parse_icmp and parse_nvgre are exclusive parse states
  parse_icmp and parse_erspan_t3 are exclusive parse states
  parse_icmp and parse_arp_rarp_req are exclusive parse states
  parse_icmp and parse_arp_rarp_res are exclusive parse states
  parse_icmp and parse_arp_rarp are exclusive parse states
  parse_icmp and parse_eompls are exclusive parse states
  parse_icmp and parse_vxlan are exclusive parse states
  parse_icmp and parse_geneve are exclusive parse states
  parse_icmp and parse_inner_ipv4 are exclusive parse states
  parse_icmp and parse_inner_icmp are exclusive parse states
  parse_icmp and parse_inner_tcp are exclusive parse states
  parse_icmp and parse_inner_udp are exclusive parse states
  parse_icmp and parse_inner_ipv6 are exclusive parse states
  parse_icmp and parse_inner_ethernet are exclusive parse states
  parse_icmp and parse_sflow are exclusive parse states
  parse_icmp and parse_set_prio_high are exclusive parse states
  parse_igmp and parse_tcp are exclusive parse states
  parse_igmp and parse_udp are exclusive parse states
  parse_igmp and parse_gre are exclusive parse states
  parse_igmp and parse_gre_ipv4 are exclusive parse states
  parse_igmp and parse_gre_ipv6 are exclusive parse states
  parse_igmp and parse_nvgre are exclusive parse states
  parse_igmp and parse_erspan_t3 are exclusive parse states
  parse_igmp and parse_arp_rarp_req are exclusive parse states
  parse_igmp and parse_arp_rarp_res are exclusive parse states
  parse_igmp and parse_arp_rarp are exclusive parse states
  parse_igmp and parse_eompls are exclusive parse states
  parse_igmp and parse_vxlan are exclusive parse states
  parse_igmp and parse_geneve are exclusive parse states
  parse_igmp and parse_inner_ipv4 are exclusive parse states
  parse_igmp and parse_inner_icmp are exclusive parse states
  parse_igmp and parse_inner_tcp are exclusive parse states
  parse_igmp and parse_inner_udp are exclusive parse states
  parse_igmp and parse_inner_ipv6 are exclusive parse states
  parse_igmp and parse_inner_ethernet are exclusive parse states
  parse_igmp and parse_sflow are exclusive parse states
  parse_igmp and parse_set_prio_med are exclusive parse states
  parse_igmp and parse_set_prio_high are exclusive parse states
  parse_tcp and parse_udp are exclusive parse states
  parse_tcp and parse_gre are exclusive parse states
  parse_tcp and parse_gre_ipv4 are exclusive parse states
  parse_tcp and parse_gre_ipv6 are exclusive parse states
  parse_tcp and parse_nvgre are exclusive parse states
  parse_tcp and parse_erspan_t3 are exclusive parse states
  parse_tcp and parse_arp_rarp_req are exclusive parse states
  parse_tcp and parse_arp_rarp_res are exclusive parse states
  parse_tcp and parse_arp_rarp are exclusive parse states
  parse_tcp and parse_eompls are exclusive parse states
  parse_tcp and parse_vxlan are exclusive parse states
  parse_tcp and parse_geneve are exclusive parse states
  parse_tcp and parse_inner_ipv4 are exclusive parse states
  parse_tcp and parse_inner_icmp are exclusive parse states
  parse_tcp and parse_inner_tcp are exclusive parse states
  parse_tcp and parse_inner_udp are exclusive parse states
  parse_tcp and parse_inner_ipv6 are exclusive parse states
  parse_tcp and parse_inner_ethernet are exclusive parse states
  parse_tcp and parse_sflow are exclusive parse states
  parse_tcp and parse_set_prio_high are exclusive parse states
  parse_udp and parse_gre are exclusive parse states
  parse_udp and parse_gre_ipv4 are exclusive parse states
  parse_udp and parse_gre_ipv6 are exclusive parse states
  parse_udp and parse_nvgre are exclusive parse states
  parse_udp and parse_erspan_t3 are exclusive parse states
  parse_udp and parse_arp_rarp_req are exclusive parse states
  parse_udp and parse_arp_rarp_res are exclusive parse states
  parse_udp and parse_arp_rarp are exclusive parse states
  parse_udp and parse_eompls are exclusive parse states
  parse_udp and parse_set_prio_high are exclusive parse states
  parse_gre and parse_arp_rarp_req are exclusive parse states
  parse_gre and parse_arp_rarp_res are exclusive parse states
  parse_gre and parse_arp_rarp are exclusive parse states
  parse_gre and parse_eompls are exclusive parse states
  parse_gre and parse_vxlan are exclusive parse states
  parse_gre and parse_geneve are exclusive parse states
  parse_gre and parse_sflow are exclusive parse states
  parse_gre and parse_set_prio_med are exclusive parse states
  parse_gre and parse_set_prio_high are exclusive parse states
  parse_gre_ipv4 and parse_gre_ipv6 are exclusive parse states
  parse_gre_ipv4 and parse_nvgre are exclusive parse states
  parse_gre_ipv4 and parse_erspan_t3 are exclusive parse states
  parse_gre_ipv4 and parse_arp_rarp_req are exclusive parse states
  parse_gre_ipv4 and parse_arp_rarp_res are exclusive parse states
  parse_gre_ipv4 and parse_arp_rarp are exclusive parse states
  parse_gre_ipv4 and parse_eompls are exclusive parse states
  parse_gre_ipv4 and parse_vxlan are exclusive parse states
  parse_gre_ipv4 and parse_geneve are exclusive parse states
  parse_gre_ipv4 and parse_inner_ipv6 are exclusive parse states
  parse_gre_ipv4 and parse_inner_ethernet are exclusive parse states
  parse_gre_ipv4 and parse_sflow are exclusive parse states
  parse_gre_ipv4 and parse_set_prio_med are exclusive parse states
  parse_gre_ipv4 and parse_set_prio_high are exclusive parse states
  parse_gre_ipv6 and parse_nvgre are exclusive parse states
  parse_gre_ipv6 and parse_erspan_t3 are exclusive parse states
  parse_gre_ipv6 and parse_arp_rarp_req are exclusive parse states
  parse_gre_ipv6 and parse_arp_rarp_res are exclusive parse states
  parse_gre_ipv6 and parse_arp_rarp are exclusive parse states
  parse_gre_ipv6 and parse_eompls are exclusive parse states
  parse_gre_ipv6 and parse_vxlan are exclusive parse states
  parse_gre_ipv6 and parse_geneve are exclusive parse states
  parse_gre_ipv6 and parse_inner_ipv4 are exclusive parse states
  parse_gre_ipv6 and parse_inner_ethernet are exclusive parse states
  parse_gre_ipv6 and parse_sflow are exclusive parse states
  parse_gre_ipv6 and parse_set_prio_med are exclusive parse states
  parse_gre_ipv6 and parse_set_prio_high are exclusive parse states
  parse_nvgre and parse_erspan_t3 are exclusive parse states
  parse_nvgre and parse_arp_rarp_req are exclusive parse states
  parse_nvgre and parse_arp_rarp_res are exclusive parse states
  parse_nvgre and parse_arp_rarp are exclusive parse states
  parse_nvgre and parse_eompls are exclusive parse states
  parse_nvgre and parse_vxlan are exclusive parse states
  parse_nvgre and parse_geneve are exclusive parse states
  parse_nvgre and parse_sflow are exclusive parse states
  parse_nvgre and parse_set_prio_med are exclusive parse states
  parse_nvgre and parse_set_prio_high are exclusive parse states
  parse_erspan_t3 and parse_arp_rarp_req are exclusive parse states
  parse_erspan_t3 and parse_arp_rarp_res are exclusive parse states
  parse_erspan_t3 and parse_arp_rarp are exclusive parse states
  parse_erspan_t3 and parse_eompls are exclusive parse states
  parse_erspan_t3 and parse_vxlan are exclusive parse states
  parse_erspan_t3 and parse_geneve are exclusive parse states
  parse_erspan_t3 and parse_sflow are exclusive parse states
  parse_erspan_t3 and parse_set_prio_med are exclusive parse states
  parse_erspan_t3 and parse_set_prio_high are exclusive parse states
  parse_arp_rarp_req and parse_arp_rarp_res are exclusive parse states
  parse_arp_rarp_req and parse_eompls are exclusive parse states
  parse_arp_rarp_req and parse_vxlan are exclusive parse states
  parse_arp_rarp_req and parse_geneve are exclusive parse states
  parse_arp_rarp_req and parse_inner_ipv4 are exclusive parse states
  parse_arp_rarp_req and parse_inner_icmp are exclusive parse states
  parse_arp_rarp_req and parse_inner_tcp are exclusive parse states
  parse_arp_rarp_req and parse_inner_udp are exclusive parse states
  parse_arp_rarp_req and parse_inner_ipv6 are exclusive parse states
  parse_arp_rarp_req and parse_inner_ethernet are exclusive parse states
  parse_arp_rarp_req and parse_sflow are exclusive parse states
  parse_arp_rarp_req and parse_set_prio_high are exclusive parse states
  parse_arp_rarp_res and parse_eompls are exclusive parse states
  parse_arp_rarp_res and parse_vxlan are exclusive parse states
  parse_arp_rarp_res and parse_geneve are exclusive parse states
  parse_arp_rarp_res and parse_inner_ipv4 are exclusive parse states
  parse_arp_rarp_res and parse_inner_icmp are exclusive parse states
  parse_arp_rarp_res and parse_inner_tcp are exclusive parse states
  parse_arp_rarp_res and parse_inner_udp are exclusive parse states
  parse_arp_rarp_res and parse_inner_ipv6 are exclusive parse states
  parse_arp_rarp_res and parse_inner_ethernet are exclusive parse states
  parse_arp_rarp_res and parse_sflow are exclusive parse states
  parse_arp_rarp_res and parse_set_prio_high are exclusive parse states
  parse_arp_rarp and parse_eompls are exclusive parse states
  parse_arp_rarp and parse_vxlan are exclusive parse states
  parse_arp_rarp and parse_geneve are exclusive parse states
  parse_arp_rarp and parse_inner_ipv4 are exclusive parse states
  parse_arp_rarp and parse_inner_icmp are exclusive parse states
  parse_arp_rarp and parse_inner_tcp are exclusive parse states
  parse_arp_rarp and parse_inner_udp are exclusive parse states
  parse_arp_rarp and parse_inner_ipv6 are exclusive parse states
  parse_arp_rarp and parse_inner_ethernet are exclusive parse states
  parse_arp_rarp and parse_sflow are exclusive parse states
  parse_arp_rarp and parse_set_prio_high are exclusive parse states
  parse_eompls and parse_vxlan are exclusive parse states
  parse_eompls and parse_geneve are exclusive parse states
  parse_eompls and parse_sflow are exclusive parse states
  parse_eompls and parse_set_prio_med are exclusive parse states
  parse_eompls and parse_set_prio_high are exclusive parse states
  parse_vxlan and parse_geneve are exclusive parse states
  parse_vxlan and parse_sflow are exclusive parse states
  parse_vxlan and parse_set_prio_med are exclusive parse states
  parse_vxlan and parse_set_prio_high are exclusive parse states
  parse_geneve and parse_sflow are exclusive parse states
  parse_geneve and parse_set_prio_med are exclusive parse states
  parse_geneve and parse_set_prio_high are exclusive parse states
  parse_inner_ipv4 and parse_inner_ipv6 are exclusive parse states
  parse_inner_ipv4 and parse_sflow are exclusive parse states
  parse_inner_ipv4 and parse_set_prio_med are exclusive parse states
  parse_inner_ipv4 and parse_set_prio_high are exclusive parse states
  parse_inner_icmp and parse_inner_tcp are exclusive parse states
  parse_inner_icmp and parse_inner_udp are exclusive parse states
  parse_inner_icmp and parse_sflow are exclusive parse states
  parse_inner_icmp and parse_set_prio_med are exclusive parse states
  parse_inner_icmp and parse_set_prio_high are exclusive parse states
  parse_inner_tcp and parse_inner_udp are exclusive parse states
  parse_inner_tcp and parse_sflow are exclusive parse states
  parse_inner_tcp and parse_set_prio_med are exclusive parse states
  parse_inner_tcp and parse_set_prio_high are exclusive parse states
  parse_inner_udp and parse_sflow are exclusive parse states
  parse_inner_udp and parse_set_prio_med are exclusive parse states
  parse_inner_udp and parse_set_prio_high are exclusive parse states
  parse_inner_ipv6 and parse_sflow are exclusive parse states
  parse_inner_ipv6 and parse_set_prio_med are exclusive parse states
  parse_inner_ipv6 and parse_set_prio_high are exclusive parse states
  parse_inner_ethernet and parse_sflow are exclusive parse states
  parse_inner_ethernet and parse_set_prio_med are exclusive parse states
  parse_inner_ethernet and parse_set_prio_high are exclusive parse states
  parse_sflow and parse_set_prio_med are exclusive parse states
  parse_sflow and parse_set_prio_high are exclusive parse states
  parse_set_prio_med and parse_set_prio_high are exclusive parse states

egress_parser critical path: 1544 bits
  __super_start__ of 0 bits
  egress_intrinsic_metadata of 64 bits
  egress_parse_aux of 8 bits
  egress_parser_control of 0 bits
  start of 0 bits
  parse_ethernet of 112 bits
  parse_fabric_header of 40 bits
  parse_fabric_header_cpu of 72 bits
  parse_fabric_timestamp_header of 48 bits
  parse_fabric_payload_header of 16 bits
  parse_llc_header of 24 bits
  parse_snap_header of 40 bits
  parse_qinq of 32 bits
  parse_qinq_vlan of 32 bits
  parse_ipv6 of 320 bits
  parse_udp of 64 bits
  parse_vxlan of 64 bits
  parse_inner_ethernet of 112 bits
  parse_inner_ipv6 of 320 bits
  parse_inner_tcp of 160 bits
  egress_for_mirror_buffer of 16 bits
  --egress-- of 0 bits

--------------------------------------
   Exclusive parse states in egress_parser
--------------------------------------
  start and start_i2e_mirrored are exclusive parse states
  start and start_e2e_mirrored are exclusive parse states
  start_i2e_mirrored and start_e2e_mirrored are exclusive parse states
  start_i2e_mirrored and parse_ethernet are exclusive parse states
  start_i2e_mirrored and parse_llc_header are exclusive parse states
  start_i2e_mirrored and parse_snap_header are exclusive parse states
  start_i2e_mirrored and parse_vlan are exclusive parse states
  start_i2e_mirrored and parse_qinq are exclusive parse states
  start_i2e_mirrored and parse_qinq_vlan are exclusive parse states
  start_i2e_mirrored and parse_mpls are exclusive parse states
  start_i2e_mirrored and parse_mpls_bos are exclusive parse states
  start_i2e_mirrored and parse_mpls_inner_ipv4 are exclusive parse states
  start_i2e_mirrored and parse_mpls_inner_ipv6 are exclusive parse states
  start_i2e_mirrored and parse_ipv4 are exclusive parse states
  start_i2e_mirrored and parse_ipv4_other are exclusive parse states
  start_i2e_mirrored and parse_ipv4_option_32b are exclusive parse states
  start_i2e_mirrored and parse_ipv4_no_options are exclusive parse states
  start_i2e_mirrored and parse_ipv4_in_ip are exclusive parse states
  start_i2e_mirrored and parse_ipv6_in_ip are exclusive parse states
  start_i2e_mirrored and parse_ipv6 are exclusive parse states
  start_i2e_mirrored and parse_icmp are exclusive parse states
  start_i2e_mirrored and parse_igmp are exclusive parse states
  start_i2e_mirrored and parse_tcp are exclusive parse states
  start_i2e_mirrored and parse_udp are exclusive parse states
  start_i2e_mirrored and parse_gre are exclusive parse states
  start_i2e_mirrored and parse_gre_ipv4 are exclusive parse states
  start_i2e_mirrored and parse_gre_ipv6 are exclusive parse states
  start_i2e_mirrored and parse_nvgre are exclusive parse states
  start_i2e_mirrored and parse_erspan_t3 are exclusive parse states
  start_i2e_mirrored and parse_arp_rarp_req are exclusive parse states
  start_i2e_mirrored and parse_arp_rarp_res are exclusive parse states
  start_i2e_mirrored and parse_arp_rarp are exclusive parse states
  start_i2e_mirrored and parse_eompls are exclusive parse states
  start_i2e_mirrored and parse_vxlan are exclusive parse states
  start_i2e_mirrored and parse_geneve are exclusive parse states
  start_i2e_mirrored and parse_inner_ipv4 are exclusive parse states
  start_i2e_mirrored and parse_inner_icmp are exclusive parse states
  start_i2e_mirrored and parse_inner_tcp are exclusive parse states
  start_i2e_mirrored and parse_inner_udp are exclusive parse states
  start_i2e_mirrored and parse_inner_ipv6 are exclusive parse states
  start_i2e_mirrored and parse_inner_ethernet are exclusive parse states
  start_i2e_mirrored and parse_sflow are exclusive parse states
  start_i2e_mirrored and parse_fabric_header are exclusive parse states
  start_i2e_mirrored and parse_fabric_header_cpu are exclusive parse states
  start_i2e_mirrored and parse_fabric_timestamp_header are exclusive parse states
  start_i2e_mirrored and parse_fabric_payload_header are exclusive parse states
  start_i2e_mirrored and parse_set_prio_med are exclusive parse states
  start_i2e_mirrored and parse_set_prio_high are exclusive parse states
  start_e2e_mirrored and parse_ethernet are exclusive parse states
  start_e2e_mirrored and parse_llc_header are exclusive parse states
  start_e2e_mirrored and parse_snap_header are exclusive parse states
  start_e2e_mirrored and parse_vlan are exclusive parse states
  start_e2e_mirrored and parse_qinq are exclusive parse states
  start_e2e_mirrored and parse_qinq_vlan are exclusive parse states
  start_e2e_mirrored and parse_mpls are exclusive parse states
  start_e2e_mirrored and parse_mpls_bos are exclusive parse states
  start_e2e_mirrored and parse_mpls_inner_ipv4 are exclusive parse states
  start_e2e_mirrored and parse_mpls_inner_ipv6 are exclusive parse states
  start_e2e_mirrored and parse_ipv4 are exclusive parse states
  start_e2e_mirrored and parse_ipv4_other are exclusive parse states
  start_e2e_mirrored and parse_ipv4_option_32b are exclusive parse states
  start_e2e_mirrored and parse_ipv4_no_options are exclusive parse states
  start_e2e_mirrored and parse_ipv4_in_ip are exclusive parse states
  start_e2e_mirrored and parse_ipv6_in_ip are exclusive parse states
  start_e2e_mirrored and parse_ipv6 are exclusive parse states
  start_e2e_mirrored and parse_icmp are exclusive parse states
  start_e2e_mirrored and parse_igmp are exclusive parse states
  start_e2e_mirrored and parse_tcp are exclusive parse states
  start_e2e_mirrored and parse_udp are exclusive parse states
  start_e2e_mirrored and parse_gre are exclusive parse states
  start_e2e_mirrored and parse_gre_ipv4 are exclusive parse states
  start_e2e_mirrored and parse_gre_ipv6 are exclusive parse states
  start_e2e_mirrored and parse_nvgre are exclusive parse states
  start_e2e_mirrored and parse_erspan_t3 are exclusive parse states
  start_e2e_mirrored and parse_arp_rarp_req are exclusive parse states
  start_e2e_mirrored and parse_arp_rarp_res are exclusive parse states
  start_e2e_mirrored and parse_arp_rarp are exclusive parse states
  start_e2e_mirrored and parse_eompls are exclusive parse states
  start_e2e_mirrored and parse_vxlan are exclusive parse states
  start_e2e_mirrored and parse_geneve are exclusive parse states
  start_e2e_mirrored and parse_inner_ipv4 are exclusive parse states
  start_e2e_mirrored and parse_inner_icmp are exclusive parse states
  start_e2e_mirrored and parse_inner_tcp are exclusive parse states
  start_e2e_mirrored and parse_inner_udp are exclusive parse states
  start_e2e_mirrored and parse_inner_ipv6 are exclusive parse states
  start_e2e_mirrored and parse_inner_ethernet are exclusive parse states
  start_e2e_mirrored and parse_sflow are exclusive parse states
  start_e2e_mirrored and parse_fabric_header are exclusive parse states
  start_e2e_mirrored and parse_fabric_header_cpu are exclusive parse states
  start_e2e_mirrored and parse_fabric_timestamp_header are exclusive parse states
  start_e2e_mirrored and parse_fabric_payload_header are exclusive parse states
  start_e2e_mirrored and parse_set_prio_med are exclusive parse states
  start_e2e_mirrored and parse_set_prio_high are exclusive parse states
  parse_vlan and parse_qinq are exclusive parse states
  parse_vlan and parse_qinq_vlan are exclusive parse states
  parse_mpls and parse_ipv4 are exclusive parse states
  parse_mpls and parse_ipv4_other are exclusive parse states
  parse_mpls and parse_ipv4_option_32b are exclusive parse states
  parse_mpls and parse_ipv4_no_options are exclusive parse states
  parse_mpls and parse_ipv4_in_ip are exclusive parse states
  parse_mpls and parse_ipv6_in_ip are exclusive parse states
  parse_mpls and parse_ipv6 are exclusive parse states
  parse_mpls and parse_icmp are exclusive parse states
  parse_mpls and parse_igmp are exclusive parse states
  parse_mpls and parse_tcp are exclusive parse states
  parse_mpls and parse_udp are exclusive parse states
  parse_mpls and parse_gre are exclusive parse states
  parse_mpls and parse_gre_ipv4 are exclusive parse states
  parse_mpls and parse_gre_ipv6 are exclusive parse states
  parse_mpls and parse_nvgre are exclusive parse states
  parse_mpls and parse_erspan_t3 are exclusive parse states
  parse_mpls and parse_arp_rarp_req are exclusive parse states
  parse_mpls and parse_arp_rarp_res are exclusive parse states
  parse_mpls and parse_arp_rarp are exclusive parse states
  parse_mpls and parse_vxlan are exclusive parse states
  parse_mpls and parse_geneve are exclusive parse states
  parse_mpls and parse_sflow are exclusive parse states
  parse_mpls and parse_set_prio_med are exclusive parse states
  parse_mpls and parse_set_prio_high are exclusive parse states
  parse_mpls_bos and parse_ipv4 are exclusive parse states
  parse_mpls_bos and parse_ipv4_other are exclusive parse states
  parse_mpls_bos and parse_ipv4_option_32b are exclusive parse states
  parse_mpls_bos and parse_ipv4_no_options are exclusive parse states
  parse_mpls_bos and parse_ipv4_in_ip are exclusive parse states
  parse_mpls_bos and parse_ipv6_in_ip are exclusive parse states
  parse_mpls_bos and parse_ipv6 are exclusive parse states
  parse_mpls_bos and parse_icmp are exclusive parse states
  parse_mpls_bos and parse_igmp are exclusive parse states
  parse_mpls_bos and parse_tcp are exclusive parse states
  parse_mpls_bos and parse_udp are exclusive parse states
  parse_mpls_bos and parse_gre are exclusive parse states
  parse_mpls_bos and parse_gre_ipv4 are exclusive parse states
  parse_mpls_bos and parse_gre_ipv6 are exclusive parse states
  parse_mpls_bos and parse_nvgre are exclusive parse states
  parse_mpls_bos and parse_erspan_t3 are exclusive parse states
  parse_mpls_bos and parse_arp_rarp_req are exclusive parse states
  parse_mpls_bos and parse_arp_rarp_res are exclusive parse states
  parse_mpls_bos and parse_arp_rarp are exclusive parse states
  parse_mpls_bos and parse_vxlan are exclusive parse states
  parse_mpls_bos and parse_geneve are exclusive parse states
  parse_mpls_bos and parse_sflow are exclusive parse states
  parse_mpls_bos and parse_set_prio_med are exclusive parse states
  parse_mpls_bos and parse_set_prio_high are exclusive parse states
  parse_mpls_inner_ipv4 and parse_mpls_inner_ipv6 are exclusive parse states
  parse_mpls_inner_ipv4 and parse_ipv4 are exclusive parse states
  parse_mpls_inner_ipv4 and parse_ipv4_other are exclusive parse states
  parse_mpls_inner_ipv4 and parse_ipv4_option_32b are exclusive parse states
  parse_mpls_inner_ipv4 and parse_ipv4_no_options are exclusive parse states
  parse_mpls_inner_ipv4 and parse_ipv4_in_ip are exclusive parse states
  parse_mpls_inner_ipv4 and parse_ipv6_in_ip are exclusive parse states
  parse_mpls_inner_ipv4 and parse_ipv6 are exclusive parse states
  parse_mpls_inner_ipv4 and parse_icmp are exclusive parse states
  parse_mpls_inner_ipv4 and parse_igmp are exclusive parse states
  parse_mpls_inner_ipv4 and parse_tcp are exclusive parse states
  parse_mpls_inner_ipv4 and parse_udp are exclusive parse states
  parse_mpls_inner_ipv4 and parse_gre are exclusive parse states
  parse_mpls_inner_ipv4 and parse_gre_ipv4 are exclusive parse states
  parse_mpls_inner_ipv4 and parse_gre_ipv6 are exclusive parse states
  parse_mpls_inner_ipv4 and parse_nvgre are exclusive parse states
  parse_mpls_inner_ipv4 and parse_erspan_t3 are exclusive parse states
  parse_mpls_inner_ipv4 and parse_arp_rarp_req are exclusive parse states
  parse_mpls_inner_ipv4 and parse_arp_rarp_res are exclusive parse states
  parse_mpls_inner_ipv4 and parse_arp_rarp are exclusive parse states
  parse_mpls_inner_ipv4 and parse_eompls are exclusive parse states
  parse_mpls_inner_ipv4 and parse_vxlan are exclusive parse states
  parse_mpls_inner_ipv4 and parse_geneve are exclusive parse states
  parse_mpls_inner_ipv4 and parse_inner_ipv6 are exclusive parse states
  parse_mpls_inner_ipv4 and parse_inner_ethernet are exclusive parse states
  parse_mpls_inner_ipv4 and parse_sflow are exclusive parse states
  parse_mpls_inner_ipv4 and parse_set_prio_med are exclusive parse states
  parse_mpls_inner_ipv4 and parse_set_prio_high are exclusive parse states
  parse_mpls_inner_ipv6 and parse_ipv4 are exclusive parse states
  parse_mpls_inner_ipv6 and parse_ipv4_other are exclusive parse states
  parse_mpls_inner_ipv6 and parse_ipv4_option_32b are exclusive parse states
  parse_mpls_inner_ipv6 and parse_ipv4_no_options are exclusive parse states
  parse_mpls_inner_ipv6 and parse_ipv4_in_ip are exclusive parse states
  parse_mpls_inner_ipv6 and parse_ipv6_in_ip are exclusive parse states
  parse_mpls_inner_ipv6 and parse_ipv6 are exclusive parse states
  parse_mpls_inner_ipv6 and parse_icmp are exclusive parse states
  parse_mpls_inner_ipv6 and parse_igmp are exclusive parse states
  parse_mpls_inner_ipv6 and parse_tcp are exclusive parse states
  parse_mpls_inner_ipv6 and parse_udp are exclusive parse states
  parse_mpls_inner_ipv6 and parse_gre are exclusive parse states
  parse_mpls_inner_ipv6 and parse_gre_ipv4 are exclusive parse states
  parse_mpls_inner_ipv6 and parse_gre_ipv6 are exclusive parse states
  parse_mpls_inner_ipv6 and parse_nvgre are exclusive parse states
  parse_mpls_inner_ipv6 and parse_erspan_t3 are exclusive parse states
  parse_mpls_inner_ipv6 and parse_arp_rarp_req are exclusive parse states
  parse_mpls_inner_ipv6 and parse_arp_rarp_res are exclusive parse states
  parse_mpls_inner_ipv6 and parse_arp_rarp are exclusive parse states
  parse_mpls_inner_ipv6 and parse_eompls are exclusive parse states
  parse_mpls_inner_ipv6 and parse_vxlan are exclusive parse states
  parse_mpls_inner_ipv6 and parse_geneve are exclusive parse states
  parse_mpls_inner_ipv6 and parse_inner_ipv4 are exclusive parse states
  parse_mpls_inner_ipv6 and parse_inner_ethernet are exclusive parse states
  parse_mpls_inner_ipv6 and parse_sflow are exclusive parse states
  parse_mpls_inner_ipv6 and parse_set_prio_med are exclusive parse states
  parse_mpls_inner_ipv6 and parse_set_prio_high are exclusive parse states
  parse_ipv4 and parse_ipv6 are exclusive parse states
  parse_ipv4 and parse_arp_rarp_req are exclusive parse states
  parse_ipv4 and parse_arp_rarp_res are exclusive parse states
  parse_ipv4 and parse_arp_rarp are exclusive parse states
  parse_ipv4 and parse_eompls are exclusive parse states
  parse_ipv4 and parse_set_prio_high are exclusive parse states
  parse_ipv4_other and parse_ipv4_option_32b are exclusive parse states
  parse_ipv4_other and parse_ipv4_no_options are exclusive parse states
  parse_ipv4_other and parse_ipv4_in_ip are exclusive parse states
  parse_ipv4_other and parse_ipv6_in_ip are exclusive parse states
  parse_ipv4_other and parse_ipv6 are exclusive parse states
  parse_ipv4_other and parse_icmp are exclusive parse states
  parse_ipv4_other and parse_igmp are exclusive parse states
  parse_ipv4_other and parse_tcp are exclusive parse states
  parse_ipv4_other and parse_udp are exclusive parse states
  parse_ipv4_other and parse_gre are exclusive parse states
  parse_ipv4_other and parse_gre_ipv4 are exclusive parse states
  parse_ipv4_other and parse_gre_ipv6 are exclusive parse states
  parse_ipv4_other and parse_nvgre are exclusive parse states
  parse_ipv4_other and parse_erspan_t3 are exclusive parse states
  parse_ipv4_other and parse_arp_rarp_req are exclusive parse states
  parse_ipv4_other and parse_arp_rarp_res are exclusive parse states
  parse_ipv4_other and parse_arp_rarp are exclusive parse states
  parse_ipv4_other and parse_eompls are exclusive parse states
  parse_ipv4_other and parse_vxlan are exclusive parse states
  parse_ipv4_other and parse_geneve are exclusive parse states
  parse_ipv4_other and parse_inner_ipv4 are exclusive parse states
  parse_ipv4_other and parse_inner_icmp are exclusive parse states
  parse_ipv4_other and parse_inner_tcp are exclusive parse states
  parse_ipv4_other and parse_inner_udp are exclusive parse states
  parse_ipv4_other and parse_inner_ipv6 are exclusive parse states
  parse_ipv4_other and parse_inner_ethernet are exclusive parse states
  parse_ipv4_other and parse_sflow are exclusive parse states
  parse_ipv4_other and parse_set_prio_med are exclusive parse states
  parse_ipv4_other and parse_set_prio_high are exclusive parse states
  parse_ipv4_option_32b and parse_ipv4_no_options are exclusive parse states
  parse_ipv4_option_32b and parse_ipv6 are exclusive parse states
  parse_ipv4_option_32b and parse_arp_rarp_req are exclusive parse states
  parse_ipv4_option_32b and parse_arp_rarp_res are exclusive parse states
  parse_ipv4_option_32b and parse_arp_rarp are exclusive parse states
  parse_ipv4_option_32b and parse_eompls are exclusive parse states
  parse_ipv4_option_32b and parse_set_prio_high are exclusive parse states
  parse_ipv4_no_options and parse_ipv6 are exclusive parse states
  parse_ipv4_no_options and parse_arp_rarp_req are exclusive parse states
  parse_ipv4_no_options and parse_arp_rarp_res are exclusive parse states
  parse_ipv4_no_options and parse_arp_rarp are exclusive parse states
  parse_ipv4_no_options and parse_eompls are exclusive parse states
  parse_ipv4_no_options and parse_set_prio_high are exclusive parse states
  parse_ipv4_in_ip and parse_ipv6_in_ip are exclusive parse states
  parse_ipv4_in_ip and parse_icmp are exclusive parse states
  parse_ipv4_in_ip and parse_igmp are exclusive parse states
  parse_ipv4_in_ip and parse_tcp are exclusive parse states
  parse_ipv4_in_ip and parse_udp are exclusive parse states
  parse_ipv4_in_ip and parse_gre are exclusive parse states
  parse_ipv4_in_ip and parse_gre_ipv4 are exclusive parse states
  parse_ipv4_in_ip and parse_gre_ipv6 are exclusive parse states
  parse_ipv4_in_ip and parse_nvgre are exclusive parse states
  parse_ipv4_in_ip and parse_erspan_t3 are exclusive parse states
  parse_ipv4_in_ip and parse_arp_rarp_req are exclusive parse states
  parse_ipv4_in_ip and parse_arp_rarp_res are exclusive parse states
  parse_ipv4_in_ip and parse_arp_rarp are exclusive parse states
  parse_ipv4_in_ip and parse_eompls are exclusive parse states
  parse_ipv4_in_ip and parse_vxlan are exclusive parse states
  parse_ipv4_in_ip and parse_geneve are exclusive parse states
  parse_ipv4_in_ip and parse_inner_ipv6 are exclusive parse states
  parse_ipv4_in_ip and parse_inner_ethernet are exclusive parse states
  parse_ipv4_in_ip and parse_sflow are exclusive parse states
  parse_ipv4_in_ip and parse_set_prio_med are exclusive parse states
  parse_ipv4_in_ip and parse_set_prio_high are exclusive parse states
  parse_ipv6_in_ip and parse_icmp are exclusive parse states
  parse_ipv6_in_ip and parse_igmp are exclusive parse states
  parse_ipv6_in_ip and parse_tcp are exclusive parse states
  parse_ipv6_in_ip and parse_udp are exclusive parse states
  parse_ipv6_in_ip and parse_gre are exclusive parse states
  parse_ipv6_in_ip and parse_gre_ipv4 are exclusive parse states
  parse_ipv6_in_ip and parse_gre_ipv6 are exclusive parse states
  parse_ipv6_in_ip and parse_nvgre are exclusive parse states
  parse_ipv6_in_ip and parse_erspan_t3 are exclusive parse states
  parse_ipv6_in_ip and parse_arp_rarp_req are exclusive parse states
  parse_ipv6_in_ip and parse_arp_rarp_res are exclusive parse states
  parse_ipv6_in_ip and parse_arp_rarp are exclusive parse states
  parse_ipv6_in_ip and parse_eompls are exclusive parse states
  parse_ipv6_in_ip and parse_vxlan are exclusive parse states
  parse_ipv6_in_ip and parse_geneve are exclusive parse states
  parse_ipv6_in_ip and parse_inner_ipv4 are exclusive parse states
  parse_ipv6_in_ip and parse_inner_ethernet are exclusive parse states
  parse_ipv6_in_ip and parse_sflow are exclusive parse states
  parse_ipv6_in_ip and parse_set_prio_med are exclusive parse states
  parse_ipv6_in_ip and parse_set_prio_high are exclusive parse states
  parse_ipv6 and parse_igmp are exclusive parse states
  parse_ipv6 and parse_arp_rarp_req are exclusive parse states
  parse_ipv6 and parse_arp_rarp_res are exclusive parse states
  parse_ipv6 and parse_arp_rarp are exclusive parse states
  parse_ipv6 and parse_eompls are exclusive parse states
  parse_ipv6 and parse_set_prio_high are exclusive parse states
  parse_icmp and parse_igmp are exclusive parse states
  parse_icmp and parse_tcp are exclusive parse states
  parse_icmp and parse_udp are exclusive parse states
  parse_icmp and parse_gre are exclusive parse states
  parse_icmp and parse_gre_ipv4 are exclusive parse states
  parse_icmp and parse_gre_ipv6 are exclusive parse states
  parse_icmp and parse_nvgre are exclusive parse states
  parse_icmp and parse_erspan_t3 are exclusive parse states
  parse_icmp and parse_arp_rarp_req are exclusive parse states
  parse_icmp and parse_arp_rarp_res are exclusive parse states
  parse_icmp and parse_arp_rarp are exclusive parse states
  parse_icmp and parse_eompls are exclusive parse states
  parse_icmp and parse_vxlan are exclusive parse states
  parse_icmp and parse_geneve are exclusive parse states
  parse_icmp and parse_inner_ipv4 are exclusive parse states
  parse_icmp and parse_inner_icmp are exclusive parse states
  parse_icmp and parse_inner_tcp are exclusive parse states
  parse_icmp and parse_inner_udp are exclusive parse states
  parse_icmp and parse_inner_ipv6 are exclusive parse states
  parse_icmp and parse_inner_ethernet are exclusive parse states
  parse_icmp and parse_sflow are exclusive parse states
  parse_icmp and parse_set_prio_high are exclusive parse states
  parse_igmp and parse_tcp are exclusive parse states
  parse_igmp and parse_udp are exclusive parse states
  parse_igmp and parse_gre are exclusive parse states
  parse_igmp and parse_gre_ipv4 are exclusive parse states
  parse_igmp and parse_gre_ipv6 are exclusive parse states
  parse_igmp and parse_nvgre are exclusive parse states
  parse_igmp and parse_erspan_t3 are exclusive parse states
  parse_igmp and parse_arp_rarp_req are exclusive parse states
  parse_igmp and parse_arp_rarp_res are exclusive parse states
  parse_igmp and parse_arp_rarp are exclusive parse states
  parse_igmp and parse_eompls are exclusive parse states
  parse_igmp and parse_vxlan are exclusive parse states
  parse_igmp and parse_geneve are exclusive parse states
  parse_igmp and parse_inner_ipv4 are exclusive parse states
  parse_igmp and parse_inner_icmp are exclusive parse states
  parse_igmp and parse_inner_tcp are exclusive parse states
  parse_igmp and parse_inner_udp are exclusive parse states
  parse_igmp and parse_inner_ipv6 are exclusive parse states
  parse_igmp and parse_inner_ethernet are exclusive parse states
  parse_igmp and parse_sflow are exclusive parse states
  parse_igmp and parse_set_prio_med are exclusive parse states
  parse_igmp and parse_set_prio_high are exclusive parse states
  parse_tcp and parse_udp are exclusive parse states
  parse_tcp and parse_gre are exclusive parse states
  parse_tcp and parse_gre_ipv4 are exclusive parse states
  parse_tcp and parse_gre_ipv6 are exclusive parse states
  parse_tcp and parse_nvgre are exclusive parse states
  parse_tcp and parse_erspan_t3 are exclusive parse states
  parse_tcp and parse_arp_rarp_req are exclusive parse states
  parse_tcp and parse_arp_rarp_res are exclusive parse states
  parse_tcp and parse_arp_rarp are exclusive parse states
  parse_tcp and parse_eompls are exclusive parse states
  parse_tcp and parse_vxlan are exclusive parse states
  parse_tcp and parse_geneve are exclusive parse states
  parse_tcp and parse_inner_ipv4 are exclusive parse states
  parse_tcp and parse_inner_icmp are exclusive parse states
  parse_tcp and parse_inner_tcp are exclusive parse states
  parse_tcp and parse_inner_udp are exclusive parse states
  parse_tcp and parse_inner_ipv6 are exclusive parse states
  parse_tcp and parse_inner_ethernet are exclusive parse states
  parse_tcp and parse_sflow are exclusive parse states
  parse_tcp and parse_set_prio_high are exclusive parse states
  parse_udp and parse_gre are exclusive parse states
  parse_udp and parse_gre_ipv4 are exclusive parse states
  parse_udp and parse_gre_ipv6 are exclusive parse states
  parse_udp and parse_nvgre are exclusive parse states
  parse_udp and parse_erspan_t3 are exclusive parse states
  parse_udp and parse_arp_rarp_req are exclusive parse states
  parse_udp and parse_arp_rarp_res are exclusive parse states
  parse_udp and parse_arp_rarp are exclusive parse states
  parse_udp and parse_eompls are exclusive parse states
  parse_udp and parse_set_prio_high are exclusive parse states
  parse_gre and parse_arp_rarp_req are exclusive parse states
  parse_gre and parse_arp_rarp_res are exclusive parse states
  parse_gre and parse_arp_rarp are exclusive parse states
  parse_gre and parse_eompls are exclusive parse states
  parse_gre and parse_vxlan are exclusive parse states
  parse_gre and parse_geneve are exclusive parse states
  parse_gre and parse_sflow are exclusive parse states
  parse_gre and parse_set_prio_med are exclusive parse states
  parse_gre and parse_set_prio_high are exclusive parse states
  parse_gre_ipv4 and parse_gre_ipv6 are exclusive parse states
  parse_gre_ipv4 and parse_nvgre are exclusive parse states
  parse_gre_ipv4 and parse_erspan_t3 are exclusive parse states
  parse_gre_ipv4 and parse_arp_rarp_req are exclusive parse states
  parse_gre_ipv4 and parse_arp_rarp_res are exclusive parse states
  parse_gre_ipv4 and parse_arp_rarp are exclusive parse states
  parse_gre_ipv4 and parse_eompls are exclusive parse states
  parse_gre_ipv4 and parse_vxlan are exclusive parse states
  parse_gre_ipv4 and parse_geneve are exclusive parse states
  parse_gre_ipv4 and parse_inner_ipv6 are exclusive parse states
  parse_gre_ipv4 and parse_inner_ethernet are exclusive parse states
  parse_gre_ipv4 and parse_sflow are exclusive parse states
  parse_gre_ipv4 and parse_set_prio_med are exclusive parse states
  parse_gre_ipv4 and parse_set_prio_high are exclusive parse states
  parse_gre_ipv6 and parse_nvgre are exclusive parse states
  parse_gre_ipv6 and parse_erspan_t3 are exclusive parse states
  parse_gre_ipv6 and parse_arp_rarp_req are exclusive parse states
  parse_gre_ipv6 and parse_arp_rarp_res are exclusive parse states
  parse_gre_ipv6 and parse_arp_rarp are exclusive parse states
  parse_gre_ipv6 and parse_eompls are exclusive parse states
  parse_gre_ipv6 and parse_vxlan are exclusive parse states
  parse_gre_ipv6 and parse_geneve are exclusive parse states
  parse_gre_ipv6 and parse_inner_ipv4 are exclusive parse states
  parse_gre_ipv6 and parse_inner_ethernet are exclusive parse states
  parse_gre_ipv6 and parse_sflow are exclusive parse states
  parse_gre_ipv6 and parse_set_prio_med are exclusive parse states
  parse_gre_ipv6 and parse_set_prio_high are exclusive parse states
  parse_nvgre and parse_erspan_t3 are exclusive parse states
  parse_nvgre and parse_arp_rarp_req are exclusive parse states
  parse_nvgre and parse_arp_rarp_res are exclusive parse states
  parse_nvgre and parse_arp_rarp are exclusive parse states
  parse_nvgre and parse_eompls are exclusive parse states
  parse_nvgre and parse_vxlan are exclusive parse states
  parse_nvgre and parse_geneve are exclusive parse states
  parse_nvgre and parse_sflow are exclusive parse states
  parse_nvgre and parse_set_prio_med are exclusive parse states
  parse_nvgre and parse_set_prio_high are exclusive parse states
  parse_erspan_t3 and parse_arp_rarp_req are exclusive parse states
  parse_erspan_t3 and parse_arp_rarp_res are exclusive parse states
  parse_erspan_t3 and parse_arp_rarp are exclusive parse states
  parse_erspan_t3 and parse_eompls are exclusive parse states
  parse_erspan_t3 and parse_vxlan are exclusive parse states
  parse_erspan_t3 and parse_geneve are exclusive parse states
  parse_erspan_t3 and parse_sflow are exclusive parse states
  parse_erspan_t3 and parse_set_prio_med are exclusive parse states
  parse_erspan_t3 and parse_set_prio_high are exclusive parse states
  parse_arp_rarp_req and parse_arp_rarp_res are exclusive parse states
  parse_arp_rarp_req and parse_eompls are exclusive parse states
  parse_arp_rarp_req and parse_vxlan are exclusive parse states
  parse_arp_rarp_req and parse_geneve are exclusive parse states
  parse_arp_rarp_req and parse_inner_ipv4 are exclusive parse states
  parse_arp_rarp_req and parse_inner_icmp are exclusive parse states
  parse_arp_rarp_req and parse_inner_tcp are exclusive parse states
  parse_arp_rarp_req and parse_inner_udp are exclusive parse states
  parse_arp_rarp_req and parse_inner_ipv6 are exclusive parse states
  parse_arp_rarp_req and parse_inner_ethernet are exclusive parse states
  parse_arp_rarp_req and parse_sflow are exclusive parse states
  parse_arp_rarp_req and parse_set_prio_high are exclusive parse states
  parse_arp_rarp_res and parse_eompls are exclusive parse states
  parse_arp_rarp_res and parse_vxlan are exclusive parse states
  parse_arp_rarp_res and parse_geneve are exclusive parse states
  parse_arp_rarp_res and parse_inner_ipv4 are exclusive parse states
  parse_arp_rarp_res and parse_inner_icmp are exclusive parse states
  parse_arp_rarp_res and parse_inner_tcp are exclusive parse states
  parse_arp_rarp_res and parse_inner_udp are exclusive parse states
  parse_arp_rarp_res and parse_inner_ipv6 are exclusive parse states
  parse_arp_rarp_res and parse_inner_ethernet are exclusive parse states
  parse_arp_rarp_res and parse_sflow are exclusive parse states
  parse_arp_rarp_res and parse_set_prio_high are exclusive parse states
  parse_arp_rarp and parse_eompls are exclusive parse states
  parse_arp_rarp and parse_vxlan are exclusive parse states
  parse_arp_rarp and parse_geneve are exclusive parse states
  parse_arp_rarp and parse_inner_ipv4 are exclusive parse states
  parse_arp_rarp and parse_inner_icmp are exclusive parse states
  parse_arp_rarp and parse_inner_tcp are exclusive parse states
  parse_arp_rarp and parse_inner_udp are exclusive parse states
  parse_arp_rarp and parse_inner_ipv6 are exclusive parse states
  parse_arp_rarp and parse_inner_ethernet are exclusive parse states
  parse_arp_rarp and parse_sflow are exclusive parse states
  parse_arp_rarp and parse_set_prio_high are exclusive parse states
  parse_eompls and parse_vxlan are exclusive parse states
  parse_eompls and parse_geneve are exclusive parse states
  parse_eompls and parse_sflow are exclusive parse states
  parse_eompls and parse_set_prio_med are exclusive parse states
  parse_eompls and parse_set_prio_high are exclusive parse states
  parse_vxlan and parse_geneve are exclusive parse states
  parse_vxlan and parse_sflow are exclusive parse states
  parse_vxlan and parse_set_prio_med are exclusive parse states
  parse_vxlan and parse_set_prio_high are exclusive parse states
  parse_geneve and parse_sflow are exclusive parse states
  parse_geneve and parse_set_prio_med are exclusive parse states
  parse_geneve and parse_set_prio_high are exclusive parse states
  parse_inner_ipv4 and parse_inner_ipv6 are exclusive parse states
  parse_inner_ipv4 and parse_sflow are exclusive parse states
  parse_inner_ipv4 and parse_set_prio_med are exclusive parse states
  parse_inner_ipv4 and parse_set_prio_high are exclusive parse states
  parse_inner_icmp and parse_inner_tcp are exclusive parse states
  parse_inner_icmp and parse_inner_udp are exclusive parse states
  parse_inner_icmp and parse_sflow are exclusive parse states
  parse_inner_icmp and parse_set_prio_med are exclusive parse states
  parse_inner_icmp and parse_set_prio_high are exclusive parse states
  parse_inner_tcp and parse_inner_udp are exclusive parse states
  parse_inner_tcp and parse_sflow are exclusive parse states
  parse_inner_tcp and parse_set_prio_med are exclusive parse states
  parse_inner_tcp and parse_set_prio_high are exclusive parse states
  parse_inner_udp and parse_sflow are exclusive parse states
  parse_inner_udp and parse_set_prio_med are exclusive parse states
  parse_inner_udp and parse_set_prio_high are exclusive parse states
  parse_inner_ipv6 and parse_sflow are exclusive parse states
  parse_inner_ipv6 and parse_set_prio_med are exclusive parse states
  parse_inner_ipv6 and parse_set_prio_high are exclusive parse states
  parse_inner_ethernet and parse_sflow are exclusive parse states
  parse_inner_ethernet and parse_set_prio_med are exclusive parse states
  parse_inner_ethernet and parse_set_prio_high are exclusive parse states
  parse_sflow and parse_set_prio_med are exclusive parse states
  parse_sflow and parse_set_prio_high are exclusive parse states
  parse_set_prio_med and parse_set_prio_high are exclusive parse states

>>Event 'pa_init' at time 1573971821.44
   Took 101.54 seconds
--------------------------------------------
PHV MAU Groups: 423
--------------------------------------------
Phv Mau Group (egress) -- 9 instances for total bit width of 144.
  udp.length_ <16 bits egress parsed R W>
  inner_udp.length_ <16 bits egress parsed R W>
  ipv6.payloadLen <16 bits egress parsed R W>
  ipv4.totalLen <16 bits egress parsed R W>
  l3_metadata.l3_mtu_check <16 bits egress meta R W>
  inner_ipv4.totalLen <16 bits egress parsed R W>
  egress_metadata.payload_length <16 bits egress meta R W>
  eg_intr_md.pkt_length <16 bits egress parsed imeta R W>
  inner_ipv6.payloadLen <16 bits egress parsed R W>

Phv Mau Group (egress) -- 7 instances for total bit width of 100.
  ingress_metadata.outer_bd <14 bits egress meta R>
  fabric_header_cpu.ingressBd <16 bits egress parsed W>
  ingress_metadata.bd <14 bits egress parsed meta R>
  egress_metadata.same_bd_check <14 bits egress meta R W>
  egress_metadata.bd <14 bits egress meta R W>
  egress_metadata.outer_bd <14 bits egress meta R W>
  l3_metadata.vrf <14 bits egress parsed meta R>

Phv Mau Group (ingress) -- 5 instances for total bit width of 74.
  ig_intr_md_for_tm.level2_mcast_hash <13 bits ingress imeta W>
  ig_intr_md_for_tm.level1_mcast_hash <13 bits ingress imeta W>
  hash_metadata.hash1 <16 bits ingress meta R W>
  hash_metadata.hash2 <16 bits ingress meta R W>
  hash_metadata.entropy_hash <16 bits ingress meta W>

Phv Mau Group (ingress) -- 5 instances for total bit width of 80.
  inner_ethernet.etherType <16 bits ingress parsed R>
  vlan_tag_[0].etherType <16 bits ingress parsed R>
  ethernet.etherType <16 bits ingress parsed R W>
  l2_metadata.lkp_mac_type <16 bits ingress meta R W>
  fabric_payload_header.etherType <16 bits ingress parsed R>

Phv Mau Group (ingress) -- 5 instances for total bit width of 20.
  inner_ipv6.version <4 bits ingress parsed R>
  inner_ipv4.version <4 bits ingress parsed R>
  ipv6.version <4 bits ingress parsed R>
  ipv4.version <4 bits ingress parsed R>
  l3_metadata.lkp_ip_version <4 bits ingress meta R W>

Phv Mau Group (ingress) -- 5 instances for total bit width of 80.
  l3_metadata.fib_nexthop <16 bits ingress meta R W>
  l2_metadata.l2_nexthop <16 bits ingress meta R W>
  l3_metadata.nexthop_index <16 bits ingress meta R W>
  acl_metadata.acl_nexthop <16 bits ingress meta R W>
  acl_metadata.racl_nexthop <16 bits ingress meta R W>

Phv Mau Group (ingress) -- 5 instances for total bit width of 5.
  acl_metadata.racl_nexthop_type <1 bits ingress meta R W>
  acl_metadata.acl_nexthop_type <1 bits ingress meta R W>
  l3_metadata.fib_nexthop_type <1 bits ingress meta R W>
  l2_metadata.l2_nexthop_type <1 bits ingress meta R W>
  nexthop_metadata.nexthop_type <1 bits ingress meta R W>

Phv Mau Group (egress) -- 5 instances for total bit width of 80.
  ethernet.etherType <16 bits egress parsed R W>
  fabric_payload_header.etherType <16 bits egress parsed W>
  vlan_tag_[0].etherType <16 bits egress parsed R W>
  inner_ethernet.etherType <16 bits egress parsed R W>
  gre.proto <16 bits egress parsed R W>

Phv Mau Group (egress) -- 5 instances for total bit width of 40.
  ipv6.nextHdr <8 bits egress parsed R W>
  inner_ipv6.nextHdr <8 bits egress parsed R W>
  ipv4.protocol <8 bits egress parsed R W>
  inner_ipv4.protocol <8 bits egress parsed R W>
  tunnel_metadata.inner_ip_proto <8 bits egress meta R W>

Phv Mau Group (egress) -- 4 instances for total bit width of 56.
  nvgre.flow_id <8 bits egress parsed W>
  udp.srcPort <16 bits egress parsed R W>
  inner_udp.srcPort <16 bits egress parsed R W>
  hash_metadata.entropy_hash <16 bits egress parsed meta R>

Phv Mau Group (egress) -- 4 instances for total bit width of 96.
  genv.vni <24 bits egress parsed W>
  vxlan.vni <24 bits egress parsed W>
  nvgre.tni <24 bits egress parsed W>
  tunnel_metadata.vnid <24 bits egress meta R W>

Phv Mau Group (ingress) -- 3 instances for total bit width of 48.
  ig_intr_md_for_tm.mcast_grp_b <16 bits ingress imeta W>
  multicast_metadata.multicast_route_mc_index <16 bits ingress meta R W>
  multicast_metadata.multicast_bridge_mc_index <16 bits ingress meta R W>

Phv Mau Group (ingress) -- 3 instances for total bit width of 24.
  ipv6.hopLimit <8 bits ingress parsed R>
  ipv4.ttl <8 bits ingress parsed R>
  l3_metadata.lkp_ip_ttl <8 bits ingress parsed meta R W>

Phv Mau Group (ingress) -- 3 instances for total bit width of 24.
  ipv6.nextHdr <8 bits ingress parsed R>
  ipv4.protocol <8 bits ingress parsed R>
  l3_metadata.lkp_ip_proto <8 bits ingress parsed meta R W>

Phv Mau Group (ingress) -- 3 instances for total bit width of 24.
  l3_metadata.lkp_outer_tcp_flags <8 bits ingress parsed meta R>
  tcp.flags <8 bits ingress parsed R>
  l3_metadata.lkp_tcp_flags <8 bits ingress parsed meta R W>

Phv Mau Group (ingress) -- 3 instances for total bit width of 42.
  ingress_metadata.ifindex <14 bits ingress meta R W>
  l2_metadata.l2_src_move <14 bits ingress meta R W>
  tunnel_metadata.vtep_ifindex <14 bits ingress meta R W>

Phv Mau Group (ingress) -- 3 instances for total bit width of 6.
  l3_metadata.urpf_mode <2 bits ingress meta R W>
  ipv4_metadata.ipv4_urpf_mode <2 bits ingress meta R W>
  ipv6_metadata.ipv6_urpf_mode <2 bits ingress meta R W>

Phv Mau Group (egress) -- 3 instances for total bit width of 144.
  ethernet.dstAddr <48 bits egress parsed R W>
  egress_metadata.mac_da <48 bits egress meta R W>
  inner_ethernet.dstAddr <48 bits egress parsed R W>

Phv Mau Group (egress) -- 3 instances for total bit width of 60.
  mpls[0].label <20 bits egress parsed R W>
  mpls[1].label <20 bits egress parsed R W>
  mpls[2].label <20 bits egress parsed W>

Phv Mau Group (egress) -- 3 instances for total bit width of 3.
  --validity_check--mpls[0] <1 bits egress parsed pov R W>
  --validity_check--mpls[1] <1 bits egress parsed pov R W>
  --validity_check--mpls[2] <1 bits egress parsed pov W>

Phv Mau Group (egress) -- 3 instances for total bit width of 9.
  mpls[0].exp <3 bits egress parsed R W>
  mpls[1].exp <3 bits egress parsed R W>
  mpls[2].exp <3 bits egress parsed W>

Phv Mau Group (egress) -- 3 instances for total bit width of 3.
  mpls[0].bos <1 bits egress parsed R W>
  mpls[1].bos <1 bits egress parsed R W>
  mpls[2].bos <1 bits egress parsed W>

Phv Mau Group (egress) -- 3 instances for total bit width of 24.
  mpls[0].ttl <8 bits egress parsed R W>
  mpls[1].ttl <8 bits egress parsed R W>
  mpls[2].ttl <8 bits egress parsed W>

Phv Mau Group (egress) -- 3 instances for total bit width of 96.
  fabric_header_timestamp.arrival_time <32 bits egress parsed W>
  erspan_t3_header.timestamp <32 bits egress parsed W>
  i2e_metadata.ingress_tstamp <32 bits egress parsed meta R>

Phv Mau Group (egress) -- 3 instances for total bit width of 3.
  egress_metadata.routed <1 bits egress meta R W>
  l3_metadata.outer_routed <1 bits egress meta R>
  l3_metadata.routed <1 bits egress parsed meta R>

Phv Mau Group (ingress) -- 2 instances for total bit width of 18.
  ig_intr_md.ingress_port <9 bits ingress parsed imeta R>
  ingress_metadata.ingress_port <9 bits ingress meta W>

Phv Mau Group (ingress) -- 2 instances for total bit width of 80.
  ig_intr_md.ingress_mac_tstamp <48 bits ingress parsed imeta R>
  i2e_metadata.ingress_tstamp <32 bits ingress meta R W>

Phv Mau Group (ingress) -- 2 instances for total bit width of 25.
  ig_intr_md_for_tm.ucast_egress_port <9 bits ingress imeta W>
  fabric_header.dstPortOrGroup <16 bits ingress parsed R>

Phv Mau Group (ingress) -- 2 instances for total bit width of 2.
  ig_intr_md_for_tm.disable_ucast_cutthru <1 bits ingress imeta W>
  l2_metadata.non_ip_packet <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 2 instances for total bit width of 96.
  ethernet.dstAddr <48 bits ingress parsed R>
  l2_metadata.lkp_mac_da <48 bits ingress parsed meta R W>

Phv Mau Group (ingress) -- 2 instances for total bit width of 96.
  ethernet.srcAddr <48 bits ingress parsed R>
  l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W>

Phv Mau Group (ingress) -- 2 instances for total bit width of 64.
  ipv4.srcAddr <32 bits ingress parsed R>
  ipv4_metadata.lkp_ipv4_sa <32 bits ingress parsed meta R W>

Phv Mau Group (ingress) -- 2 instances for total bit width of 64.
  ipv4.dstAddr <32 bits ingress parsed R>
  ipv4_metadata.lkp_ipv4_da <32 bits ingress parsed meta R W>

Phv Mau Group (ingress) -- 2 instances for total bit width of 256.
  ipv6.srcAddr <128 bits ingress parsed R>
  ipv6_metadata.lkp_ipv6_sa <128 bits ingress parsed meta R W>

Phv Mau Group (ingress) -- 2 instances for total bit width of 256.
  ipv6.dstAddr <128 bits ingress parsed R>
  ipv6_metadata.lkp_ipv6_da <128 bits ingress parsed meta R W>

Phv Mau Group (ingress) -- 2 instances for total bit width of 2.
  fabric_header_cpu.txBypass <1 bits ingress parsed R>
  egress_metadata.bypass <1 bits ingress meta W>

Phv Mau Group (ingress) -- 2 instances for total bit width of 2.
  fabric_header_cpu.capture_tstamp_on_tx <1 bits ingress parsed R>
  egress_metadata.capture_tstamp_on_tx <1 bits ingress meta W>

Phv Mau Group (ingress) -- 2 instances for total bit width of 28.
  ingress_metadata.bd <14 bits ingress meta R W>
  l3_metadata.same_bd_check <14 bits ingress meta R W>

Phv Mau Group (ingress) -- 2 instances for total bit width of 32.
  l3_metadata.lkp_l4_sport <16 bits ingress parsed meta R W>
  l3_metadata.lkp_outer_l4_sport <16 bits ingress parsed meta R>

Phv Mau Group (ingress) -- 2 instances for total bit width of 32.
  l3_metadata.lkp_l4_dport <16 bits ingress parsed meta R W>
  l3_metadata.lkp_outer_l4_dport <16 bits ingress parsed meta R>

Phv Mau Group (ingress) -- 2 instances for total bit width of 2.
  tunnel_metadata.tunnel_terminate <1 bits ingress meta R W>
  tunnel_metadata.tunnel_if_check <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 2 instances for total bit width of 28.
  multicast_metadata.bd_mrpf_group <14 bits ingress meta R W>
  multicast_metadata.mcast_rpf_group <14 bits ingress meta R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 2.
  --validity_check--ethernet <1 bits egress parsed pov R W>
  --validity_check--inner_ethernet <1 bits egress parsed pov R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 96.
  ethernet.srcAddr <48 bits egress parsed R W>
  inner_ethernet.srcAddr <48 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 8.
  ipv4.version <4 bits egress parsed R W>
  inner_ipv4.version <4 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 2.
  --validity_check--ipv4 <1 bits egress parsed pov R W>
  --validity_check--inner_ipv4 <1 bits egress parsed pov R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 8.
  ipv4.ihl <4 bits egress parsed R W>
  inner_ipv4.ihl <4 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 16.
  ipv4.diffserv <8 bits egress parsed R W>
  inner_ipv4.diffserv <8 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 32.
  ipv4.identification <16 bits egress parsed R W>
  inner_ipv4.identification <16 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 6.
  ipv4.flags <3 bits egress parsed R W>
  inner_ipv4.flags <3 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 26.
  ipv4.fragOffset <13 bits egress parsed R W>
  inner_ipv4.fragOffset <13 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 16.
  ipv4.ttl <8 bits egress parsed R W>
  inner_ipv4.ttl <8 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 32.
  ipv4.hdrChecksum <16 bits egress parsed R W>
  inner_ipv4.hdrChecksum <16 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 64.
  ipv4.srcAddr <32 bits egress parsed R W>
  inner_ipv4.srcAddr <32 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 64.
  ipv4.dstAddr <32 bits egress parsed R W>
  inner_ipv4.dstAddr <32 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 8.
  ipv6.version <4 bits egress parsed R W>
  inner_ipv6.version <4 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 2.
  --validity_check--ipv6 <1 bits egress parsed pov R W>
  --validity_check--inner_ipv6 <1 bits egress parsed pov R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 16.
  ipv6.trafficClass <8 bits egress parsed R W>
  inner_ipv6.trafficClass <8 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 40.
  ipv6.flowLabel <20 bits egress parsed R W>
  inner_ipv6.flowLabel <20 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 16.
  ipv6.hopLimit <8 bits egress parsed R W>
  inner_ipv6.hopLimit <8 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 256.
  ipv6.srcAddr <128 bits egress parsed R W>
  inner_ipv6.srcAddr <128 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 256.
  ipv6.dstAddr <128 bits egress parsed R W>
  inner_ipv6.dstAddr <128 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 32.
  icmp.typeCode <16 bits egress parsed R W>
  inner_icmp.typeCode <16 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 2.
  --validity_check--icmp <1 bits egress parsed pov R W>
  --validity_check--inner_icmp <1 bits egress parsed pov R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 32.
  icmp.hdrChecksum <16 bits egress parsed R W>
  inner_icmp.hdrChecksum <16 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 32.
  tcp.srcPort <16 bits egress parsed R W>
  inner_tcp.srcPort <16 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 2.
  --validity_check--tcp <1 bits egress parsed pov R W>
  --validity_check--inner_tcp <1 bits egress parsed pov R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 32.
  tcp.dstPort <16 bits egress parsed R W>
  inner_tcp.dstPort <16 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 64.
  tcp.seqNo <32 bits egress parsed R W>
  inner_tcp.seqNo <32 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 64.
  tcp.ackNo <32 bits egress parsed R W>
  inner_tcp.ackNo <32 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 8.
  tcp.dataOffset <4 bits egress parsed R W>
  inner_tcp.dataOffset <4 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 8.
  tcp.res <4 bits egress parsed R W>
  inner_tcp.res <4 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 16.
  tcp.flags <8 bits egress parsed R W>
  inner_tcp.flags <8 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 32.
  tcp.window <16 bits egress parsed R W>
  inner_tcp.window <16 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 32.
  tcp.checksum <16 bits egress parsed R W>
  inner_tcp.checksum <16 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 32.
  tcp.urgentPtr <16 bits egress parsed R W>
  inner_tcp.urgentPtr <16 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 2.
  --validity_check--udp <1 bits egress parsed pov R W>
  --validity_check--inner_udp <1 bits egress parsed pov R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 32.
  udp.dstPort <16 bits egress parsed R W>
  inner_udp.dstPort <16 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 32.
  udp.checksum <16 bits egress parsed R W>
  inner_udp.checksum <16 bits egress parsed R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 32.
  erspan_t3_header.priority_span_id <16 bits egress parsed W>
  i2e_metadata.mirror_session_id <16 bits egress parsed meta R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 25.
  fabric_header_cpu.ingressPort <16 bits egress parsed W>
  ingress_metadata.ingress_port <9 bits egress parsed meta R>

Phv Mau Group (egress) -- 2 instances for total bit width of 30.
  fabric_header_cpu.ingressIfindex <16 bits egress parsed W>
  ingress_metadata.ifindex <14 bits egress parsed meta R>

Phv Mau Group (egress) -- 2 instances for total bit width of 32.
  fabric_header_cpu.reasonCode <16 bits egress parsed W>
  fabric_metadata.reason_code <16 bits egress parsed meta R W>

Phv Mau Group (egress) -- 2 instances for total bit width of 32.
  fabric_header_timestamp.arrival_time_hi <16 bits egress parsed W>
  i2e_metadata.ingress_tstamp_hi <16 bits egress parsed meta R>

Phv Mau Group (egress) -- 2 instances for total bit width of 2.
  egress_metadata.capture_tstamp_on_tx <1 bits egress parsed meta R>
  eg_intr_md_for_oport.capture_tstamp_on_tx <1 bits egress imeta W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  ig_intr_md.resubmit_flag <1 bits ingress parsed imeta R>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  ig_intr_md._pad1 <1 bits ingress parsed imeta>

Phv Mau Group (ingress) -- 1 instance for total bit width of 2.
  ig_intr_md._pad2 <2 bits ingress parsed imeta>

Phv Mau Group (ingress) -- 1 instance for total bit width of 3.
  ig_intr_md._pad3 <3 bits ingress parsed imeta>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  ig_intr_md_from_parser_aux.ingress_parser_err <16 bits ingress parsed imeta R>

Phv Mau Group (ingress) -- 1 instance for total bit width of 3.
  ig_intr_md_for_tm.drop_ctl <3 bits ingress imeta W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 3.
  ig_intr_md_for_tm.ingress_cos <3 bits ingress imeta W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 5.
  ig_intr_md_for_tm.qid <5 bits ingress imeta W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  ig_intr_md_for_tm.copy_to_cpu <1 bits ingress imeta W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 2.
  ig_intr_md_for_tm.packet_color <2 bits ingress imeta W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  ig_intr_md_for_tm.level1_exclusion_id <16 bits ingress imeta W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 9.
  ig_intr_md_for_tm.level2_exclusion_id <9 bits ingress imeta W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  ig_intr_md_for_tm.rid <16 bits ingress imeta W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 10.
  ig_intr_md_for_mb.ingress_mirror_id <10 bits ingress imeta W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--ethernet <1 bits ingress parsed pov R>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  llc_header.dsap <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--llc_header <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  llc_header.ssap <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  llc_header.control_ <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 24.
  snap_header.oui <24 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--snap_header <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  snap_header.type_ <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 3.
  vlan_tag_[0].pcp <3 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--vlan_tag_[0] <1 bits ingress parsed pov R>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  vlan_tag_[0].cfi <1 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 12.
  vlan_tag_[0].vid <12 bits ingress parsed R>

Phv Mau Group (ingress) -- 1 instance for total bit width of 3.
  vlan_tag_[1].pcp <3 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--vlan_tag_[1] <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  vlan_tag_[1].cfi <1 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 12.
  vlan_tag_[1].vid <12 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  vlan_tag_[1].etherType <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 20.
  mpls[0].label <20 bits ingress parsed R>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--mpls[0] <1 bits ingress parsed pov R>

Phv Mau Group (ingress) -- 1 instance for total bit width of 3.
  mpls[0].exp <3 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  mpls[0].bos <1 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  mpls[0].ttl <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 20.
  mpls[1].label <20 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--mpls[1] <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 3.
  mpls[1].exp <3 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  mpls[1].bos <1 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  mpls[1].ttl <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 20.
  mpls[2].label <20 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--mpls[2] <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 3.
  mpls[2].exp <3 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  mpls[2].bos <1 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  mpls[2].ttl <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--ipv4 <1 bits ingress parsed pov R>

Phv Mau Group (ingress) -- 1 instance for total bit width of 4.
  ipv4.ihl <4 bits ingress parsed R>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  ipv4.diffserv <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  ipv4.totalLen <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  ipv4.identification <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 3.
  ipv4.flags <3 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 13.
  ipv4.fragOffset <13 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  ipv4.hdrChecksum <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 32.
  ipv4_option_32b.option_fields <32 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--ipv4_option_32b <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--ipv6 <1 bits ingress parsed pov R>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  ipv6.trafficClass <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 20.
  ipv6.flowLabel <20 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  ipv6.payloadLen <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  icmp.typeCode <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--icmp <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  icmp.hdrChecksum <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  igmp.typeCode <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--igmp <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  igmp.hdrChecksum <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  tcp.srcPort <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--tcp <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  tcp.dstPort <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 32.
  tcp.seqNo <32 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 32.
  tcp.ackNo <32 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 4.
  tcp.dataOffset <4 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 4.
  tcp.res <4 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  tcp.window <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  tcp.checksum <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  tcp.urgentPtr <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  udp.srcPort <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--udp <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  udp.dstPort <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  udp.length_ <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  udp.checksum <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  gre.C <1 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--gre <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  gre.R <1 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  gre.K <1 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  gre.S <1 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  gre.s <1 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 3.
  gre.recurse <3 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 5.
  gre.flags <5 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 3.
  gre.ver <3 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  gre.proto <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 24.
  nvgre.tni <24 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--nvgre <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  nvgre.flow_id <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 48.
  inner_ethernet.dstAddr <48 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--inner_ethernet <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 48.
  inner_ethernet.srcAddr <48 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--inner_ipv4 <1 bits ingress parsed pov R>

Phv Mau Group (ingress) -- 1 instance for total bit width of 4.
  inner_ipv4.ihl <4 bits ingress parsed R>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  inner_ipv4.diffserv <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  inner_ipv4.totalLen <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  inner_ipv4.identification <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 3.
  inner_ipv4.flags <3 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 13.
  inner_ipv4.fragOffset <13 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  inner_ipv4.ttl <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  inner_ipv4.protocol <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  inner_ipv4.hdrChecksum <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 32.
  inner_ipv4.srcAddr <32 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 32.
  inner_ipv4.dstAddr <32 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--inner_ipv6 <1 bits ingress parsed pov R>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  inner_ipv6.trafficClass <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 20.
  inner_ipv6.flowLabel <20 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  inner_ipv6.payloadLen <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  inner_ipv6.nextHdr <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  inner_ipv6.hopLimit <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 128.
  inner_ipv6.srcAddr <128 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 128.
  inner_ipv6.dstAddr <128 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 4.
  erspan_t3_header.version <4 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--erspan_t3_header <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 12.
  erspan_t3_header.vlan <12 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  erspan_t3_header.priority_span_id <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 32.
  erspan_t3_header.timestamp <32 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  erspan_t3_header.sgt <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  erspan_t3_header.ft_d_other <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  vxlan.flags <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--vxlan <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 24.
  vxlan.reserved <24 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 24.
  vxlan.vni <24 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  vxlan.reserved2 <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 2.
  genv.ver <2 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--genv <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 6.
  genv.optLen <6 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  genv.oam <1 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  genv.critical <1 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 6.
  genv.reserved <6 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  genv.protoType <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 24.
  genv.vni <24 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  genv.reserved2 <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  inner_icmp.typeCode <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--inner_icmp <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  inner_icmp.hdrChecksum <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  inner_tcp.srcPort <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--inner_tcp <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  inner_tcp.dstPort <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 32.
  inner_tcp.seqNo <32 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 32.
  inner_tcp.ackNo <32 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 4.
  inner_tcp.dataOffset <4 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 4.
  inner_tcp.res <4 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  inner_tcp.flags <8 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  inner_tcp.window <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  inner_tcp.checksum <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  inner_tcp.urgentPtr <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  inner_udp.srcPort <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--inner_udp <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  inner_udp.dstPort <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  inner_udp.length_ <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  inner_udp.checksum <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 3.
  fabric_header.packetType <3 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--fabric_header <1 bits ingress parsed pov W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 2.
  fabric_header.headerVersion <2 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 2.
  fabric_header.packetVersion <2 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  fabric_header.pad1 <1 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 3.
  fabric_header.fabricColor <3 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 5.
  fabric_header.fabricQos <5 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  fabric_header.dstDevice <8 bits ingress parsed R>

Phv Mau Group (ingress) -- 1 instance for total bit width of 5.
  fabric_header_cpu.egressQueue <5 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--fabric_header_cpu <1 bits ingress parsed pov R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  fabric_header_cpu.reserved <1 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  fabric_header_cpu.ingressPort <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  fabric_header_cpu.ingressIfindex <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  fabric_header_cpu.ingressBd <16 bits ingress parsed R>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  fabric_header_cpu.reasonCode <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--fabric_payload_header <1 bits ingress parsed pov W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  fabric_header_timestamp.arrival_time_hi <16 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--fabric_header_timestamp <1 bits ingress parsed pov>

Phv Mau Group (ingress) -- 1 instance for total bit width of 32.
  fabric_header_timestamp.arrival_time <32 bits ingress parsed tagalong>

Phv Mau Group (ingress) -- 1 instance for total bit width of 10.
  ingress_metadata.port_lag_index <10 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 10.
  ingress_metadata.egress_port_lag_index <10 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 14.
  ingress_metadata.egress_ifindex <14 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 2.
  ingress_metadata.port_type <2 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  ingress_metadata.drop_flag <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  ingress_metadata.drop_reason <8 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  ingress_metadata.bypass_lookups <8 bits ingress parsed meta R>

Phv Mau Group (ingress) -- 1 instance for total bit width of 3.
  l2_metadata.lkp_pkt_type <3 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 2.
  l2_metadata.arp_opcode <2 bits ingress parsed meta R>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  l2_metadata.l2_redirect <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  l2_metadata.l2_src_miss <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  l2_metadata.l2_dst_miss <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 10.
  l2_metadata.stp_group <10 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 3.
  l2_metadata.stp_state <3 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 14.
  l2_metadata.bd_stats_idx <14 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  l2_metadata.learning_enabled <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  l2_metadata.port_learning_enabled <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  l2_metadata.port_vlan_mapping_miss <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 14.
  l2_metadata.same_if_check <14 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 2.
  l3_metadata.lkp_ip_type <2 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  l3_metadata.lkp_ip_llmc <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  l3_metadata.lkp_ip_mc <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 14.
  l3_metadata.vrf <14 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 10.
  l3_metadata.rmac_group <10 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  l3_metadata.rmac_hit <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  l3_metadata.urpf_hit <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  l3_metadata.urpf_check_fail <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 14.
  l3_metadata.urpf_bd_group <14 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  l3_metadata.fib_hit <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  l3_metadata.fib_hit_myip <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  l3_metadata.routed <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  l3_metadata.l3_copy <1 bits ingress meta R>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  ipv4_metadata.ipv4_unicast_enabled <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  ipv6_metadata.ipv6_unicast_enabled <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  ipv6_metadata.ipv6_src_is_link_local <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 5.
  tunnel_metadata.ingress_tunnel_type <5 bits ingress parsed meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 24.
  tunnel_metadata.tunnel_vni <24 bits ingress parsed meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  tunnel_metadata.tunnel_dst_index <16 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  tunnel_metadata.tunnel_lookup <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  tunnel_metadata.src_vtep_hit <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  tunnel_metadata.tunnel_term_type <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  acl_metadata.acl_deny <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  acl_metadata.racl_deny <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  acl_metadata.acl_redirect <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  acl_metadata.racl_redirect <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  acl_metadata.port_lag_label <16 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  acl_metadata.bd_label <16 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 12.
  acl_metadata.acl_stats_index <12 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 12.
  acl_metadata.racl_stats_index <12 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  acl_metadata.ingress_src_port_range_id <8 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 8.
  acl_metadata.ingress_dst_port_range_id <8 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  i2e_metadata.ingress_tstamp_hi <16 bits ingress meta W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  i2e_metadata.mirror_session_id <16 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  multicast_metadata.mcast_route_hit <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  multicast_metadata.mcast_route_s_g_hit <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  multicast_metadata.mcast_bridge_hit <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  multicast_metadata.mcast_copy_to_cpu <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  multicast_metadata.ipv4_multicast_enabled <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  multicast_metadata.ipv6_multicast_enabled <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  multicast_metadata.igmp_snooping_enabled <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  multicast_metadata.mld_snooping_enabled <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  multicast_metadata.mcast_rpf_fail <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  multicast_metadata.flood_to_mrouters <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 2.
  multicast_metadata.mcast_mode <2 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  nexthop_metadata.nexthop_glean <1 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 16.
  fabric_metadata.reason_code <16 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 2.
  meter_metadata.storm_control_color <2 bits ingress meta R W>

Phv Mau Group (ingress) -- 1 instance for total bit width of 1.
  --validity_check--metadata_bridge <1 bits ingress parsed pov>

Phv Mau Group (egress) -- 1 instance for total bit width of 8.
  llc_header.dsap <8 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  --validity_check--llc_header <1 bits egress parsed pov>

Phv Mau Group (egress) -- 1 instance for total bit width of 8.
  llc_header.ssap <8 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 8.
  llc_header.control_ <8 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 24.
  snap_header.oui <24 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  --validity_check--snap_header <1 bits egress parsed pov>

Phv Mau Group (egress) -- 1 instance for total bit width of 16.
  snap_header.type_ <16 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 3.
  vlan_tag_[0].pcp <3 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  --validity_check--vlan_tag_[0] <1 bits egress parsed pov R W>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  vlan_tag_[0].cfi <1 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 12.
  vlan_tag_[0].vid <12 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 3.
  vlan_tag_[1].pcp <3 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  --validity_check--vlan_tag_[1] <1 bits egress parsed pov>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  vlan_tag_[1].cfi <1 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 12.
  vlan_tag_[1].vid <12 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 16.
  vlan_tag_[1].etherType <16 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 32.
  ipv4_option_32b.option_fields <32 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  --validity_check--ipv4_option_32b <1 bits egress parsed pov>

Phv Mau Group (egress) -- 1 instance for total bit width of 16.
  igmp.typeCode <16 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  --validity_check--igmp <1 bits egress parsed pov>

Phv Mau Group (egress) -- 1 instance for total bit width of 16.
  igmp.hdrChecksum <16 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  gre.C <1 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  --validity_check--gre <1 bits egress parsed pov W>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  gre.R <1 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  gre.K <1 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  gre.S <1 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  gre.s <1 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 3.
  gre.recurse <3 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 5.
  gre.flags <5 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 3.
  gre.ver <3 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  --validity_check--nvgre <1 bits egress parsed pov W>

Phv Mau Group (egress) -- 1 instance for total bit width of 4.
  erspan_t3_header.version <4 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  --validity_check--erspan_t3_header <1 bits egress parsed pov W>

Phv Mau Group (egress) -- 1 instance for total bit width of 12.
  erspan_t3_header.vlan <12 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 16.
  erspan_t3_header.sgt <16 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 16.
  erspan_t3_header.ft_d_other <16 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 8.
  vxlan.flags <8 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  --validity_check--vxlan <1 bits egress parsed pov W>

Phv Mau Group (egress) -- 1 instance for total bit width of 24.
  vxlan.reserved <24 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 8.
  vxlan.reserved2 <8 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 2.
  genv.ver <2 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  --validity_check--genv <1 bits egress parsed pov W>

Phv Mau Group (egress) -- 1 instance for total bit width of 6.
  genv.optLen <6 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  genv.oam <1 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  genv.critical <1 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 6.
  genv.reserved <6 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 16.
  genv.protoType <16 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 8.
  genv.reserved2 <8 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 3.
  fabric_header.packetType <3 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  --validity_check--fabric_header <1 bits egress parsed pov W>

Phv Mau Group (egress) -- 1 instance for total bit width of 2.
  fabric_header.headerVersion <2 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 2.
  fabric_header.packetVersion <2 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  fabric_header.pad1 <1 bits egress parsed W>

Phv Mau Group (egress) -- 1 instance for total bit width of 3.
  fabric_header.fabricColor <3 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 5.
  fabric_header.fabricQos <5 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 8.
  fabric_header.dstDevice <8 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 16.
  fabric_header.dstPortOrGroup <16 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 5.
  fabric_header_cpu.egressQueue <5 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  --validity_check--fabric_header_cpu <1 bits egress parsed pov W>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  fabric_header_cpu.txBypass <1 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  fabric_header_cpu.capture_tstamp_on_tx <1 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  fabric_header_cpu.reserved <1 bits egress parsed tagalong>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  --validity_check--fabric_header_timestamp <1 bits egress parsed pov W>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  --validity_check--fabric_payload_header <1 bits egress parsed pov W>

Phv Mau Group (egress) -- 1 instance for total bit width of 9.
  eg_intr_md.egress_port <9 bits egress parsed imeta R>

Phv Mau Group (egress) -- 1 instance for total bit width of 2.
  egress_metadata.port_type <2 bits egress meta R W>

Phv Mau Group (egress) -- 1 instance for total bit width of 14.
  ingress_metadata.egress_ifindex <14 bits egress parsed meta R W>

Phv Mau Group (egress) -- 1 instance for total bit width of 3.
  l2_metadata.lkp_pkt_type <3 bits egress parsed meta R>

Phv Mau Group (egress) -- 1 instance for total bit width of 9.
  egress_metadata.smac_idx <9 bits egress meta R W>

Phv Mau Group (egress) -- 1 instance for total bit width of 8.
  l3_metadata.mtu_index <8 bits egress meta R W>

Phv Mau Group (egress) -- 1 instance for total bit width of 8.
  tunnel_metadata.tunnel_smac_index <8 bits egress meta R W>

Phv Mau Group (egress) -- 1 instance for total bit width of 4.
  eg_intr_md_from_parser_aux.clone_src <4 bits egress parsed imeta R>

Phv Mau Group (egress) -- 1 instance for total bit width of 5.
  tunnel_metadata.ingress_tunnel_type <5 bits egress parsed meta R>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  multicast_metadata.inner_replica <1 bits egress meta R W>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  multicast_metadata.replica <1 bits egress meta R W>

Phv Mau Group (egress) -- 1 instance for total bit width of 5.
  tunnel_metadata.egress_tunnel_type <5 bits egress meta R W>

Phv Mau Group (egress) -- 1 instance for total bit width of 4.
  tunnel_metadata.egress_header_count <4 bits egress meta R W>

Phv Mau Group (egress) -- 1 instance for total bit width of 14.
  tunnel_metadata.tunnel_index <14 bits egress meta R W>

Phv Mau Group (egress) -- 1 instance for total bit width of 16.
  tunnel_metadata.tunnel_dst_index <16 bits egress parsed meta R>

Phv Mau Group (egress) -- 1 instance for total bit width of 12.
  tunnel_metadata.tunnel_dmac_index <12 bits egress meta R W>

Phv Mau Group (egress) -- 1 instance for total bit width of 2.
  ig_intr_md_for_tm.packet_color <2 bits egress parsed imeta R>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  eg_intr_md.deflection_flag <1 bits egress parsed imeta R>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  egress_metadata.bypass <1 bits egress parsed meta R>

Phv Mau Group (egress) -- 1 instance for total bit width of 3.
  eg_intr_md_for_oport.drop_ctl <3 bits egress imeta W>

Phv Mau Group (egress) -- 1 instance for total bit width of 10.
  eg_intr_md_for_mb.egress_mirror_id <10 bits egress imeta W>

Phv Mau Group (egress) -- 1 instance for total bit width of 16.
  eg_intr_md.egress_rid <16 bits egress parsed imeta R>

Phv Mau Group (egress) -- 1 instance for total bit width of 16.
  l3_metadata.nexthop_index <16 bits egress parsed meta R>

Phv Mau Group (egress) -- 1 instance for total bit width of 4.
  eg_intr_md_from_parser_aux.clone_digest_id <4 bits egress parsed imeta R>

Phv Mau Group (egress) -- 1 instance for total bit width of 1.
  tunnel_metadata.tunnel_terminate <1 bits egress parsed meta R>

Phv Mau Group (egress) -- 1 instance for total bit width of 7.
  eg_intr_md._pad0 <7 bits egress parsed imeta>

Phv Mau Group (egress) -- 1 instance for total bit width of 5.
  eg_intr_md._pad7 <5 bits egress parsed imeta>

Phv Mau Group (egress) -- 1 instance for total bit width of 3.
  eg_intr_md.egress_cos <3 bits egress parsed imeta>

Phv Mau Group (egress) -- 1 instance for total bit width of 7.
  eg_intr_md._pad8 <7 bits egress parsed imeta>

Phv Mau Group (egress) -- 1 instance for total bit width of 6.
  eg_intr_md_for_mb._pad1 <6 bits egress imeta>


>>Event 'pa_resv' at time 1573971821.47
   Took 0.03 seconds

-----------------------------------------------
  Container reservations
-----------------------------------------------
Allocation Step
ingress reservations:
   8-bit containers: 2
   16-bit containers: 0
   32-bit containers: 0

MAU groups: 3
  Group 4 8 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv64
  Group 6 8 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv96
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv112
Reserving 8-bit container for ingress: phv64

MAU groups: 3
  Group 4 8 bits -- avail 15 -- ingress avail 15 and remain 14 and promised 1 and req 1 -- egress avail 8 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv65
  Group 6 8 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv96
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv112
Reserving 8-bit container for ingress: phv65
egress reservations:
   8-bit containers: 1
   16-bit containers: 0
   32-bit containers: 0

MAU groups: 3
  Group 5 8 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv80
  Group 6 8 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv96
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv112
Reserving 8-bit container for egress: phv80

-----------------------------------------------
  Tagalong container reservations
-----------------------------------------------
Allocation Step
ingress reservations:
   8-bit containers: 0
   16-bit containers: 0
   32-bit containers: 0
egress reservations:
   8-bit containers: 0
   16-bit containers: 0
   32-bit containers: 0
   None required.

-----------------------------------------------
  POV bit index reservations
-----------------------------------------------
Allocation Step
POV bit indicies requested for ingress: [3]

MAU groups: 3
  Group 4 8 bits -- avail 14 -- ingress avail 14 and remain 13 and promised 1 and req 1 -- egress avail 8 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv66
  Group 6 8 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv96
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv112
Reserving 8-bit container for ingress: phv66
>>Event 'pa_bridge' at time 1573971821.72
   Took 0.25 seconds

-----------------------------------------------
  Allocating fields related to bridged metadata
-----------------------------------------------
Allocation Step
  ig_intr_md_for_tm.packet_color <2 bits ingress imeta W> and ig_intr_md_for_tm.packet_color <2 bits egress parsed imeta R>
  ingress_metadata.ingress_port <9 bits ingress meta W> and ingress_metadata.ingress_port <9 bits egress parsed meta R>
  ingress_metadata.ifindex <14 bits ingress meta R W> and ingress_metadata.ifindex <14 bits egress parsed meta R>
  ingress_metadata.egress_ifindex <14 bits ingress meta R W> and ingress_metadata.egress_ifindex <14 bits egress parsed meta R W>
  ingress_metadata.bd <14 bits ingress meta R W> and ingress_metadata.bd <14 bits egress parsed meta R>
  egress_metadata.capture_tstamp_on_tx <1 bits ingress meta W> and egress_metadata.capture_tstamp_on_tx <1 bits egress parsed meta R>
  egress_metadata.bypass <1 bits ingress meta W> and egress_metadata.bypass <1 bits egress parsed meta R>
  l2_metadata.lkp_pkt_type <3 bits ingress meta R W> and l2_metadata.lkp_pkt_type <3 bits egress parsed meta R>
  l3_metadata.vrf <14 bits ingress meta R W> and l3_metadata.vrf <14 bits egress parsed meta R>
  l3_metadata.nexthop_index <16 bits ingress meta R W> and l3_metadata.nexthop_index <16 bits egress parsed meta R>
  l3_metadata.routed <1 bits ingress meta R W> and l3_metadata.routed <1 bits egress parsed meta R>
  tunnel_metadata.ingress_tunnel_type <5 bits ingress parsed meta R W> and tunnel_metadata.ingress_tunnel_type <5 bits egress parsed meta R>
  tunnel_metadata.tunnel_dst_index <16 bits ingress meta R W> and tunnel_metadata.tunnel_dst_index <16 bits egress parsed meta R>
  tunnel_metadata.tunnel_terminate <1 bits ingress meta R W> and tunnel_metadata.tunnel_terminate <1 bits egress parsed meta R>
  i2e_metadata.ingress_tstamp <32 bits ingress meta R W> and i2e_metadata.ingress_tstamp <32 bits egress parsed meta R>
  i2e_metadata.ingress_tstamp_hi <16 bits ingress meta W> and i2e_metadata.ingress_tstamp_hi <16 bits egress parsed meta R>
  i2e_metadata.mirror_session_id <16 bits ingress meta R W> and i2e_metadata.mirror_session_id <16 bits egress parsed meta R W>
  fabric_metadata.reason_code <16 bits ingress meta R W> and fabric_metadata.reason_code <16 bits egress parsed meta R W>
  hash_metadata.entropy_hash <16 bits ingress meta W> and hash_metadata.entropy_hash <16 bits egress parsed meta R>


Allowed alignment for fields:
  ig_intr_md_for_tm.packet_color -> [0, 1, 2, 3, 4, 5, 6]
  ingress_metadata.ingress_port -> [0, 8, 16, 24]
  ingress_metadata.ifindex -> [0, 8, 16, 24]
  ingress_metadata.egress_ifindex -> any container bit
  ingress_metadata.bd -> [0, 8, 16, 24]
  egress_metadata.capture_tstamp_on_tx -> any container bit
  egress_metadata.bypass -> any container bit
  l2_metadata.lkp_pkt_type -> any container bit
  l3_metadata.vrf -> any container bit
  l3_metadata.nexthop_index -> any container bit
  l3_metadata.routed -> any container bit
  tunnel_metadata.ingress_tunnel_type -> [0, 8, 16, 24]
  tunnel_metadata.tunnel_dst_index -> any container bit
  tunnel_metadata.tunnel_terminate -> any container bit
  i2e_metadata.ingress_tstamp -> [0, 8, 16, 24]
  i2e_metadata.ingress_tstamp_hi -> any container bit
  i2e_metadata.mirror_session_id -> [0, 8, 16, 24]
  fabric_metadata.reason_code -> [0, 8, 16, 24]
  hash_metadata.entropy_hash -> any container bit

Required packing for bridged metadata: 0
ig_intr_md_for_tm.packet_color can share with fields: (9) : total bits 51
  ingress_metadata.ingress_port / 9 bits
  ingress_metadata.egress_ifindex / 14 bits
  egress_metadata.capture_tstamp_on_tx / 1 bits
  egress_metadata.bypass / 1 bits
  l2_metadata.lkp_pkt_type / 3 bits
  l3_metadata.vrf / 14 bits
  l3_metadata.routed / 1 bits
  tunnel_metadata.ingress_tunnel_type / 5 bits
  tunnel_metadata.tunnel_terminate / 1 bits
ingress_metadata.ingress_port can share with fields: (7) : total bits 34
  ig_intr_md_for_tm.packet_color / 2 bits
  ingress_metadata.egress_ifindex / 14 bits
  egress_metadata.capture_tstamp_on_tx / 1 bits
  egress_metadata.bypass / 1 bits
  l3_metadata.routed / 1 bits
  tunnel_metadata.ingress_tunnel_type / 5 bits
  tunnel_metadata.tunnel_terminate / 1 bits
ingress_metadata.egress_ifindex can share with fields: (8) : total bits 48
  ig_intr_md_for_tm.packet_color / 2 bits
  ingress_metadata.ingress_port / 9 bits
  egress_metadata.capture_tstamp_on_tx / 1 bits
  egress_metadata.bypass / 1 bits
  l3_metadata.vrf / 14 bits
  l3_metadata.routed / 1 bits
  tunnel_metadata.ingress_tunnel_type / 5 bits
  tunnel_metadata.tunnel_terminate / 1 bits
egress_metadata.capture_tstamp_on_tx can share with fields: (7) : total bits 34
  ig_intr_md_for_tm.packet_color / 2 bits
  ingress_metadata.ingress_port / 9 bits
  ingress_metadata.egress_ifindex / 14 bits
  egress_metadata.bypass / 1 bits
  l3_metadata.routed / 1 bits
  tunnel_metadata.ingress_tunnel_type / 5 bits
  tunnel_metadata.tunnel_terminate / 1 bits
egress_metadata.bypass can share with fields: (7) : total bits 34
  ig_intr_md_for_tm.packet_color / 2 bits
  ingress_metadata.ingress_port / 9 bits
  ingress_metadata.egress_ifindex / 14 bits
  egress_metadata.capture_tstamp_on_tx / 1 bits
  l3_metadata.routed / 1 bits
  tunnel_metadata.ingress_tunnel_type / 5 bits
  tunnel_metadata.tunnel_terminate / 1 bits
l2_metadata.lkp_pkt_type can share with fields: (4) : total bits 12
  ig_intr_md_for_tm.packet_color / 2 bits
  l3_metadata.routed / 1 bits
  tunnel_metadata.ingress_tunnel_type / 5 bits
  tunnel_metadata.tunnel_terminate / 1 bits
l3_metadata.vrf can share with fields: (5) : total bits 37
  ig_intr_md_for_tm.packet_color / 2 bits
  ingress_metadata.egress_ifindex / 14 bits
  l3_metadata.routed / 1 bits
  tunnel_metadata.ingress_tunnel_type / 5 bits
  tunnel_metadata.tunnel_terminate / 1 bits
l3_metadata.routed can share with fields: (9) : total bits 51
  ig_intr_md_for_tm.packet_color / 2 bits
  ingress_metadata.ingress_port / 9 bits
  ingress_metadata.egress_ifindex / 14 bits
  egress_metadata.capture_tstamp_on_tx / 1 bits
  egress_metadata.bypass / 1 bits
  l2_metadata.lkp_pkt_type / 3 bits
  l3_metadata.vrf / 14 bits
  tunnel_metadata.ingress_tunnel_type / 5 bits
  tunnel_metadata.tunnel_terminate / 1 bits
tunnel_metadata.ingress_tunnel_type can share with fields: (8) : total bits 50
  ig_intr_md_for_tm.packet_color / 2 bits
  ingress_metadata.ingress_port / 9 bits
  ingress_metadata.egress_ifindex / 14 bits
  egress_metadata.capture_tstamp_on_tx / 1 bits
  egress_metadata.bypass / 1 bits
  l2_metadata.lkp_pkt_type / 3 bits
  l3_metadata.vrf / 14 bits
  l3_metadata.routed / 1 bits
tunnel_metadata.tunnel_terminate can share with fields: (8) : total bits 46
  ig_intr_md_for_tm.packet_color / 2 bits
  ingress_metadata.ingress_port / 9 bits
  ingress_metadata.egress_ifindex / 14 bits
  egress_metadata.capture_tstamp_on_tx / 1 bits
  egress_metadata.bypass / 1 bits
  l2_metadata.lkp_pkt_type / 3 bits
  l3_metadata.vrf / 14 bits
  l3_metadata.routed / 1 bits
ingress_metadata.ifindex cannot share with any fields:  total bits 14
ingress_metadata.bd cannot share with any fields:  total bits 14


Created preliminary group: ['l3_metadata.vrf', 'l3_metadata.routed', 'tunnel_metadata.tunnel_terminate']
Created preliminary group: ['ingress_metadata.egress_ifindex', 'egress_metadata.capture_tstamp_on_tx', 'egress_metadata.bypass']
Remaining fields: ['ig_intr_md_for_tm.packet_color', 'l2_metadata.lkp_pkt_type', 'tunnel_metadata.ingress_tunnel_type', 'ingress_metadata.ingress_port']
All combinations = 256
Valid combinations = 144
Choosing to pack non-byte multiple metadata as below, which wastes 11 bits
  Group 0 (5 bits)
    ig_intr_md_for_tm.packet_color
    l2_metadata.lkp_pkt_type
  Group 1 (14 bits)
    tunnel_metadata.ingress_tunnel_type
    ingress_metadata.ingress_port
  Group 2 (16 bits)
    l3_metadata.vrf
    l3_metadata.routed
    tunnel_metadata.tunnel_terminate
  Group 3 (16 bits)
    ingress_metadata.egress_ifindex
    egress_metadata.capture_tstamp_on_tx
    egress_metadata.bypass

Sharing capabilities of groups: (13)
Group ['tunnel_metadata.tunnel_dst_index'] can share with 9 other groups:
  ['l3_metadata.nexthop_index']
  ['i2e_metadata.ingress_tstamp']
  ['i2e_metadata.ingress_tstamp_hi']
  ['i2e_metadata.mirror_session_id']
  ['hash_metadata.entropy_hash']
  ['ig_intr_md_for_tm.packet_color', 'l2_metadata.lkp_pkt_type']
  ['tunnel_metadata.ingress_tunnel_type', 'ingress_metadata.ingress_port']
  ['l3_metadata.vrf', 'l3_metadata.routed', 'tunnel_metadata.tunnel_terminate']
  ['ingress_metadata.egress_ifindex', 'egress_metadata.capture_tstamp_on_tx', 'egress_metadata.bypass']
Group ['i2e_metadata.mirror_session_id'] can share with 7 other groups:
  ['l3_metadata.nexthop_index']
  ['tunnel_metadata.tunnel_dst_index']
  ['i2e_metadata.ingress_tstamp']
  ['i2e_metadata.ingress_tstamp_hi']
  ['hash_metadata.entropy_hash']
  ['tunnel_metadata.ingress_tunnel_type', 'ingress_metadata.ingress_port']
  ['l3_metadata.vrf', 'l3_metadata.routed', 'tunnel_metadata.tunnel_terminate']
Group ['l3_metadata.nexthop_index'] can share with 6 other groups:
  ['tunnel_metadata.tunnel_dst_index']
  ['i2e_metadata.ingress_tstamp']
  ['i2e_metadata.ingress_tstamp_hi']
  ['i2e_metadata.mirror_session_id']
  ['ig_intr_md_for_tm.packet_color', 'l2_metadata.lkp_pkt_type']
  ['tunnel_metadata.ingress_tunnel_type', 'ingress_metadata.ingress_port']
Group ['i2e_metadata.ingress_tstamp'] can share with 6 other groups:
  ['l3_metadata.nexthop_index']
  ['tunnel_metadata.tunnel_dst_index']
  ['i2e_metadata.mirror_session_id']
  ['hash_metadata.entropy_hash']
  ['tunnel_metadata.ingress_tunnel_type', 'ingress_metadata.ingress_port']
  ['ingress_metadata.egress_ifindex', 'egress_metadata.capture_tstamp_on_tx', 'egress_metadata.bypass']
Group ['hash_metadata.entropy_hash'] can share with 6 other groups:
  ['tunnel_metadata.tunnel_dst_index']
  ['i2e_metadata.ingress_tstamp']
  ['i2e_metadata.ingress_tstamp_hi']
  ['i2e_metadata.mirror_session_id']
  ['ig_intr_md_for_tm.packet_color', 'l2_metadata.lkp_pkt_type']
  ['tunnel_metadata.ingress_tunnel_type', 'ingress_metadata.ingress_port']
Group ['tunnel_metadata.ingress_tunnel_type', 'ingress_metadata.ingress_port'] can share with 6 other groups:
  ['l3_metadata.nexthop_index']
  ['tunnel_metadata.tunnel_dst_index']
  ['i2e_metadata.ingress_tstamp']
  ['i2e_metadata.mirror_session_id']
  ['hash_metadata.entropy_hash']
  ['ingress_metadata.egress_ifindex', 'egress_metadata.capture_tstamp_on_tx', 'egress_metadata.bypass']
Group ['i2e_metadata.ingress_tstamp_hi'] can share with 5 other groups:
  ['l3_metadata.nexthop_index']
  ['tunnel_metadata.tunnel_dst_index']
  ['i2e_metadata.mirror_session_id']
  ['hash_metadata.entropy_hash']
  ['ingress_metadata.egress_ifindex', 'egress_metadata.capture_tstamp_on_tx', 'egress_metadata.bypass']
Group ['ingress_metadata.egress_ifindex', 'egress_metadata.capture_tstamp_on_tx', 'egress_metadata.bypass'] can share with 4 other groups:
  ['tunnel_metadata.tunnel_dst_index']
  ['i2e_metadata.ingress_tstamp']
  ['i2e_metadata.ingress_tstamp_hi']
  ['tunnel_metadata.ingress_tunnel_type', 'ingress_metadata.ingress_port']
Group ['ig_intr_md_for_tm.packet_color', 'l2_metadata.lkp_pkt_type'] can share with 3 other groups:
  ['l3_metadata.nexthop_index']
  ['tunnel_metadata.tunnel_dst_index']
  ['hash_metadata.entropy_hash']
Group ['l3_metadata.vrf', 'l3_metadata.routed', 'tunnel_metadata.tunnel_terminate'] can share with 2 other groups:
  ['tunnel_metadata.tunnel_dst_index']
  ['i2e_metadata.mirror_session_id']
Group ['fabric_metadata.reason_code'] can share with 0 other groups:
Group ['ingress_metadata.ifindex'] can share with 0 other groups:
Group ['ingress_metadata.bd'] can share with 0 other groups:
Could still parse ingress? True
Could still parse egress? False

Reducing egress container size to 16 and trying again for ['i2e_metadata.mirror_session_id', 'i2e_metadata.ingress_tstamp_hi'].

Merged sharing capabilities of groups: (11)
Group ['tunnel_metadata.tunnel_dst_index', 'l3_metadata.nexthop_index'] can share with 4 other groups (32 bits):
  ['i2e_metadata.mirror_session_id', 'i2e_metadata.ingress_tstamp_hi']
  ['i2e_metadata.ingress_tstamp']
  ['tunnel_metadata.ingress_tunnel_type', 'ingress_metadata.ingress_port']
  ['ig_intr_md_for_tm.packet_color', 'l2_metadata.lkp_pkt_type']
Group ['i2e_metadata.ingress_tstamp'] can share with 4 other groups (32 bits):
  ['tunnel_metadata.tunnel_dst_index', 'l3_metadata.nexthop_index']
  ['hash_metadata.entropy_hash']
  ['tunnel_metadata.ingress_tunnel_type', 'ingress_metadata.ingress_port']
  ['ingress_metadata.egress_ifindex', 'egress_metadata.capture_tstamp_on_tx', 'egress_metadata.bypass']
Group ['hash_metadata.entropy_hash'] can share with 4 other groups (16 bits):
  ['i2e_metadata.mirror_session_id', 'i2e_metadata.ingress_tstamp_hi']
  ['i2e_metadata.ingress_tstamp']
  ['tunnel_metadata.ingress_tunnel_type', 'ingress_metadata.ingress_port']
  ['ig_intr_md_for_tm.packet_color', 'l2_metadata.lkp_pkt_type']
Group ['tunnel_metadata.ingress_tunnel_type', 'ingress_metadata.ingress_port'] can share with 4 other groups (14 bits):
  ['tunnel_metadata.tunnel_dst_index', 'l3_metadata.nexthop_index']
  ['i2e_metadata.ingress_tstamp']
  ['hash_metadata.entropy_hash']
  ['ingress_metadata.egress_ifindex', 'egress_metadata.capture_tstamp_on_tx', 'egress_metadata.bypass']
Group ['i2e_metadata.mirror_session_id', 'i2e_metadata.ingress_tstamp_hi'] can share with 2 other groups (32 bits):
  ['tunnel_metadata.tunnel_dst_index', 'l3_metadata.nexthop_index']
  ['hash_metadata.entropy_hash']
Group ['ingress_metadata.egress_ifindex', 'egress_metadata.capture_tstamp_on_tx', 'egress_metadata.bypass'] can share with 2 other groups (16 bits):
  ['i2e_metadata.ingress_tstamp']
  ['tunnel_metadata.ingress_tunnel_type', 'ingress_metadata.ingress_port']
Group ['ig_intr_md_for_tm.packet_color', 'l2_metadata.lkp_pkt_type'] can share with 2 other groups (5 bits):
  ['tunnel_metadata.tunnel_dst_index', 'l3_metadata.nexthop_index']
  ['hash_metadata.entropy_hash']
Group ['l3_metadata.vrf', 'l3_metadata.routed', 'tunnel_metadata.tunnel_terminate'] can share with 0 other groups (16 bits):
Group ['fabric_metadata.reason_code'] can share with 0 other groups (16 bits):
Group ['ingress_metadata.ifindex'] can share with 0 other groups (14 bits):
Group ['ingress_metadata.bd'] can share with 0 other groups (14 bits):

Final group packing:
Group 0:
  ['ingress_metadata.bd']
Group 1:
  ['ingress_metadata.ifindex']
Group 2:
  ['fabric_metadata.reason_code']
Group 3:
  ['l3_metadata.vrf', 'l3_metadata.routed', 'tunnel_metadata.tunnel_terminate']
Group 4:
  ['ig_intr_md_for_tm.packet_color', 'l2_metadata.lkp_pkt_type']
Group 5:
  ['ingress_metadata.egress_ifindex', 'egress_metadata.capture_tstamp_on_tx', 'egress_metadata.bypass']
Group 6:
  ['tunnel_metadata.ingress_tunnel_type', 'ingress_metadata.ingress_port']
Group 7:
  ['hash_metadata.entropy_hash']
Group 8:
  ['i2e_metadata.ingress_tstamp']
Group 9:
  ['tunnel_metadata.tunnel_dst_index', 'l3_metadata.nexthop_index']
Group 10:
  ['i2e_metadata.mirror_session_id', 'i2e_metadata.ingress_tstamp_hi']
Preferred packing is [16, 16, 16, 16, 8, 16, 8, 16, 16, 32, 32, 32]

Final ingress bridged metadata packing: 224 bits (28 bytes)
  -pad-0- / 2 bits
  ingress_metadata.bd / 14 bits
  -pad-1- / 2 bits
  ingress_metadata.ifindex / 14 bits
  fabric_metadata.reason_code / 16 bits
  tunnel_metadata.tunnel_terminate / 1 bits
  l3_metadata.routed / 1 bits
  l3_metadata.vrf / 14 bits
  -pad-2- / 3 bits
  l2_metadata.lkp_pkt_type / 3 bits
  ig_intr_md_for_tm.packet_color / 2 bits
  egress_metadata.bypass / 1 bits
  egress_metadata.capture_tstamp_on_tx / 1 bits
  ingress_metadata.egress_ifindex / 14 bits
  -pad-3- / 3 bits
  tunnel_metadata.ingress_tunnel_type / 5 bits
  -pad-4- / 7 bits
  ingress_metadata.ingress_port / 9 bits
  hash_metadata.entropy_hash / 16 bits
  i2e_metadata.ingress_tstamp / 32 bits
  tunnel_metadata.tunnel_dst_index / 16 bits
  l3_metadata.nexthop_index / 16 bits
  i2e_metadata.ingress_tstamp_hi / 16 bits
  i2e_metadata.mirror_session_id / 16 bits

Final egress bridged metadata packing: 224 bits (28 bytes)
  -pad-0- / 2 bits
  ingress_metadata.bd / 14 bits
  -pad-1- / 2 bits
  ingress_metadata.ifindex / 14 bits
  fabric_metadata.reason_code / 16 bits
  tunnel_metadata.tunnel_terminate / 1 bits
  l3_metadata.routed / 1 bits
  l3_metadata.vrf / 14 bits
  -pad-2- / 3 bits
  l2_metadata.lkp_pkt_type / 3 bits
  ig_intr_md_for_tm.packet_color / 2 bits
  egress_metadata.bypass / 1 bits
  egress_metadata.capture_tstamp_on_tx / 1 bits
  ingress_metadata.egress_ifindex / 14 bits
  -pad-3- / 3 bits
  tunnel_metadata.ingress_tunnel_type / 5 bits
  -pad-5- / 7 bits
  ingress_metadata.ingress_port / 9 bits
  hash_metadata.entropy_hash / 16 bits
  i2e_metadata.ingress_tstamp / 32 bits
  tunnel_metadata.tunnel_dst_index / 16 bits
  l3_metadata.nexthop_index / 16 bits
  i2e_metadata.ingress_tstamp_hi / 16 bits
  i2e_metadata.mirror_session_id / 16 bits

-------------------------------------------
Allocating parsed header: pkt fields (24) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 224
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 224
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 224
Parse state 0 (224 bits)
  -pad-0- [1:0]
  ingress_metadata.bd [13:0]
  -pad-1- [1:0]
  ingress_metadata.ifindex [13:0]
  fabric_metadata.reason_code [15:0]
  tunnel_metadata.tunnel_terminate [0:0]
  l3_metadata.routed [0:0]
  l3_metadata.vrf [13:0]
  -pad-2- [2:0]
  l2_metadata.lkp_pkt_type [2:0]
  ig_intr_md_for_tm.packet_color [1:0]
  egress_metadata.bypass [0:0]
  egress_metadata.capture_tstamp_on_tx [0:0]
  ingress_metadata.egress_ifindex [13:0]
  -pad-3- [2:0]
  tunnel_metadata.ingress_tunnel_type [4:0]
  -pad-4- [6:0]
  ingress_metadata.ingress_port [8:0]
  hash_metadata.entropy_hash [15:0]
  i2e_metadata.ingress_tstamp [31:0]
  tunnel_metadata.tunnel_dst_index [15:0]
  l3_metadata.nexthop_index [15:0]
  i2e_metadata.ingress_tstamp_hi [15:0]
  i2e_metadata.mirror_session_id [15:0]
-----------------------------------------------------------------------------------------------------------
|                 Name                 | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------------
|               -pad-0-                | 2  |    True   |  -  |  -   |     -     |   None   |     1      |
|         ingress_metadata.bd          | 14 |   False   |  -  | [16] |     -     |   None   |     2      |
|               -pad-1-                | 2  |    True   |  -  |  -   |     -     |   None   |     1      |
|       ingress_metadata.ifindex       | 14 |   False   |  -  | [16] |     -     |   None   |     3      |
|     fabric_metadata.reason_code      | 16 |   False   |  -  | [16] |     -     |   None   |     1      |
|   tunnel_metadata.tunnel_terminate   | 1  |   False   |  -  | [8]  |     -     |   None   |     2      |
|          l3_metadata.routed          | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
|           l3_metadata.vrf            | 14 |   False   |  -  |  -   |     -     |   None   |     1      |
|               -pad-2-                | 3  |    True   |  -  |  -   |     -     |   None   |     1      |
|       l2_metadata.lkp_pkt_type       | 3  |   False   |  -  |  -   |     -     |   None   |     1      |
|    ig_intr_md_for_tm.packet_color    | 2  |   False   |  -  |  -   |     -     |    1     |     1      |
|        egress_metadata.bypass        | 1  |   False   |  -  |  -   |     -     |   None   |     2      |
| egress_metadata.capture_tstamp_on_tx | 1  |   False   |  -  |  -   |     -     |   None   |     2      |
|   ingress_metadata.egress_ifindex    | 14 |   False   |  -  |  -   |     -     |   None   |     1      |
|               -pad-3-                | 3  |    True   |  -  |  -   |     -     |   None   |     1      |
| tunnel_metadata.ingress_tunnel_type  | 5  |   False   |  -  |  -   |     -     |    1     |     1      |
|               -pad-4-                | 7  |    True   |  -  |  -   |     -     |   None   |     1      |
|    ingress_metadata.ingress_port     | 9  |   False   |  -  |  -   |     -     |   None   |     2      |
|      hash_metadata.entropy_hash      | 16 |   False   |  -  |  -   |    [8]    |   None   |     5      |
|     i2e_metadata.ingress_tstamp      | 32 |   False   |  -  |  -   |     -     |   None   |     2      |
|   tunnel_metadata.tunnel_dst_index   | 16 |   False   |  -  |  -   |     -     |   None   |     1      |
|      l3_metadata.nexthop_index       | 16 |   False   |  -  |  -   |     -     |   None   |     5      |
|    i2e_metadata.ingress_tstamp_hi    | 16 |   False   |  -  |  -   |     -     |   None   |     1      |
|    i2e_metadata.mirror_session_id    | 16 |   False   |  -  |  -   |     -     |   None   |     1      |
-----------------------------------------------------------------------------------------------------------

Packing options: 191
MAU containers available:
  8-bit: 45
  16-bit: 80
  32-bit: 48
Tagalong containers available:
  8-bit: 32
  16-bit: 48
  32-bit: 32
Initial packing options: 191

Packing option 0:  [16, 16, 16, 16, 8, 16, 8, 16, 16, 32, 32, 32]
MAU containers after:
  8-bit: 43
  16-bit: 63
  32-bit: 40
+-----------------------------------------------+
|  -pad-0- [1:0]                                |
|  ingress_metadata.bd [13:0]                   |
+-----------------------------------------------+
|  -pad-1- [1:0]                                |
|  ingress_metadata.ifindex [13:0]              |
+-----------------------------------------------+
|  fabric_metadata.reason_code [15:0]           |
+-----------------------------------------------+
|  tunnel_metadata.tunnel_terminate [0:0]       |
|  l3_metadata.routed [0:0]                     |
|  l3_metadata.vrf [13:0]                       |
+-----------------------------------------------+
|  -pad-2- [2:0]                                |
|  l2_metadata.lkp_pkt_type [2:0]               |
|  ig_intr_md_for_tm.packet_color [1:0]         |
+-----------------------------------------------+
|  egress_metadata.bypass [0:0]                 |
|  egress_metadata.capture_tstamp_on_tx [0:0]   |
|  ingress_metadata.egress_ifindex [13:0]       |
+-----------------------------------------------+
|  -pad-3- [2:0]                                |
|  tunnel_metadata.ingress_tunnel_type [4:0]    |
+-----------------------------------------------+
|  -pad-4- [6:0]                                |
|  ingress_metadata.ingress_port [8:0]          |
+-----------------------------------------------+
|  hash_metadata.entropy_hash [15:0]            |
+-----------------------------------------------+
|  i2e_metadata.ingress_tstamp [31:0]           |
+-----------------------------------------------+
|  tunnel_metadata.tunnel_dst_index [15:0]      |
|  l3_metadata.nexthop_index [15:0]             |
+-----------------------------------------------+
|  i2e_metadata.ingress_tstamp_hi [15:0]        |
|  i2e_metadata.mirror_session_id [15:0]        |
+-----------------------------------------------+

Looking at -pad-0- (ingress) [1:0], with test_alloc = False
Looking at ingress_metadata.bd (ingress) [13:0], with test_alloc = True
----> ingress_metadata.bd (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
Could not find container to overlay in.

MAU groups: 5
  Group 8 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv128
  Group 10 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv160
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv208
***Allocating phv128[15:14] for -pad-0-[1:0]
***Allocating phv128[13:0] for ingress_metadata.bd[13:0]
Looking at -pad-1- (ingress) [1:0], with test_alloc = False
Looking at ingress_metadata.ifindex (ingress) [13:0], with test_alloc = True
----> ingress_metadata.ifindex (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 8 16 bits -- deparsed True -- avail 15 and promised 4 -- ingress promised 4 and remain 1 and req 11 -- egress promised 0 and remain 8 and req 0 -- act like deparsed True -- container_to_use phv129 -- fails False
Could not find container to overlay in.

MAU groups: 5
  Group 8 16 bits -- avail 15 -- ingress avail 15 and remain 11 and promised 4 and req 1 -- egress avail 8 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv129
  Group 10 16 bits -- avail 16 -- ingress avail 16 and remain 13 and promised 3 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv160
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 13 and promised 3 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 13 and promised 3 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 13 and promised 3 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv208
***Allocating phv129[15:14] for -pad-1-[1:0]
***Allocating phv129[13:0] for ingress_metadata.ifindex[13:0]
Looking at fabric_metadata.reason_code (ingress) [15:0], with test_alloc = True
----> fabric_metadata.reason_code (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 8 16 bits -- deparsed True -- avail 14 and promised 4 -- ingress promised 4 and remain 1 and req 10 -- egress promised 0 and remain 8 and req 0 -- act like deparsed True -- container_to_use phv130 -- fails False
Could not find container to overlay in.

MAU groups: 5
  Group 8 16 bits -- avail 14 -- ingress avail 14 and remain 10 and promised 4 and req 1 -- egress avail 8 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv130
  Group 10 16 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv160
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv208
***Allocating phv130[15:0] for fabric_metadata.reason_code[15:0]
Looking at tunnel_metadata.tunnel_terminate (ingress) [0:0], with test_alloc = True
----> tunnel_metadata.tunnel_terminate (ingress) is allocated? False
Looking at l3_metadata.routed (ingress) [0:0], with test_alloc = True
Looking at l3_metadata.vrf (ingress) [13:0], with test_alloc = True
Checking if can overlay metadata field.
No required PHV group.
  Group 8 16 bits -- deparsed True -- avail 13 and promised 4 -- ingress promised 4 and remain 1 and req 9 -- egress promised 0 and remain 8 and req 0 -- act like deparsed True -- container_to_use phv131 -- fails False
Could not find container to overlay in.

MAU groups: 5
  Group 8 16 bits -- avail 13 -- ingress avail 13 and remain 9 and promised 4 and req 1 -- egress avail 8 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv131
  Group 10 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv160
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv208
***Allocating phv131[15:15] for tunnel_metadata.tunnel_terminate[0:0]
***Allocating phv131[14:14] for l3_metadata.routed[0:0]
***Allocating phv131[13:0] for l3_metadata.vrf[13:0]
Looking at -pad-2- (ingress) [2:0], with test_alloc = False
Looking at l2_metadata.lkp_pkt_type (ingress) [2:0], with test_alloc = True
----> l2_metadata.lkp_pkt_type (ingress) is allocated? False
Looking at ig_intr_md_for_tm.packet_color (ingress) [1:0], with test_alloc = True
Checking if can overlay metadata field.
No required PHV group.
  Group 4 8 bits -- deparsed True -- avail 13 and promised 1 -- ingress promised 1 and remain 1 and req 12 -- egress promised 0 and remain 8 and req 0 -- act like deparsed True -- container_to_use phv67 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 4 8 bits -- avail 13 -- ingress avail 13 and remain 12 and promised 1 and req 1 -- egress avail 8 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv67
  Group 6 8 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv96
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv112
***Allocating phv67[7:5] for -pad-2-[2:0]
***Allocating phv67[4:2] for l2_metadata.lkp_pkt_type[2:0]
***Allocating phv67[1:0] for ig_intr_md_for_tm.packet_color[1:0]
Looking at egress_metadata.bypass (ingress) [0:0], with test_alloc = True
----> egress_metadata.bypass (ingress) is allocated? False
Looking at egress_metadata.capture_tstamp_on_tx (ingress) [0:0], with test_alloc = True
Looking at ingress_metadata.egress_ifindex (ingress) [13:0], with test_alloc = True
Checking if can overlay metadata field.
No required PHV group.
  Group 8 16 bits -- deparsed True -- avail 12 and promised 5 -- ingress promised 5 and remain 2 and req 7 -- egress promised 0 and remain 7 and req 0 -- act like deparsed True -- container_to_use phv132 -- fails False
Could not find container to overlay in.

MAU groups: 5
  Group 8 16 bits -- avail 12 -- ingress avail 12 and remain 7 and promised 5 and req 2 -- egress avail 8 and remain 7 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv132
  Group 10 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 2 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv160
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 2 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 2 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 2 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv208
***Allocating phv132[15:15] for egress_metadata.bypass[0:0]
***Allocating phv132[14:14] for egress_metadata.capture_tstamp_on_tx[0:0]
***Allocating phv132[13:0] for ingress_metadata.egress_ifindex[13:0]
Looking at -pad-3- (ingress) [2:0], with test_alloc = False
Looking at tunnel_metadata.ingress_tunnel_type (ingress) [4:0], with test_alloc = True
----> tunnel_metadata.ingress_tunnel_type (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 4 8 bits -- deparsed True -- avail 12 and promised 1 -- ingress promised 1 and remain 1 and req 11 -- egress promised 0 and remain 8 and req 0 -- act like deparsed True -- container_to_use phv68 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 4 8 bits -- avail 12 -- ingress avail 12 and remain 11 and promised 1 and req 1 -- egress avail 8 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv68
  Group 6 8 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv96
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv112
***Allocating phv68[7:5] for -pad-3-[2:0]
***Allocating phv68[4:0] for tunnel_metadata.ingress_tunnel_type[4:0]
Looking at -pad-4- (ingress) [6:0], with test_alloc = False
Looking at ingress_metadata.ingress_port (ingress) [8:0], with test_alloc = True
----> ingress_metadata.ingress_port (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 8 16 bits -- deparsed True -- avail 11 and promised 6 -- ingress promised 6 and remain 2 and req 5 -- egress promised 0 and remain 5 and req 0 -- act like deparsed True -- container_to_use phv134 -- fails False
Could not find container to overlay in.

MAU groups: 5
  Group 8 16 bits -- avail 11 -- ingress avail 11 and remain 5 and promised 6 and req 2 -- egress avail 8 and remain 5 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv134
  Group 10 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv160
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv208
***Allocating phv134[15:9] for -pad-4-[6:0]
***Allocating phv134[8:0] for ingress_metadata.ingress_port[8:0]
Looking at hash_metadata.entropy_hash (ingress) [15:0], with test_alloc = True
----> hash_metadata.entropy_hash (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 8 16 bits -- deparsed True -- avail 10 and promised 10 -- ingress promised 10 and remain 4 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed True -- container_to_use phv135 -- fails False
Could not find container to overlay in.

MAU groups: 5
  Group 8 16 bits -- avail 10 -- ingress avail 10 and remain 0 and promised 10 and req 4 -- egress avail 8 and remain 0 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv135
  Group 10 16 bits -- avail 16 -- ingress avail 16 and remain 11 and promised 5 and req 3 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv160
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 11 and promised 5 and req 3 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 11 and promised 5 and req 3 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 11 and promised 5 and req 3 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv208
***Allocating phv135[15:0] for hash_metadata.entropy_hash[15:0]
Looking at i2e_metadata.ingress_tstamp (ingress) [31:0], with test_alloc = True
----> i2e_metadata.ingress_tstamp (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
Could not find container to overlay in.

MAU groups: 3
  Group 0 32 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 12 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv0
  Group 2 32 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 12 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv32
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 12 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv48
***Allocating phv0[31:0] for i2e_metadata.ingress_tstamp[31:0]
Looking at tunnel_metadata.tunnel_dst_index (ingress) [15:0], with test_alloc = True
----> tunnel_metadata.tunnel_dst_index (ingress) is allocated? False
Looking at l3_metadata.nexthop_index (ingress) [15:0], with test_alloc = True
Checking if can overlay metadata field.
No required PHV group.
  Group 0 32 bits -- deparsed True -- avail 15 and promised 4 -- ingress promised 4 and remain 1 and req 11 -- egress promised 0 and remain 11 and req 0 -- act like deparsed True -- container_to_use phv1 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 0 32 bits -- avail 15 -- ingress avail 15 and remain 11 and promised 4 and req 1 -- egress avail 12 and remain 11 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv1
  Group 2 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 4 and req 1 -- egress avail 16 and remain 12 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv32
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 4 and req 1 -- egress avail 16 and remain 12 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv48
***Allocating phv1[31:16] for tunnel_metadata.tunnel_dst_index[15:0]
***Allocating phv1[15:0] for l3_metadata.nexthop_index[15:0]
Looking at i2e_metadata.ingress_tstamp_hi (ingress) [15:0], with test_alloc = True
----> i2e_metadata.ingress_tstamp_hi (ingress) is allocated? False
Looking at i2e_metadata.mirror_session_id (ingress) [15:0], with test_alloc = True
Checking if can overlay metadata field.
No required PHV group.
  Group 0 32 bits -- deparsed True -- avail 14 and promised 4 -- ingress promised 4 and remain 1 and req 10 -- egress promised 0 and remain 10 and req 0 -- act like deparsed True -- container_to_use phv2 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 0 32 bits -- avail 14 -- ingress avail 14 and remain 10 and promised 4 and req 1 -- egress avail 12 and remain 10 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv2
  Group 2 32 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 1 -- egress avail 16 and remain 12 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv32
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 1 -- egress avail 16 and remain 12 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv48
***Allocating phv2[31:16] for i2e_metadata.ingress_tstamp_hi[15:0]
***Allocating phv2[15:0] for i2e_metadata.mirror_session_id[15:0]
Packing options tried: 1
Packing options skipped: 0


-------------------------------------------
Allocating parsed header: pkt fields (24) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 224
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 224
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 224
Parse state 0 (224 bits)
  -pad-0- [1:0]
  ingress_metadata.bd [13:0]
  -pad-1- [1:0]
  ingress_metadata.ifindex [13:0]
  fabric_metadata.reason_code [15:0]
  tunnel_metadata.tunnel_terminate [0:0]
  l3_metadata.routed [0:0]
  l3_metadata.vrf [13:0]
  -pad-2- [2:0]
  l2_metadata.lkp_pkt_type [2:0]
  ig_intr_md_for_tm.packet_color [1:0]
  egress_metadata.bypass [0:0]
  egress_metadata.capture_tstamp_on_tx [0:0]
  ingress_metadata.egress_ifindex [13:0]
  -pad-3- [2:0]
  tunnel_metadata.ingress_tunnel_type [4:0]
  -pad-5- [6:0]
  ingress_metadata.ingress_port [8:0]
  hash_metadata.entropy_hash [15:0]
  i2e_metadata.ingress_tstamp [31:0]
  tunnel_metadata.tunnel_dst_index [15:0]
  l3_metadata.nexthop_index [15:0]
  i2e_metadata.ingress_tstamp_hi [15:0]
  i2e_metadata.mirror_session_id [15:0]
-----------------------------------------------------------------------------------------------------------
|                 Name                 | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------------
|               -pad-0-                | 2  |    True   |  -  |  -   |     -     |   None   |     1      |
|         ingress_metadata.bd          | 14 |   False   |  -  | [16] |     -     |   None   |     7      |
|               -pad-1-                | 2  |    True   |  -  |  -   |     -     |   None   |     1      |
|       ingress_metadata.ifindex       | 14 |   False   |  -  | [16] |     -     |   None   |     2      |
|     fabric_metadata.reason_code      | 16 |   False   |  -  |  -   |     -     |   None   |     2      |
|   tunnel_metadata.tunnel_terminate   | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
|          l3_metadata.routed          | 1  |   False   |  -  |  -   |     -     |   None   |     3      |
|           l3_metadata.vrf            | 14 |   False   |  -  |  -   |     -     |   None   |     7      |
|               -pad-2-                | 3  |    True   |  -  |  -   |     -     |   None   |     1      |
|       l2_metadata.lkp_pkt_type       | 3  |   False   |  -  |  -   |     -     |   None   |     1      |
|    ig_intr_md_for_tm.packet_color    | 2  |   False   |  -  |  -   |     -     |   None   |     1      |
|        egress_metadata.bypass        | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
| egress_metadata.capture_tstamp_on_tx | 1  |   False   |  -  |  -   |     -     |   None   |     2      |
|   ingress_metadata.egress_ifindex    | 14 |   False   |  -  |  -   |     -     |   None   |     1      |
|               -pad-3-                | 3  |    True   |  -  |  -   |     -     |   None   |     1      |
| tunnel_metadata.ingress_tunnel_type  | 5  |   False   |  -  |  -   |     -     |   None   |     1      |
|               -pad-5-                | 7  |    True   |  -  |  -   |     -     |   None   |     1      |
|    ingress_metadata.ingress_port     | 9  |   False   |  -  | [16] |     -     |   None   |     2      |
|      hash_metadata.entropy_hash      | 16 |   False   |  -  |  -   |     -     |   None   |     4      |
|     i2e_metadata.ingress_tstamp      | 32 |   False   |  -  |  -   |     -     |   None   |     3      |
|   tunnel_metadata.tunnel_dst_index   | 16 |   False   |  -  |  -   |     -     |   None   |     1      |
|      l3_metadata.nexthop_index       | 16 |   False   |  -  |  -   |     -     |   None   |     1      |
|    i2e_metadata.ingress_tstamp_hi    | 16 |   False   |  -  |  -   |     -     |   None   |     2      |
|    i2e_metadata.mirror_session_id    | 16 |   False   |  -  |  -   |     -     |   None   |     2      |
-----------------------------------------------------------------------------------------------------------

Packing options: 191
MAU containers available:
  8-bit: 47
  16-bit: 80
  32-bit: 48
Tagalong containers available:
  8-bit: 32
  16-bit: 48
  32-bit: 32
Initial packing options: 191

Packing option 62:  [16, 16, 16, 16, 8, 16, 8, 16, 8, 8, 32, 32, 16, 16]
MAU containers after:
  8-bit: 37
  16-bit: 54
  32-bit: 44
+-----------------------------------------------+
|  -pad-0- [1:0]                                |
|  ingress_metadata.bd [13:0]                   |
+-----------------------------------------------+
|  -pad-1- [1:0]                                |
|  ingress_metadata.ifindex [13:0]              |
+-----------------------------------------------+
|  fabric_metadata.reason_code [15:0]           |
+-----------------------------------------------+
|  tunnel_metadata.tunnel_terminate [0:0]       |
|  l3_metadata.routed [0:0]                     |
|  l3_metadata.vrf [13:0]                       |
+-----------------------------------------------+
|  -pad-2- [2:0]                                |
|  l2_metadata.lkp_pkt_type [2:0]               |
|  ig_intr_md_for_tm.packet_color [1:0]         |
+-----------------------------------------------+
|  egress_metadata.bypass [0:0]                 |
|  egress_metadata.capture_tstamp_on_tx [0:0]   |
|  ingress_metadata.egress_ifindex [13:0]       |
+-----------------------------------------------+
|  -pad-3- [2:0]                                |
|  tunnel_metadata.ingress_tunnel_type [4:0]    |
+-----------------------------------------------+
|  -pad-5- [6:0]                                |
|  ingress_metadata.ingress_port [8:0]          |
+-----------------------------------------------+
|  hash_metadata.entropy_hash [15:8]            |
+-----------------------------------------------+
|  hash_metadata.entropy_hash [7:0]             |
+-----------------------------------------------+
|  i2e_metadata.ingress_tstamp [31:0]           |
+-----------------------------------------------+
|  tunnel_metadata.tunnel_dst_index [15:0]      |
|  l3_metadata.nexthop_index [15:0]             |
+-----------------------------------------------+
|  i2e_metadata.ingress_tstamp_hi [15:0]        |
+-----------------------------------------------+
|  i2e_metadata.mirror_session_id [15:0]        |
+-----------------------------------------------+

Looking at -pad-0- (egress) [1:0], with test_alloc = False
Looking at ingress_metadata.bd (egress) [13:0], with test_alloc = True
----> ingress_metadata.bd (egress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
Could not find container to overlay in.

MAU groups: 5
  Group 9 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 9 and promised 7 and req 2 -- as if deparsed True -- container_to_use phv144
  Group 10 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 9 and promised 7 and req 2 -- as if deparsed True -- container_to_use phv160
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 9 and promised 7 and req 2 -- as if deparsed True -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 9 and promised 7 and req 2 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 9 and promised 7 and req 2 -- as if deparsed True -- container_to_use phv208
***Allocating phv144[15:14] for -pad-0-[1:0]
***Allocating phv144[13:0] for ingress_metadata.bd[13:0]
Looking at -pad-1- (egress) [1:0], with test_alloc = False
Looking at ingress_metadata.ifindex (egress) [13:0], with test_alloc = True
----> ingress_metadata.ifindex (egress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 9 16 bits -- deparsed True -- avail 15 and promised 8 -- ingress promised 0 and remain 0 and req 7 -- egress promised 8 and remain 7 and req 3 -- act like deparsed True -- container_to_use phv146 -- fails False
Could not find container to overlay in.

MAU groups: 5
  Group 9 16 bits -- avail 15 -- ingress avail 8 and remain 7 and promised 0 and req 0 -- egress avail 15 and remain 7 and promised 8 and req 3 -- as if deparsed True -- container_to_use phv146
  Group 10 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv160
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208
***Allocating phv146[15:14] for -pad-1-[1:0]
***Allocating phv146[13:0] for ingress_metadata.ifindex[13:0]
Looking at fabric_metadata.reason_code (egress) [15:0], with test_alloc = True
----> fabric_metadata.reason_code (egress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 9 16 bits -- deparsed True -- avail 14 and promised 9 -- ingress promised 0 and remain 0 and req 5 -- egress promised 9 and remain 5 and req 4 -- act like deparsed True -- container_to_use phv148 -- fails False
Could not find container to overlay in.

MAU groups: 5
  Group 9 16 bits -- avail 14 -- ingress avail 8 and remain 5 and promised 0 and req 0 -- egress avail 14 and remain 5 and promised 9 and req 4 -- as if deparsed True -- container_to_use phv148
  Group 10 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv160
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208
***Allocating phv148[15:0] for fabric_metadata.reason_code[15:0]
Looking at tunnel_metadata.tunnel_terminate (egress) [0:0], with test_alloc = True
----> tunnel_metadata.tunnel_terminate (egress) is allocated? False
Looking at l3_metadata.routed (egress) [0:0], with test_alloc = True
Looking at l3_metadata.vrf (egress) [13:0], with test_alloc = True
Checking if can overlay metadata field.
Required PHV group: Group 9 16 bits
  Group 9 16 bits -- deparsed False -- avail 13 and promised 8 -- ingress promised 0 and remain 0 and req 5 -- egress promised 8 and remain 5 and req 3 -- act like deparsed False -- container_to_use phv151 -- fails False
Could not find container to overlay in.
  Group 9 16 bits -- deparsed False -- promised 8 -- ingress promised 0 and remain 0 and req 5 -- egress promised 8 and remain 5 and req 3 -- act like deparsed False -- container_to_use phv151 -- fails False
  treat as deparsed? False
Required PHV group: Group 9 16 bits
Found new container in required group phv151
***Allocating phv151[15:15] for tunnel_metadata.tunnel_terminate[0:0]
***Allocating phv151[14:14] for l3_metadata.routed[0:0]
***Allocating phv151[13:0] for l3_metadata.vrf[13:0]
Looking at -pad-2- (egress) [2:0], with test_alloc = False
Looking at l2_metadata.lkp_pkt_type (egress) [2:0], with test_alloc = True
----> l2_metadata.lkp_pkt_type (egress) is allocated? False
Looking at ig_intr_md_for_tm.packet_color (egress) [1:0], with test_alloc = True
Checking if can overlay metadata field.
No required PHV group.
  Group 4 8 bits -- deparsed False -- avail 11 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- deparsed False -- avail 15 and promised 1 -- ingress promised 0 and remain 0 and req 8 -- egress promised 1 and remain 14 and req 0 -- act like deparsed False -- container_to_use phv81 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 5 8 bits -- avail 15 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 15 and remain 14 and promised 1 and req 0 -- as if deparsed False -- container_to_use phv81
  Group 6 8 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 0 -- as if deparsed False -- container_to_use phv96
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 0 -- as if deparsed False -- container_to_use phv112
***Allocating phv81[7:5] for -pad-2-[2:0]
***Allocating phv81[4:2] for l2_metadata.lkp_pkt_type[2:0]
***Allocating phv81[1:0] for ig_intr_md_for_tm.packet_color[1:0]
Looking at egress_metadata.bypass (egress) [0:0], with test_alloc = True
----> egress_metadata.bypass (egress) is allocated? False
Looking at egress_metadata.capture_tstamp_on_tx (egress) [0:0], with test_alloc = True
Looking at ingress_metadata.egress_ifindex (egress) [13:0], with test_alloc = True
Checking if can overlay metadata field.
No required PHV group.
  Group 8 16 bits -- deparsed False -- avail 9 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- deparsed False -- avail 12 and promised 8 -- ingress promised 0 and remain 0 and req 4 -- egress promised 8 and remain 4 and req 3 -- act like deparsed False -- container_to_use phv155 -- fails False
Could not find container to overlay in.

MAU groups: 5
  Group 9 16 bits -- avail 12 -- ingress avail 8 and remain 4 and promised 0 and req 0 -- egress avail 12 and remain 4 and promised 8 and req 3 -- as if deparsed False -- container_to_use phv155
  Group 10 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 1 -- as if deparsed False -- container_to_use phv161
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 1 -- as if deparsed False -- container_to_use phv177
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 1 -- as if deparsed False -- container_to_use phv193
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 1 -- as if deparsed False -- container_to_use phv209
***Allocating phv155[15:15] for egress_metadata.bypass[0:0]
***Allocating phv155[14:14] for egress_metadata.capture_tstamp_on_tx[0:0]
***Allocating phv155[13:0] for ingress_metadata.egress_ifindex[13:0]
Looking at -pad-3- (egress) [2:0], with test_alloc = False
Looking at tunnel_metadata.ingress_tunnel_type (egress) [4:0], with test_alloc = True
----> tunnel_metadata.ingress_tunnel_type (egress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 4 8 bits -- deparsed False -- avail 11 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- deparsed False -- avail 14 and promised 1 -- ingress promised 0 and remain 0 and req 8 -- egress promised 1 and remain 13 and req 0 -- act like deparsed False -- container_to_use phv82 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 5 8 bits -- avail 14 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 14 and remain 13 and promised 1 and req 0 -- as if deparsed False -- container_to_use phv82
  Group 6 8 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 0 -- as if deparsed False -- container_to_use phv96
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 0 -- as if deparsed False -- container_to_use phv112
***Allocating phv82[7:5] for -pad-3-[2:0]
***Allocating phv82[4:0] for tunnel_metadata.ingress_tunnel_type[4:0]
Looking at -pad-5- (egress) [6:0], with test_alloc = False
Looking at ingress_metadata.ingress_port (egress) [8:0], with test_alloc = True
----> ingress_metadata.ingress_port (egress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 9 16 bits -- deparsed True -- avail 11 and promised 9 -- ingress promised 0 and remain 0 and req 0 -- egress promised 9 and remain 2 and req 5 -- act like deparsed True -- container_to_use phv150 -- fails False
Could not find container to overlay in.

MAU groups: 5
  Group 9 16 bits -- avail 11 -- ingress avail 7 and remain 0 and promised 0 and req 0 -- egress avail 11 and remain 2 and promised 9 and req 5 -- as if deparsed True -- container_to_use phv150
  Group 10 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv160
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208
***Allocating phv150[15:9] for -pad-5-[6:0]
***Allocating phv150[8:0] for ingress_metadata.ingress_port[8:0]
Looking at hash_metadata.entropy_hash (egress) [15:8], with test_alloc = True
----> hash_metadata.entropy_hash (egress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 4 8 bits -- deparsed False -- avail 11 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- deparsed False -- avail 13 and promised 3 -- ingress promised 0 and remain 0 and req 8 -- egress promised 3 and remain 10 and req 2 -- act like deparsed False -- container_to_use phv85 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 5 8 bits -- avail 13 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 13 and remain 10 and promised 3 and req 2 -- as if deparsed False -- container_to_use phv85
  Group 6 8 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 13 and promised 3 and req 2 -- as if deparsed False -- container_to_use phv98
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 13 and promised 3 and req 2 -- as if deparsed False -- container_to_use phv114
***Allocating phv85[7:0] for hash_metadata.entropy_hash[15:8]
Looking at hash_metadata.entropy_hash (egress) [7:0], with test_alloc = True
----> hash_metadata.entropy_hash (egress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 4 8 bits -- deparsed False -- avail 11 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- deparsed False -- avail 12 and promised 5 -- ingress promised 0 and remain 0 and req 7 -- egress promised 5 and remain 7 and req 4 -- act like deparsed False -- container_to_use phv88 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 5 8 bits -- avail 12 -- ingress avail 8 and remain 7 and promised 0 and req 0 -- egress avail 12 and remain 7 and promised 5 and req 4 -- as if deparsed False -- container_to_use phv88
  Group 6 8 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 13 and promised 3 and req 2 -- as if deparsed False -- container_to_use phv98
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 13 and promised 3 and req 2 -- as if deparsed False -- container_to_use phv114
***Allocating phv88[7:0] for hash_metadata.entropy_hash[7:0]
Looking at i2e_metadata.ingress_tstamp (egress) [31:0], with test_alloc = True
----> i2e_metadata.ingress_tstamp (egress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
Could not find container to overlay in.

MAU groups: 3
  Group 1 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 13 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv16
  Group 2 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 13 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv32
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 13 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv48
***Allocating phv16[31:0] for i2e_metadata.ingress_tstamp[31:0]
Looking at tunnel_metadata.tunnel_dst_index (egress) [15:0], with test_alloc = True
----> tunnel_metadata.tunnel_dst_index (egress) is allocated? False
Looking at l3_metadata.nexthop_index (egress) [15:0], with test_alloc = True
Checking if can overlay metadata field.
No required PHV group.
  Group 0 32 bits -- deparsed False -- avail 13 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- deparsed False -- avail 15 and promised 3 -- ingress promised 0 and remain 0 and req 12 -- egress promised 3 and remain 12 and req 2 -- act like deparsed False -- container_to_use phv19 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 1 32 bits -- avail 15 -- ingress avail 12 and remain 12 and promised 0 and req 0 -- egress avail 15 and remain 12 and promised 3 and req 2 -- as if deparsed False -- container_to_use phv19
  Group 2 32 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 0 -- as if deparsed False -- container_to_use phv32
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 0 -- as if deparsed False -- container_to_use phv48
***Allocating phv19[31:16] for tunnel_metadata.tunnel_dst_index[15:0]
***Allocating phv19[15:0] for l3_metadata.nexthop_index[15:0]
Looking at i2e_metadata.ingress_tstamp_hi (egress) [15:0], with test_alloc = True
----> i2e_metadata.ingress_tstamp_hi (egress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 8 16 bits -- deparsed False -- avail 9 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- deparsed False -- avail 10 and promised 10 -- ingress promised 0 and remain 0 and req 0 -- egress promised 10 and remain 0 and req 5 -- act like deparsed False -- container_to_use phv159 -- fails False
Could not find container to overlay in.

MAU groups: 5
  Group 9 16 bits -- avail 10 -- ingress avail 7 and remain 0 and promised 0 and req 0 -- egress avail 10 and remain 0 and promised 10 and req 5 -- as if deparsed False -- container_to_use phv159
  Group 10 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 1 -- as if deparsed False -- container_to_use phv161
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 1 -- as if deparsed False -- container_to_use phv177
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 1 -- as if deparsed False -- container_to_use phv193
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 1 -- as if deparsed False -- container_to_use phv209
***Allocating phv159[15:0] for i2e_metadata.ingress_tstamp_hi[15:0]
Looking at i2e_metadata.mirror_session_id (egress) [15:0], with test_alloc = True
----> i2e_metadata.mirror_session_id (egress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
--> Since packet field is not parsed and does not have overlapping liveness, can overlay erspan_t3_header.priority_span_id <16 bits egress parsed W> and -pad-0- <2 bits egress meta tagalong>.
--> Since packet field is not parsed and does not have overlapping liveness, can overlay erspan_t3_header.priority_span_id <16 bits egress parsed W> and -pad-1- <2 bits egress meta tagalong>.
--> Since packet field is not parsed and does not have overlapping liveness, can overlay erspan_t3_header.priority_span_id <16 bits egress parsed W> and -pad-5- <7 bits egress meta tagalong>.
  Group 9 16 bits -- deparsed True -- avail 9 and promised 10 -- ingress promised 0 and remain 0 and req 0 -- egress promised 11 and remain 0 and req 7 -- act like deparsed True -- container_to_use phv154 -- fails True
Could not find container to overlay in.

MAU groups: 4
  Group 10 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv160
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208
***Allocating phv160[15:0] for i2e_metadata.mirror_session_id[15:0]
Packing options tried: 63
Packing options skipped: 0
Failure Reasons:
  Packing violates parse state constraints (case 8) -- tried 6 variants
    culprit: parse_state parse_nvgre, reason: 
  Field violates some size constraints (case 800) -- tried 20 variants
  Field violates source operand constraints (case 701) -- tried 36 variants


After allocating bridged metadata:
Allocation state: Final Allocation
-----------------------------------------------------------------------------
|       PHV Group        | Containers Used |  Bits Used   | Bits Available |
| (container bit widths) |     (% used)    |   (% used)   |                |
-----------------------------------------------------------------------------
|         0 (32)         |    3 (18.75%)   | 96 (18.75%)  |      512       |
|         1 (32)         |    2 (12.50%)   | 64 (12.50%)  |      512       |
|         2 (32)         |    0 (0.00%)    |  0 (0.00%)   |      512       |
|         3 (32)         |    0 (0.00%)    |  0 (0.00%)   |      512       |
|    Total for 32 bit    |    5 (7.81%)    | 160 (7.81%)  |      2048      |
|                        |                 |              |                |
|         4 (8)          |    5 (31.25%)   | 40 (31.25%)  |      128       |
|         5 (8)          |    5 (31.25%)   | 40 (31.25%)  |      128       |
|         6 (8)          |    0 (0.00%)    |  0 (0.00%)   |      128       |
|         7 (8)          |    0 (0.00%)    |  0 (0.00%)   |      128       |
|    Total for 8 bit     |   10 (15.62%)   | 80 (15.62%)  |      512       |
|                        |                 |              |                |
|         8 (16)         |    7 (43.75%)   | 112 (43.75%) |      256       |
|         9 (16)         |    7 (43.75%)   | 112 (43.75%) |      256       |
|        10 (16)         |    1 (6.25%)    |  16 (6.25%)  |      256       |
|        11 (16)         |    0 (0.00%)    |  0 (0.00%)   |      256       |
|        12 (16)         |    0 (0.00%)    |  0 (0.00%)   |      256       |
|        13 (16)         |    0 (0.00%)    |  0 (0.00%)   |      256       |
|    Total for 16 bit    |   15 (15.62%)   | 240 (15.62%) |      1536      |
|                        |                 |              |                |
|       14 (32) T        |    0 (0.00%)    |  0 (0.00%)   |      512       |
|       15 (32) T        |    0 (0.00%)    |  0 (0.00%)   |      512       |
|    Total for 32 bit    |    0 (0.00%)    |  0 (0.00%)   |      1024      |
|                        |                 |              |                |
|        16 (8) T        |    0 (0.00%)    |  0 (0.00%)   |      128       |
|        17 (8) T        |    0 (0.00%)    |  0 (0.00%)   |      128       |
|    Total for 8 bit     |    0 (0.00%)    |  0 (0.00%)   |      256       |
|                        |                 |              |                |
|       18 (16) T        |    0 (0.00%)    |  0 (0.00%)   |      256       |
|       19 (16) T        |    0 (0.00%)    |  0 (0.00%)   |      256       |
|       20 (16) T        |    0 (0.00%)    |  0 (0.00%)   |      256       |
|    Total for 16 bit    |    0 (0.00%)    |  0 (0.00%)   |      768       |
|                        |                 |              |                |
|       MAU total        |   30 (13.39%)   | 480 (11.72%) |      4096      |
|     Tagalong total     |    0 (0.00%)    |  0 (0.00%)   |      2048      |
|     Overall total      |    30 (8.93%)   | 480 (7.81%)  |      6144      |
-----------------------------------------------------------------------------

>>Event 'pa_phase0' at time 1573971830.18
   Took 8.45 seconds

-----------------------------------------------
  Allocating Phase 0-related metadata
-----------------------------------------------
Allocation Step
  Phase 0 fields total 12 bits.
    ingress_metadata.port_lag_index <10 bits ingress meta R W>
    ingress_metadata.port_type <2 bits ingress meta R W>

Required packing for Phase 0 metadata: 0
Allowed alignment for fields:
  ingress_metadata.port_lag_index -> any container bit
  ingress_metadata.port_type -> any container bit
ingress_metadata.port_lag_index can share with fields: (1) : total bits 12
  ingress_metadata.port_type / 2 bits
ingress_metadata.port_type can share with fields: (1) : total bits 12
  ingress_metadata.port_lag_index / 10 bits


All combinations = 4
Valid combinations = 4
Choosing to pack non-byte multiple metadata as below, which wastes 8 bits
  Group 0 (12 bits)
    ingress_metadata.port_type
    ingress_metadata.port_lag_index

Sharing capabilities of groups:
Group ['ingress_metadata.port_type', 'ingress_metadata.port_lag_index'] can share with 0 other groups:

Final group packing:
Group 0:
  ['ingress_metadata.port_type', 'ingress_metadata.port_lag_index']
Preferred packing is [16]

Final Phase 0 packing: 16 bits (2 bytes)
  -pad-6- / 4 bits
  ingress_metadata.port_type / 2 bits
  ingress_metadata.port_lag_index / 10 bits
Phase 0 Grouping: 1
 Group 0
   -pad-6-[3:0]
   ingress_metadata.port_type[1:0]
   ingress_metadata.port_lag_index[9:0]

-------------------------------------------
Allocating parsed header: pkt fields (3) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 16
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (16 bits)
  -pad-6- [3:0]
  ingress_metadata.port_type [1:0]
  ingress_metadata.port_lag_index [9:0]
------------------------------------------------------------------------------------------------------
|               Name              | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------------
|             -pad-6-             | 4  |    True   |  -  |  -   |     -     |   None   |     1      |
|    ingress_metadata.port_type   | 2  |   False   |  -  |  -   |     -     |   None   |     1      |
| ingress_metadata.port_lag_index | 10 |   False   |  -  |  -   |     -     |   None   |     1      |
------------------------------------------------------------------------------------------------------

Packing options: 5
MAU containers available:
  8-bit: 43
  16-bit: 56
  32-bit: 42
Tagalong containers available:
  8-bit: 32
  16-bit: 48
  32-bit: 32
Initial packing options: 5

Packing option 0:  [16]
MAU containers after:
  8-bit: 43
  16-bit: 55
  32-bit: 42
+------------------------------------------+
|  -pad-6- [3:0]                           |
|  ingress_metadata.port_type [1:0]        |
|  ingress_metadata.port_lag_index [9:0]   |
+------------------------------------------+

Looking at -pad-6- (ingress) [3:0], with test_alloc = False
Looking at ingress_metadata.port_type (ingress) [1:0], with test_alloc = True
----> ingress_metadata.port_type (ingress) is allocated? False
Looking at ingress_metadata.port_lag_index (ingress) [9:0], with test_alloc = True
Checking if can overlay metadata field.
No required PHV group.
  Group 8 16 bits -- deparsed False -- avail 9 and promised 10 -- ingress promised 10 and remain 3 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- deparsed False -- avail 9 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- deparsed False -- avail 15 and promised 2 -- ingress promised 1 and remain 0 and req 7 -- egress promised 1 and remain 13 and req 1 -- act like deparsed False -- container_to_use phv168 -- fails False
Could not find container to overlay in.

MAU groups: 4
  Group 10 16 bits -- avail 15 -- ingress avail 8 and remain 7 and promised 1 and req 0 -- egress avail 15 and remain 13 and promised 1 and req 1 -- as if deparsed False -- container_to_use phv168
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 0 -- egress avail 16 and remain 15 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 0 -- egress avail 16 and remain 15 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 0 -- egress avail 16 and remain 15 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv208
***Allocating phv168[15:12] for -pad-6-[3:0]
***Allocating phv168[11:10] for ingress_metadata.port_type[1:0]
***Allocating phv168[9:0] for ingress_metadata.port_lag_index[9:0]
Packing options tried: 1
Packing options skipped: 0


After allocating data written by Phase 0:
Allocation state: Final Allocation
-----------------------------------------------------------------------------
|       PHV Group        | Containers Used |  Bits Used   | Bits Available |
| (container bit widths) |     (% used)    |   (% used)   |                |
-----------------------------------------------------------------------------
|         0 (32)         |    3 (18.75%)   | 96 (18.75%)  |      512       |
|         1 (32)         |    2 (12.50%)   | 64 (12.50%)  |      512       |
|         2 (32)         |    0 (0.00%)    |  0 (0.00%)   |      512       |
|         3 (32)         |    0 (0.00%)    |  0 (0.00%)   |      512       |
|    Total for 32 bit    |    5 (7.81%)    | 160 (7.81%)  |      2048      |
|                        |                 |              |                |
|         4 (8)          |    5 (31.25%)   | 40 (31.25%)  |      128       |
|         5 (8)          |    5 (31.25%)   | 40 (31.25%)  |      128       |
|         6 (8)          |    0 (0.00%)    |  0 (0.00%)   |      128       |
|         7 (8)          |    0 (0.00%)    |  0 (0.00%)   |      128       |
|    Total for 8 bit     |   10 (15.62%)   | 80 (15.62%)  |      512       |
|                        |                 |              |                |
|         8 (16)         |    7 (43.75%)   | 112 (43.75%) |      256       |
|         9 (16)         |    7 (43.75%)   | 112 (43.75%) |      256       |
|        10 (16)         |    2 (12.50%)   | 32 (12.50%)  |      256       |
|        11 (16)         |    0 (0.00%)    |  0 (0.00%)   |      256       |
|        12 (16)         |    0 (0.00%)    |  0 (0.00%)   |      256       |
|        13 (16)         |    0 (0.00%)    |  0 (0.00%)   |      256       |
|    Total for 16 bit    |   16 (16.67%)   | 256 (16.67%) |      1536      |
|                        |                 |              |                |
|       14 (32) T        |    0 (0.00%)    |  0 (0.00%)   |      512       |
|       15 (32) T        |    0 (0.00%)    |  0 (0.00%)   |      512       |
|    Total for 32 bit    |    0 (0.00%)    |  0 (0.00%)   |      1024      |
|                        |                 |              |                |
|        16 (8) T        |    0 (0.00%)    |  0 (0.00%)   |      128       |
|        17 (8) T        |    0 (0.00%)    |  0 (0.00%)   |      128       |
|    Total for 8 bit     |    0 (0.00%)    |  0 (0.00%)   |      256       |
|                        |                 |              |                |
|       18 (16) T        |    0 (0.00%)    |  0 (0.00%)   |      256       |
|       19 (16) T        |    0 (0.00%)    |  0 (0.00%)   |      256       |
|       20 (16) T        |    0 (0.00%)    |  0 (0.00%)   |      256       |
|    Total for 16 bit    |    0 (0.00%)    |  0 (0.00%)   |      768       |
|                        |                 |              |                |
|       MAU total        |   31 (13.84%)   | 496 (12.11%) |      4096      |
|     Tagalong total     |    0 (0.00%)    |  0 (0.00%)   |      2048      |
|     Overall total      |    31 (9.23%)   | 496 (8.07%)  |      6144      |
-----------------------------------------------------------------------------

>>Event 'pa_critical' at time 1573971830.45
   Took 0.28 seconds

-----------------------------------------------
  Allocating headers on longest parse paths
-----------------------------------------------
Allocation Step

All Sorted parse nodes:
  parse_ipv6 (egress) with bits = 320 and max = 9
  parse_inner_ipv6 (egress) with bits = 320 and max = 9
  egress_intrinsic_metadata (egress) with bits = 64 and max = 9
  parse_udp (egress) with bits = 64 and max = 9
  parse_fabric_header_cpu (egress) with bits = 72 and max = 7
  parse_inner_ipv6 (ingress) with bits = 592 and max = 5
  parse_ipv6 (ingress) with bits = 320 and max = 5
  parse_inner_ethernet (ingress) with bits = 208 and max = 5
  parse_ethernet (ingress) with bits = 112 and max = 5
  parse_ethernet (egress) with bits = 112 and max = 5
  parse_inner_ethernet (egress) with bits = 112 and max = 5
  parse_qinq (ingress) with bits = 32 and max = 5
  parse_qinq (egress) with bits = 32 and max = 5
  parse_fabric_payload_header (ingress) with bits = 16 and max = 5
  parse_fabric_payload_header (egress) with bits = 16 and max = 5
  parse_vxlan (egress) with bits = 64 and max = 4
  parse_inner_tcp (ingress) with bits = 200 and max = 3
  parse_fabric_timestamp_header (egress) with bits = 48 and max = 3
  parse_inner_tcp (egress) with bits = 160 and max = 2
  parse_udp (ingress) with bits = 96 and max = 2
  parse_fabric_header_cpu (ingress) with bits = 80 and max = 2
  ingress_intrinsic_metadata (ingress) with bits = 64 and max = 2
  parse_fabric_header (ingress) with bits = 40 and max = 2
  parse_vxlan (ingress) with bits = 93 and max = 1
  parse_fabric_timestamp_header (ingress) with bits = 48 and max = 1
  parse_snap_header (ingress) with bits = 40 and max = 1
  parse_fabric_header (egress) with bits = 40 and max = 1
  parse_snap_header (egress) with bits = 40 and max = 1
  parse_qinq_vlan (ingress) with bits = 32 and max = 1
  parse_qinq_vlan (egress) with bits = 32 and max = 1
  parse_llc_header (ingress) with bits = 24 and max = 1
  parse_llc_header (egress) with bits = 24 and max = 1
  ingress_parse_aux (ingress) with bits = 16 and max = 1
  egress_for_mirror_buffer (egress) with bits = 16 and max = 1
  egress_parse_aux (egress) with bits = 8 and max = 1
  start () with bits = 0 and max = 0
  ingress_parser_control () with bits = 0 and max = 0
  --ingress-- () with bits = 0 and max = 0
  __super_start__ () with bits = 0 and max = 0
  egress_parser_control () with bits = 0 and max = 0
  start () with bits = 0 and max = 0
  --egress-- () with bits = 0 and max = 0
Total packet bits: 3080
Total meta bits: 477
Total bits: 3557
Working on parse node parse_ipv6 (20) (egress)

-------------------------------------------
Allocating parsed header: pkt fields (8) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 320
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 320
unused_metadata_container_bits = 0
min_parse_states = 2
bits_per_state = 160
Parse state 0 (224 bits)
  ipv6.version [3:0]
  ipv6.trafficClass [7:0]
  ipv6.flowLabel [19:0]
  ipv6.payloadLen [15:0]
  ipv6.nextHdr [7:0]
  ipv6.hopLimit [7:0]
  ipv6.srcAddr [127:0]
  ipv6.dstAddr [127:96]
-----------------------------------------------------------------------------------------
|        Name       |  BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------
|    ipv6.version   |  4  |   False   |  -  |  -   |     -     |    1     |     2      |
| ipv6.trafficClass |  8  |   False   |  -  |  -   |     -     |    2     |     2      |
|   ipv6.flowLabel  |  20 |   False   |  -  |  -   |     -     |    3     |     2      |
|  ipv6.payloadLen  |  16 |   False   |  -  | [16] |    [8]    |    1     |     9      |
|    ipv6.nextHdr   |  8  |   False   |  -  |  -   |     -     |    1     |     5      |
|   ipv6.hopLimit   |  8  |   False   |  -  | [8]  |     -     |    1     |     2      |
|    ipv6.srcAddr   | 128 |   False   |  -  |  -   |     -     |    16    |     2      |
|    ipv6.dstAddr   |  32 |   False   |  -  |  -   |     -     |    16    |     2      |
-----------------------------------------------------------------------------------------

Packing options: 44716
MAU containers available:
  8-bit: 39
  16-bit: 61
  32-bit: 44
Tagalong containers available:
  8-bit: 32
  16-bit: 48
  32-bit: 32
Initial packing options: 524

Packing option 0:  [32, 16, 8, 8, 32, 32, 32, 32, 32]
MAU containers after:
  8-bit: 32
  16-bit: 52
  32-bit: 32
+----------------------------+
|  ipv6.version [3:0]        |
|  ipv6.trafficClass [7:0]   |
|  ipv6.flowLabel [19:0]     |
+----------------------------+
|  ipv6.payloadLen [15:0]    |
+----------------------------+
|  ipv6.nextHdr [7:0]        |
+----------------------------+
|  ipv6.hopLimit [7:0]       |
+----------------------------+
|  ipv6.srcAddr [127:96]     |
+----------------------------+
|  ipv6.srcAddr [95:64]      |
+----------------------------+
|  ipv6.srcAddr [63:32]      |
+----------------------------+
|  ipv6.srcAddr [31:0]       |
+----------------------------+
|  ipv6.dstAddr [127:96]     |
+----------------------------+

Looking at ipv6.version (egress) [3:0], with test_alloc = True
----> ipv6.version (egress) is allocated? False
Looking at ipv6.trafficClass (egress) [7:0], with test_alloc = True
Looking at ipv6.flowLabel (egress) [19:0], with test_alloc = True

MAU groups: 3
  Group 1 32 bits -- avail 14 -- ingress avail 12 and remain 8 and promised 0 and req 0 -- egress avail 14 and remain 10 and promised 4 and req 4 -- as if deparsed True -- container_to_use phv20
  Group 2 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv32
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv48
***Allocating phv20[31:28] for ipv6.version[3:0]
***Allocating phv20[27:20] for ipv6.trafficClass[7:0]
***Allocating phv20[19:0] for ipv6.flowLabel[19:0]
Looking at ipv6.payloadLen (egress) [15:0], with test_alloc = True
----> ipv6.payloadLen (egress) is allocated? False

MAU groups: 4
  Group 10 16 bits -- avail 14 -- ingress avail 7 and remain 7 and promised 0 and req 0 -- egress avail 14 and remain 7 and promised 7 and req 4 -- as if deparsed True -- container_to_use phv162
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 9 and promised 7 and req 4 -- as if deparsed True -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 9 and promised 7 and req 4 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 9 and promised 7 and req 4 -- as if deparsed True -- container_to_use phv208
***Allocating phv162[15:0] for ipv6.payloadLen[15:0]
Looking at ipv6.nextHdr (egress) [7:0], with test_alloc = True
----> ipv6.nextHdr (egress) is allocated? False

MAU groups: 3
  Group 5 8 bits -- avail 11 -- ingress avail 7 and remain 0 and promised 0 and req 0 -- egress avail 11 and remain 4 and promised 7 and req 6 -- as if deparsed True -- container_to_use phv89
  Group 6 8 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 13 and promised 3 and req 2 -- as if deparsed True -- container_to_use phv96
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 13 and promised 3 and req 2 -- as if deparsed True -- container_to_use phv112
***Allocating phv89[7:0] for ipv6.nextHdr[7:0]
Looking at ipv6.hopLimit (egress) [7:0], with test_alloc = True
----> ipv6.hopLimit (egress) is allocated? False

MAU groups: 3
  Group 5 8 bits -- avail 10 -- ingress avail 0 and remain 0 and promised 0 and req 0 -- egress avail 10 and remain 2 and promised 8 and req 7 -- as if deparsed True -- container_to_use phv91
  Group 6 8 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv96
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv112
***Allocating phv91[7:0] for ipv6.hopLimit[7:0]
Looking at ipv6.srcAddr (egress) [127:96], with test_alloc = True
----> ipv6.srcAddr (egress) is allocated? False

MAU groups: 3
  Group 1 32 bits -- avail 13 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 13 and remain 8 and promised 5 and req 5 -- as if deparsed True -- container_to_use phv22
  Group 2 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv32
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv48
***Allocating phv22[31:0] for ipv6.srcAddr[127:96]
Looking at ipv6.srcAddr (egress) [95:64], with test_alloc = True
----> ipv6.srcAddr (egress) is allocated? False

MAU groups: 3
  Group 1 32 bits -- avail 12 -- ingress avail 8 and remain 4 and promised 0 and req 0 -- egress avail 12 and remain 6 and promised 6 and req 6 -- as if deparsed True -- container_to_use phv24
  Group 2 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv32
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv48
***Allocating phv24[31:0] for ipv6.srcAddr[95:64]
Looking at ipv6.srcAddr (egress) [63:32], with test_alloc = True
----> ipv6.srcAddr (egress) is allocated? False

MAU groups: 3
  Group 1 32 bits -- avail 11 -- ingress avail 4 and remain 4 and promised 0 and req 0 -- egress avail 11 and remain 4 and promised 7 and req 7 -- as if deparsed True -- container_to_use phv26
  Group 2 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv32
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv48
***Allocating phv26[31:0] for ipv6.srcAddr[63:32]
Looking at ipv6.srcAddr (egress) [31:0], with test_alloc = True
----> ipv6.srcAddr (egress) is allocated? False

MAU groups: 3
  Group 1 32 bits -- avail 10 -- ingress avail 4 and remain 0 and promised 0 and req 0 -- egress avail 10 and remain 2 and promised 8 and req 8 -- as if deparsed True -- container_to_use phv28
  Group 2 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv32
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv48
***Allocating phv28[31:0] for ipv6.srcAddr[31:0]
Looking at ipv6.dstAddr (egress) [127:96], with test_alloc = True
----> ipv6.dstAddr (egress) is allocated? False

MAU groups: 3
  Group 1 32 bits -- avail 9 -- ingress avail 0 and remain 0 and promised 0 and req 0 -- egress avail 9 and remain 0 and promised 9 and req 9 -- as if deparsed True -- container_to_use phv30
  Group 2 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv32
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv48
***Allocating phv30[31:0] for ipv6.dstAddr[127:96]
Packing options tried: 1
Packing options skipped: 0

Parse state 1 (96 bits)
  ipv6.dstAddr [95:0]
-----------------------------------------------------------------------------------
|     Name     | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------
| ipv6.dstAddr | 96 |   False   |  -  |  -   |     -     |    16    |     2      |
-----------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 292
MAU containers available:
  8-bit: 34
  16-bit: 55
  32-bit: 32
Tagalong containers available:
  8-bit: 32
  16-bit: 48
  32-bit: 32
Initial packing options: 292

Packing option 0:  [8, 8, 16, 32, 32]
MAU containers after:
  8-bit: 30
  16-bit: 53
  32-bit: 28
+-----------------------+
|  ipv6.dstAddr [95:88] |
+-----------------------+
|  ipv6.dstAddr [87:80] |
+-----------------------+
|  ipv6.dstAddr [79:64] |
+-----------------------+
|  ipv6.dstAddr [63:32] |
+-----------------------+
|  ipv6.dstAddr [31:0]  |
+-----------------------+

Looking at ipv6.dstAddr (egress) [95:88], with test_alloc = True
----> ipv6.dstAddr (egress) is allocated? False

MAU groups: 3
  Group 5 8 bits -- avail 9 -- ingress avail 0 and remain 0 and promised 0 and req 0 -- egress avail 9 and remain 0 and promised 9 and req 8 -- as if deparsed True -- container_to_use phv93
  Group 6 8 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv96
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv112
***Allocating phv93[7:0] for ipv6.dstAddr[95:88]
Looking at ipv6.dstAddr (egress) [87:80], with test_alloc = True
----> ipv6.dstAddr (egress) is allocated? False

MAU groups: 2
  Group 6 8 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv96
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv112
***Allocating phv96[7:0] for ipv6.dstAddr[87:80]
Looking at ipv6.dstAddr (egress) [79:64], with test_alloc = True
----> ipv6.dstAddr (egress) is allocated? False

MAU groups: 4
  Group 10 16 bits -- avail 13 -- ingress avail 7 and remain 5 and promised 0 and req 0 -- egress avail 13 and remain 5 and promised 8 and req 5 -- as if deparsed True -- container_to_use phv165
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208
***Allocating phv165[15:0] for ipv6.dstAddr[79:64]
Looking at ipv6.dstAddr (egress) [63:32], with test_alloc = True
----> ipv6.dstAddr (egress) is allocated? False

MAU groups: 2
  Group 2 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv32
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv48
***Allocating phv32[31:0] for ipv6.dstAddr[63:32]
Looking at ipv6.dstAddr (egress) [31:0], with test_alloc = True
----> ipv6.dstAddr (egress) is allocated? False

MAU groups: 2
  Group 2 32 bits -- avail 15 -- ingress avail 12 and remain 12 and promised 0 and req 0 -- egress avail 15 and remain 12 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv34
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv48
***Allocating phv34[31:0] for ipv6.dstAddr[31:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_ipv6 (egress) took 10.32 seconds
Working on parse node parse_inner_ipv6 (40) (egress)

-------------------------------------------
Allocating parsed header: pkt fields (8) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 320
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 320
unused_metadata_container_bits = 0
min_parse_states = 2
bits_per_state = 160
Parse state 0 (224 bits)
  inner_ipv6.version [3:0]
  inner_ipv6.trafficClass [7:0]
  inner_ipv6.flowLabel [19:0]
  inner_ipv6.payloadLen [15:0]
  inner_ipv6.nextHdr [7:0]
  inner_ipv6.hopLimit [7:0]
  inner_ipv6.srcAddr [127:0]
  inner_ipv6.dstAddr [127:96]
------------------------------------------------------------------------------------------------------------------------------------
|           Name          |  BW | Tagalong? |                   Req                    | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------------------------------------------
|    inner_ipv6.version   |  4  |   False   |                [(32, 4)]                 |  -   |     -     |    1     |     2      |
| inner_ipv6.trafficClass |  8  |   False   |                [(32, 8)]                 |  -   |     -     |    2     |     2      |
|   inner_ipv6.flowLabel  |  20 |   False   |                [(32, 20)]                |  -   |     -     |    3     |     2      |
|  inner_ipv6.payloadLen  |  16 |   False   |                [(16, 16)]                |  -   |    [8]    |    2     |     9      |
|    inner_ipv6.nextHdr   |  8  |   False   |                 [(8, 8)]                 |  -   |     -     |    1     |     5      |
|   inner_ipv6.hopLimit   |  8  |   False   |                 [(8, 8)]                 |  -   |     -     |    1     |     2      |
|    inner_ipv6.srcAddr   | 128 |   False   | [(32, 32), (32, 32), (32, 32), (32, 32)] |  -   |     -     |    16    |     2      |
|    inner_ipv6.dstAddr   |  32 |   False   |                [(32, 32)]                |  -   |     -     |    16    |     2      |
------------------------------------------------------------------------------------------------------------------------------------

Packing options: 44716
MAU containers available:
  8-bit: 30
  16-bit: 53
  32-bit: 28
Tagalong containers available:
  8-bit: 32
  16-bit: 48
  32-bit: 32
Initial packing options: 44716

Packing option 419:  [32, 16, 8, 8, 32, 32, 32, 32, 32]
MAU containers after:
  8-bit: 30
  16-bit: 53
  32-bit: 28
+----------------------------------+
|  inner_ipv6.version [3:0]        |
|  inner_ipv6.trafficClass [7:0]   |
|  inner_ipv6.flowLabel [19:0]     |
+----------------------------------+
|  inner_ipv6.payloadLen [15:0]    |
+----------------------------------+
|  inner_ipv6.nextHdr [7:0]        |
+----------------------------------+
|  inner_ipv6.hopLimit [7:0]       |
+----------------------------------+
|  inner_ipv6.srcAddr [127:96]     |
+----------------------------------+
|  inner_ipv6.srcAddr [95:64]      |
+----------------------------------+
|  inner_ipv6.srcAddr [63:32]      |
+----------------------------------+
|  inner_ipv6.srcAddr [31:0]       |
+----------------------------------+
|  inner_ipv6.dstAddr [127:96]     |
+----------------------------------+

Looking at inner_ipv6.version (egress) [3:0], with test_alloc = True
----> inner_ipv6.version (egress) is allocated? False
Looking at inner_ipv6.trafficClass (egress) [7:0], with test_alloc = True
Looking at inner_ipv6.flowLabel (egress) [19:0], with test_alloc = True
  Group 1 32 bits -- deparsed True -- promised 8 -- ingress promised 0 and remain 0 and req 0 -- egress promised 8 and remain 0 and req 8 -- act like deparsed True -- container_to_use phv21 -- fails False
  treat as deparsed? True
Required PHV group: Group 1 32 bits
Found new container in required group phv21
***Allocating phv21[31:28] for inner_ipv6.version[3:0]
***Allocating phv21[27:20] for inner_ipv6.trafficClass[7:0]
***Allocating phv21[19:0] for inner_ipv6.flowLabel[19:0]
Looking at inner_ipv6.payloadLen (egress) [15:0], with test_alloc = True
----> inner_ipv6.payloadLen (egress) is allocated? False
  Group 10 16 bits -- deparsed True -- promised 7 -- ingress promised 0 and remain 0 and req 5 -- egress promised 7 and remain 5 and req 4 -- act like deparsed True -- container_to_use phv164 -- fails False
  treat as deparsed? True
Required PHV group: Group 10 16 bits
Found new container in required group phv164
***Allocating phv164[15:0] for inner_ipv6.payloadLen[15:0]
Looking at inner_ipv6.nextHdr (egress) [7:0], with test_alloc = True
----> inner_ipv6.nextHdr (egress) is allocated? False
  Group 5 8 bits -- deparsed True -- promised 8 -- ingress promised 0 and remain 0 and req 0 -- egress promised 8 and remain 0 and req 7 -- act like deparsed True -- container_to_use phv90 -- fails False
  treat as deparsed? True
Required PHV group: Group 5 8 bits
Found new container in required group phv90
***Allocating phv90[7:0] for inner_ipv6.nextHdr[7:0]
Looking at inner_ipv6.hopLimit (egress) [7:0], with test_alloc = True
----> inner_ipv6.hopLimit (egress) is allocated? False
  Group 5 8 bits -- deparsed True -- promised 7 -- ingress promised 0 and remain 0 and req 0 -- egress promised 7 and remain 0 and req 6 -- act like deparsed True -- container_to_use phv92 -- fails False
  treat as deparsed? True
Required PHV group: Group 5 8 bits
Found new container in required group phv92
***Allocating phv92[7:0] for inner_ipv6.hopLimit[7:0]
Looking at inner_ipv6.srcAddr (egress) [127:96], with test_alloc = True
----> inner_ipv6.srcAddr (egress) is allocated? False
  Group 1 32 bits -- deparsed True -- promised 7 -- ingress promised 0 and remain 0 and req 0 -- egress promised 7 and remain 0 and req 7 -- act like deparsed True -- container_to_use phv23 -- fails False
  treat as deparsed? True
Required PHV group: Group 1 32 bits
Found new container in required group phv23
***Allocating phv23[31:0] for inner_ipv6.srcAddr[127:96]
Looking at inner_ipv6.srcAddr (egress) [95:64], with test_alloc = True
----> inner_ipv6.srcAddr (egress) is allocated? False
  Group 1 32 bits -- deparsed True -- promised 6 -- ingress promised 0 and remain 0 and req 0 -- egress promised 6 and remain 0 and req 6 -- act like deparsed True -- container_to_use phv25 -- fails False
  treat as deparsed? True
Required PHV group: Group 1 32 bits
Found new container in required group phv25
***Allocating phv25[31:0] for inner_ipv6.srcAddr[95:64]
Looking at inner_ipv6.srcAddr (egress) [63:32], with test_alloc = True
----> inner_ipv6.srcAddr (egress) is allocated? False
  Group 1 32 bits -- deparsed True -- promised 5 -- ingress promised 0 and remain 0 and req 0 -- egress promised 5 and remain 0 and req 5 -- act like deparsed True -- container_to_use phv27 -- fails False
  treat as deparsed? True
Required PHV group: Group 1 32 bits
Found new container in required group phv27
***Allocating phv27[31:0] for inner_ipv6.srcAddr[63:32]
Looking at inner_ipv6.srcAddr (egress) [31:0], with test_alloc = True
----> inner_ipv6.srcAddr (egress) is allocated? False
  Group 1 32 bits -- deparsed True -- promised 4 -- ingress promised 0 and remain 0 and req 0 -- egress promised 4 and remain 0 and req 4 -- act like deparsed True -- container_to_use phv29 -- fails False
  treat as deparsed? True
Required PHV group: Group 1 32 bits
Found new container in required group phv29
***Allocating phv29[31:0] for inner_ipv6.srcAddr[31:0]
Looking at inner_ipv6.dstAddr (egress) [127:96], with test_alloc = True
----> inner_ipv6.dstAddr (egress) is allocated? False
  Group 1 32 bits -- deparsed True -- promised 3 -- ingress promised 0 and remain 0 and req 0 -- egress promised 3 and remain 0 and req 3 -- act like deparsed True -- container_to_use phv31 -- fails False
  treat as deparsed? True
Required PHV group: Group 1 32 bits
Found new container in required group phv31
***Allocating phv31[31:0] for inner_ipv6.dstAddr[127:96]
Packing options tried: 420
Packing options skipped: 0
Failure Reasons:
  Field pack does not fit (case 1) -- tried 86 variants
  Inconsistent field pack (case 2) -- tried 333 variants

Parse state 1 (96 bits)
  inner_ipv6.dstAddr [95:0]
------------------------------------------------------------------------------------------------------------------------------------
|        Name        | BW | Tagalong? |                      Req                       | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------------------------------------------
| inner_ipv6.dstAddr | 96 |   False   | [(8, 8), (8, 8), (16, 16), (32, 32), (32, 32)] |  -   |     -     |    16    |     2      |
------------------------------------------------------------------------------------------------------------------------------------

min_extracts[8] = 3
min_extracts[16] = 2
min_extracts[32] = 3
Packing options: 292
MAU containers available:
  8-bit: 30
  16-bit: 53
  32-bit: 28
Tagalong containers available:
  8-bit: 32
  16-bit: 48
  32-bit: 32
Initial packing options: 292

Packing option 0:  [8, 8, 16, 32, 32]
MAU containers after:
  8-bit: 30
  16-bit: 53
  32-bit: 28
+-----------------------------+
|  inner_ipv6.dstAddr [95:88] |
+-----------------------------+
|  inner_ipv6.dstAddr [87:80] |
+-----------------------------+
|  inner_ipv6.dstAddr [79:64] |
+-----------------------------+
|  inner_ipv6.dstAddr [63:32] |
+-----------------------------+
|  inner_ipv6.dstAddr [31:0]  |
+-----------------------------+

Looking at inner_ipv6.dstAddr (egress) [95:88], with test_alloc = True
----> inner_ipv6.dstAddr (egress) is allocated? False
  Group 5 8 bits -- deparsed True -- promised 6 -- ingress promised 0 and remain 0 and req 0 -- egress promised 6 and remain 0 and req 5 -- act like deparsed True -- container_to_use phv94 -- fails False
  treat as deparsed? True
Required PHV group: Group 5 8 bits
Found new container in required group phv94
***Allocating phv94[7:0] for inner_ipv6.dstAddr[95:88]
Looking at inner_ipv6.dstAddr (egress) [87:80], with test_alloc = True
----> inner_ipv6.dstAddr (egress) is allocated? False
  Group 6 8 bits -- deparsed True -- promised 1 -- ingress promised 0 and remain 0 and req 8 -- egress promised 1 and remain 14 and req 1 -- act like deparsed True -- container_to_use phv97 -- fails False
  treat as deparsed? True
Required PHV group: Group 6 8 bits
Found new container in required group phv97
***Allocating phv97[7:0] for inner_ipv6.dstAddr[87:80]
Looking at inner_ipv6.dstAddr (egress) [79:64], with test_alloc = True
----> inner_ipv6.dstAddr (egress) is allocated? False
  Group 10 16 bits -- deparsed True -- promised 6 -- ingress promised 0 and remain 0 and req 5 -- egress promised 6 and remain 5 and req 3 -- act like deparsed True -- container_to_use phv166 -- fails False
  treat as deparsed? True
Required PHV group: Group 10 16 bits
Found new container in required group phv166
***Allocating phv166[15:0] for inner_ipv6.dstAddr[79:64]
Looking at inner_ipv6.dstAddr (egress) [63:32], with test_alloc = True
----> inner_ipv6.dstAddr (egress) is allocated? False
  Group 2 32 bits -- deparsed True -- promised 2 -- ingress promised 0 and remain 0 and req 12 -- egress promised 2 and remain 12 and req 2 -- act like deparsed True -- container_to_use phv33 -- fails False
  treat as deparsed? True
Required PHV group: Group 2 32 bits
Found new container in required group phv33
***Allocating phv33[31:0] for inner_ipv6.dstAddr[63:32]
Looking at inner_ipv6.dstAddr (egress) [31:0], with test_alloc = True
----> inner_ipv6.dstAddr (egress) is allocated? False
  Group 2 32 bits -- deparsed True -- promised 1 -- ingress promised 0 and remain 0 and req 12 -- egress promised 1 and remain 12 and req 1 -- act like deparsed True -- container_to_use phv35 -- fails False
  treat as deparsed? True
Required PHV group: Group 2 32 bits
Found new container in required group phv35
***Allocating phv35[31:0] for inner_ipv6.dstAddr[31:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_inner_ipv6 (egress) took 125.45 seconds
Working on parse node egress_intrinsic_metadata (50) (egress)

-------------------------------------------
Allocating parsed header: pkt fields (8) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 64
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 64
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 64
Parse state 0 (64 bits)
  eg_intr_md._pad0 [6:0]
  eg_intr_md.egress_port [8:0]
  eg_intr_md.egress_rid [15:0]
  eg_intr_md._pad7 [4:0]
  eg_intr_md.egress_cos [2:0]
  eg_intr_md._pad8 [6:0]
  eg_intr_md.deflection_flag [0:0]
  eg_intr_md.pkt_length [15:0]
--------------------------------------------------------------------------------------------------------
|            Name            | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------------------
|      eg_intr_md._pad0      | 7  |   False   |     -      |  -   |     -     |    1     |     1      |
|   eg_intr_md.egress_port   | 9  |   False   |     -      |  -   |    [8]    |    1     |     1      |
|   eg_intr_md.egress_rid    | 16 |   False   |     -      |  -   |     -     |    2     |     1      |
|      eg_intr_md._pad7      | 5  |   False   |     -      |  -   |     -     |    1     |     1      |
|   eg_intr_md.egress_cos    | 3  |   False   |     -      |  -   |     -     |    1     |     1      |
|      eg_intr_md._pad8      | 7  |   False   |     -      |  -   |     -     |    1     |     1      |
| eg_intr_md.deflection_flag | 1  |   False   |     -      |  -   |     -     |    1     |     1      |
|   eg_intr_md.pkt_length    | 16 |   False   | [(16, 16)] | [16] |    [8]    |    1     |     9      |
--------------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 2
min_extracts[32] = 1
Packing options: 47
MAU containers available:
  8-bit: 30
  16-bit: 53
  32-bit: 28
Tagalong containers available:
  8-bit: 32
  16-bit: 48
  32-bit: 32
Initial packing options: 29

Packing option 16:  [16, 8, 16, 8, 16]
MAU containers after:
  8-bit: 28
  16-bit: 51
  32-bit: 28
+-------------------------------------+
|  eg_intr_md._pad0 [6:0]             |
|  eg_intr_md.egress_port [8:0]       |
+-------------------------------------+
|  eg_intr_md.egress_rid [15:8]       |
+-------------------------------------+
|  eg_intr_md.egress_rid [7:0]        |
|  eg_intr_md._pad7 [4:0]             |
|  eg_intr_md.egress_cos [2:0]        |
+-------------------------------------+
|  eg_intr_md._pad8 [6:0]             |
|  eg_intr_md.deflection_flag [0:0]   |
+-------------------------------------+
|  eg_intr_md.pkt_length [15:0]       |
+-------------------------------------+

Looking at eg_intr_md._pad0 (egress) [6:0], with test_alloc = True
----> eg_intr_md._pad0 (egress) is allocated? False
Looking at eg_intr_md.egress_port (egress) [8:0], with test_alloc = True
Checking if can overlay metadata field.
No required PHV group.
  Group 9 16 bits -- deparsed True -- avail 9 and promised 10 -- ingress promised 0 and remain 0 and req 0 -- egress promised 10 and remain 0 and req 6 -- act like deparsed True -- container_to_use phv154 -- fails True
  Group 10 16 bits -- deparsed True -- avail 10 and promised 6 -- ingress promised 0 and remain 0 and req 4 -- egress promised 6 and remain 4 and req 3 -- act like deparsed True -- container_to_use phv167 -- fails False
Could not find container to overlay in.

MAU groups: 4
  Group 10 16 bits -- avail 10 -- ingress avail 7 and remain 4 and promised 0 and req 0 -- egress avail 10 and remain 4 and promised 6 and req 3 -- as if deparsed True -- container_to_use phv167
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv208
***Allocating phv167[15:9] for eg_intr_md._pad0[6:0]
***Allocating phv167[8:0] for eg_intr_md.egress_port[8:0]
Looking at eg_intr_md.egress_rid (egress) [15:8], with test_alloc = True
----> eg_intr_md.egress_rid (egress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 4 8 bits -- deparsed False -- avail 11 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- deparsed False -- avail 5 and promised 5 -- ingress promised 0 and remain 0 and req 0 -- egress promised 5 and remain 0 and req 4 -- act like deparsed False -- container_to_use phv95 -- fails False
  Group 6 8 bits -- deparsed False -- avail 14 and promised 1 -- ingress promised 0 and remain 0 and req 8 -- egress promised 1 and remain 13 and req 0 -- act like deparsed False -- container_to_use phv98 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 5 8 bits -- avail 5 -- ingress avail 0 and remain 0 and promised 0 and req 0 -- egress avail 5 and remain 0 and promised 5 and req 4 -- as if deparsed False -- container_to_use phv95
  Group 6 8 bits -- avail 14 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 14 and remain 13 and promised 1 and req 0 -- as if deparsed False -- container_to_use phv98
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 0 -- as if deparsed False -- container_to_use phv112
***Allocating phv95[7:0] for eg_intr_md.egress_rid[15:8]
Looking at eg_intr_md.egress_rid (egress) [7:0], with test_alloc = True
----> eg_intr_md.egress_rid (egress) is allocated? False
Looking at eg_intr_md._pad7 (egress) [4:0], with test_alloc = True
Looking at eg_intr_md.egress_cos (egress) [2:0], with test_alloc = True
Checking if can overlay metadata field.
No required PHV group.
  Group 9 16 bits -- deparsed True -- avail 9 and promised 10 -- ingress promised 0 and remain 0 and req 0 -- egress promised 10 and remain 0 and req 6 -- act like deparsed True -- container_to_use phv154 -- fails True
  Group 10 16 bits -- deparsed True -- avail 9 and promised 6 -- ingress promised 0 and remain 0 and req 0 -- egress promised 6 and remain 3 and req 3 -- act like deparsed True -- container_to_use phv169 -- fails False
Could not find container to overlay in.

MAU groups: 4
  Group 10 16 bits -- avail 9 -- ingress avail 7 and remain 0 and promised 0 and req 0 -- egress avail 9 and remain 3 and promised 6 and req 3 -- as if deparsed True -- container_to_use phv169
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv208
***Allocating phv169[15:8] for eg_intr_md.egress_rid[7:0]
***Allocating phv169[7:3] for eg_intr_md._pad7[4:0]
***Allocating phv169[2:0] for eg_intr_md.egress_cos[2:0]
Looking at eg_intr_md._pad8 (egress) [6:0], with test_alloc = True
----> eg_intr_md._pad8 (egress) is allocated? False
Looking at eg_intr_md.deflection_flag (egress) [0:0], with test_alloc = True
Checking if can overlay metadata field.
No required PHV group.
  Group 4 8 bits -- deparsed False -- avail 11 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- deparsed False -- avail 4 and promised 5 -- ingress promised 0 and remain 0 and req 0 -- egress promised 5 and remain 0 and req 4 -- act like deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- deparsed False -- avail 14 and promised 1 -- ingress promised 0 and remain 0 and req 8 -- egress promised 1 and remain 13 and req 0 -- act like deparsed False -- container_to_use phv98 -- fails False
Could not find container to overlay in.

MAU groups: 2
  Group 6 8 bits -- avail 14 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 14 and remain 13 and promised 1 and req 0 -- as if deparsed False -- container_to_use phv98
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 0 -- as if deparsed False -- container_to_use phv112
***Allocating phv98[7:1] for eg_intr_md._pad8[6:0]
***Allocating phv98[0:0] for eg_intr_md.deflection_flag[0:0]
Looking at eg_intr_md.pkt_length (egress) [15:0], with test_alloc = True
----> eg_intr_md.pkt_length (egress) is allocated? False
Checking if can overlay metadata field.
Required PHV group: Group 10 16 bits
  Group 10 16 bits -- deparsed False -- avail 8 and promised 5 -- ingress promised 0 and remain 0 and req 0 -- egress promised 5 and remain 3 and req 2 -- act like deparsed False -- container_to_use phv172 -- fails False
Could not find container to overlay in.
  Group 10 16 bits -- deparsed False -- promised 5 -- ingress promised 0 and remain 0 and req 0 -- egress promised 5 and remain 3 and req 2 -- act like deparsed False -- container_to_use phv172 -- fails False
  treat as deparsed? False
Required PHV group: Group 10 16 bits
Found new container in required group phv172
***Allocating phv172[15:0] for eg_intr_md.pkt_length[15:0]
Packing options tried: 17
Packing options skipped: 0
Failure Reasons:
  Field violates bit field alignment (case 6b) -- tried 5 variants
    field: eg_intr_md.egress_port missaligned bits (container_bit, field_bit): (16, 0)
    field: eg_intr_md.egress_cos missaligned bits (container_bit, field_bit): (8, 0)
  Field in disallowed list (case 3) -- tried 5 variants
    field: eg_intr_md.egress_port
    with constraints: [
      Solitary Constraint: eg_intr_md.pkt_length <16 bits egress parsed imeta R W>
      MauGroup Constraint: eg_intr_md.pkt_length <16 bits egress parsed imeta R W> -- other field instance: egress_metadata.payload_length <16 bits egress meta R W>
      AluFieldAlignment Constraint: eg_intr_md.pkt_length <16 bits egress parsed imeta R W> -- lsb bit 0 and other instance egress_metadata.payload_length <16 bits egress meta R W> -- bit: 0
      MauGroup Constraint: eg_intr_md.pkt_length <16 bits egress parsed imeta R W> -- other field instance: ipv4.totalLen <16 bits egress parsed R W>
      AluFieldAlignment Constraint: eg_intr_md.pkt_length <16 bits egress parsed imeta R W> -- lsb bit 0 and other instance ipv4.totalLen <16 bits egress parsed R W> -- bit: 0
      ParsedAlignment Constraint: eg_intr_md.pkt_length <16 bits egress parsed imeta R W> -- lsb bit: 0
      MaxFieldSplit Constraint: eg_intr_md.pkt_length <16 bits egress parsed imeta R W> -- max split: 1
      RightAdjacentAlignment Constraint: (left) eg_intr_md.deflection_flag <1 bits egress parsed imeta R>  -- (right) eg_intr_md.pkt_length <16 bits egress parsed imeta R W>
]
  Field pack does not fit (case 1) -- tried 6 variants

>> egress_intrinsic_metadata (egress) took 4.90 seconds
Working on parse node parse_udp (24) (egress)

-------------------------------------------
Allocating parsed header: pkt fields (4) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 64
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 64
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 64
Parse state 0 (64 bits)
  udp.srcPort [15:0]
  udp.dstPort [15:0]
  udp.length_ [15:0]
  udp.checksum [15:0]
------------------------------------------------------------------------------------------------
|     Name     | BW | Tagalong? |       Req        | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------
| udp.srcPort  | 16 |   False   | [(8, 8), (8, 8)] |  -   |     -     |    2     |     4      |
| udp.dstPort  | 16 |   False   |        -         |  -   |     -     |    2     |     2      |
| udp.length_  | 16 |   False   |    [(16, 16)]    | [16] |    [8]    |    1     |     9      |
| udp.checksum | 16 |   False   |        -         |  -   |     -     |    2     |     2      |
------------------------------------------------------------------------------------------------

min_extracts[8] = 3
min_extracts[16] = 2
min_extracts[32] = 1
Packing options: 47
MAU containers available:
  8-bit: 29
  16-bit: 51
  32-bit: 28
Tagalong containers available:
  8-bit: 32
  16-bit: 48
  32-bit: 32
Initial packing options: 11

Packing option 0:  [8, 8, 16, 16, 16]
MAU containers after:
  8-bit: 29
  16-bit: 47
  32-bit: 28
+-----------------------+
|  udp.srcPort [15:8]   |
+-----------------------+
|  udp.srcPort [7:0]    |
+-----------------------+
|  udp.dstPort [15:0]   |
+-----------------------+
|  udp.length_ [15:0]   |
+-----------------------+
|  udp.checksum [15:0]  |
+-----------------------+

Looking at udp.srcPort (egress) [15:8], with test_alloc = True
----> udp.srcPort (egress) is allocated? False
  Group 5 8 bits -- deparsed True -- promised 4 -- ingress promised 0 and remain 0 and req 0 -- egress promised 4 and remain 0 and req 4 -- act like deparsed True -- container_to_use phv83 -- fails False
  treat as deparsed? True
Required PHV group: Group 5 8 bits
Found new container in required group phv83
***Allocating phv83[7:0] for udp.srcPort[15:8]
Looking at udp.srcPort (egress) [7:0], with test_alloc = True
----> udp.srcPort (egress) is allocated? False
  Group 5 8 bits -- deparsed True -- promised 3 -- ingress promised 0 and remain 0 and req 0 -- egress promised 3 and remain 0 and req 3 -- act like deparsed True -- container_to_use phv86 -- fails False
  treat as deparsed? True
Required PHV group: Group 5 8 bits
Found new container in required group phv86
***Allocating phv86[7:0] for udp.srcPort[7:0]
Looking at udp.dstPort (egress) [15:0], with test_alloc = True
----> udp.dstPort (egress) is allocated? False

MAU groups: 4
  Group 10 16 bits -- avail 7 -- ingress avail 0 and remain 0 and promised 0 and req 0 -- egress avail 7 and remain 1 and promised 6 and req 4 -- as if deparsed True -- container_to_use phv170
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208
***Allocating phv170[15:0] for udp.dstPort[15:0]
Looking at udp.length_ (egress) [15:0], with test_alloc = True
----> udp.length_ (egress) is allocated? False
  Group 10 16 bits -- deparsed True -- promised 5 -- ingress promised 0 and remain 0 and req 0 -- egress promised 5 and remain 1 and req 3 -- act like deparsed True -- container_to_use phv161 -- fails False
  treat as deparsed? True
Required PHV group: Group 10 16 bits
Found new container in required group phv161
***Allocating phv161[15:0] for udp.length_[15:0]
Looking at udp.checksum (egress) [15:0], with test_alloc = True
----> udp.checksum (egress) is allocated? False

MAU groups: 3
  Group 11 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv176
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208
***Allocating phv176[15:0] for udp.checksum[15:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_udp (egress) took 1.01 seconds
Working on parse node parse_fabric_header_cpu (44) (egress)

-------------------------------------------
Allocating parsed header: pkt fields (8) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 72
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 72
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 72
Parse state 0 (72 bits)
  fabric_header_cpu.egressQueue [4:0]
  fabric_header_cpu.txBypass [0:0]
  fabric_header_cpu.capture_tstamp_on_tx [0:0]
  fabric_header_cpu.reserved [0:0]
  fabric_header_cpu.ingressPort [15:0]
  fabric_header_cpu.ingressIfindex [15:0]
  fabric_header_cpu.ingressBd [15:0]
  fabric_header_cpu.reasonCode [15:0]
--------------------------------------------------------------------------------------------------------------------
|                  Name                  | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------------------------------
|     fabric_header_cpu.egressQueue      | 5  |    True   |     -      |  -   |     -     |    1     |     1      |
|       fabric_header_cpu.txBypass       | 1  |    True   |     -      |  -   |     -     |    1     |     1      |
| fabric_header_cpu.capture_tstamp_on_tx | 1  |    True   |     -      |  -   |     -     |    1     |     1      |
|       fabric_header_cpu.reserved       | 1  |    True   |     -      |  -   |     -     |    1     |     1      |
|     fabric_header_cpu.ingressPort      | 16 |   False   | [(16, 9)]  | [16] |     -     |    2     |     2      |
|    fabric_header_cpu.ingressIfindex    | 16 |   False   | [(16, 14)] | [16] |     -     |    2     |     2      |
|      fabric_header_cpu.ingressBd       | 16 |   False   | [(16, 14)] | [16] |     -     |    2     |     7      |
|      fabric_header_cpu.reasonCode      | 16 |   False   | [(16, 16)] |  -   |     -     |    2     |     2      |
--------------------------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 5
min_extracts[32] = 1
Packing options: 60
MAU containers available:
  8-bit: 29
  16-bit: 47
  32-bit: 28
Tagalong containers available:
  8-bit: 32
  16-bit: 48
  32-bit: 32
Initial packing options: 5

Packing option 4:  [8, 16, 16, 16, 16]
MAU containers after:
  8-bit: 29
  16-bit: 47
  32-bit: 28
+-------------------------------------------------+
|  fabric_header_cpu.egressQueue [4:0]            |
|  fabric_header_cpu.txBypass [0:0]               |
|  fabric_header_cpu.capture_tstamp_on_tx [0:0]   |
|  fabric_header_cpu.reserved [0:0]               |
+-------------------------------------------------+
|  fabric_header_cpu.ingressPort [15:0]           |
+-------------------------------------------------+
|  fabric_header_cpu.ingressIfindex [15:0]        |
+-------------------------------------------------+
|  fabric_header_cpu.ingressBd [15:0]             |
+-------------------------------------------------+
|  fabric_header_cpu.reasonCode [15:0]            |
+-------------------------------------------------+

Looking at fabric_header_cpu.egressQueue (egress) [4:0], with test_alloc = True
----> fabric_header_cpu.egressQueue (egress) is allocated? False
Looking at fabric_header_cpu.txBypass (egress) [0:0], with test_alloc = True
Looking at fabric_header_cpu.capture_tstamp_on_tx (egress) [0:0], with test_alloc = True
Looking at fabric_header_cpu.reserved (egress) [0:0], with test_alloc = True
***Allocating phv288[7:3] for fabric_header_cpu.egressQueue[4:0]
***Allocating phv288[2:2] for fabric_header_cpu.txBypass[0:0]
***Allocating phv288[1:1] for fabric_header_cpu.capture_tstamp_on_tx[0:0]
***Allocating phv288[0:0] for fabric_header_cpu.reserved[0:0]
Looking at fabric_header_cpu.ingressPort (egress) [15:0], with test_alloc = True
----> fabric_header_cpu.ingressPort (egress) is allocated? False
  Group 9 16 bits -- deparsed True -- promised 9 -- ingress promised 0 and remain 0 and req 0 -- egress promised 9 and remain 0 and req 5 -- act like deparsed True -- container_to_use phv152 -- fails False
  treat as deparsed? True
Required PHV group: Group 9 16 bits
Found new container in required group phv152
***Allocating phv152[15:0] for fabric_header_cpu.ingressPort[15:0]
Looking at fabric_header_cpu.ingressIfindex (egress) [15:0], with test_alloc = True
----> fabric_header_cpu.ingressIfindex (egress) is allocated? False
  Group 9 16 bits -- deparsed True -- promised 8 -- ingress promised 0 and remain 0 and req 0 -- egress promised 8 and remain 0 and req 4 -- act like deparsed True -- container_to_use phv147 -- fails False
  treat as deparsed? True
Required PHV group: Group 9 16 bits
Found new container in required group phv147
***Allocating phv147[15:0] for fabric_header_cpu.ingressIfindex[15:0]
Looking at fabric_header_cpu.ingressBd (egress) [15:0], with test_alloc = True
----> fabric_header_cpu.ingressBd (egress) is allocated? False
  Group 9 16 bits -- deparsed True -- promised 7 -- ingress promised 0 and remain 0 and req 0 -- egress promised 7 and remain 0 and req 3 -- act like deparsed True -- container_to_use phv145 -- fails False
  treat as deparsed? True
Required PHV group: Group 9 16 bits
Found new container in required group phv145
***Allocating phv145[15:0] for fabric_header_cpu.ingressBd[15:0]
Looking at fabric_header_cpu.reasonCode (egress) [15:0], with test_alloc = True
----> fabric_header_cpu.reasonCode (egress) is allocated? False
  Group 9 16 bits -- deparsed True -- promised 6 -- ingress promised 0 and remain 0 and req 0 -- egress promised 6 and remain 0 and req 2 -- act like deparsed True -- container_to_use phv149 -- fails False
  treat as deparsed? True
Required PHV group: Group 9 16 bits
Found new container in required group phv149
***Allocating phv149[15:0] for fabric_header_cpu.reasonCode[15:0]
Packing options tried: 5
Packing options skipped: 0
Failure Reasons:
  Field pack does not fit (case 1) -- tried 4 variants

>> parse_fabric_header_cpu (egress) took 0.30 seconds
Working on parse node parse_inner_ipv6 (40) (ingress)

-------------------------------------------
Allocating parsed header: pkt fields (8) / meta fields (4) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 320
Set metadata bits: 272
Gress: ingress
bits_will_need_to_parse = 592
unused_metadata_container_bits = 0
min_parse_states = 3
bits_per_state = 200

 >>>> meta field ipv6_metadata.lkp_ipv6_sa (ingress) is allocated?  False

 >>>> meta field ipv6_metadata.lkp_ipv6_da (ingress) is allocated?  False

 >>>> meta field l3_metadata.lkp_ip_proto (ingress) is allocated?  False

 >>>> meta field l3_metadata.lkp_ip_ttl (ingress) is allocated?  False
Parse state 0 (224 bits)
  inner_ipv6.version [3:0]
  inner_ipv6.trafficClass [7:0]
  inner_ipv6.flowLabel [19:0]
  inner_ipv6.payloadLen [15:0]
  inner_ipv6.nextHdr [7:0]
  inner_ipv6.hopLimit [7:0]
  inner_ipv6.srcAddr [127:112]
  l3_metadata.lkp_ip_proto [7:0]
  l3_metadata.lkp_ip_ttl [7:0]
  ipv6_metadata.lkp_ipv6_sa [127:0]
-------------------------------------------------------------------------------------------------
|            Name           |  BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------
|     inner_ipv6.version    |  4  |   False   |  -  |  -   |     -     |    1     |     5      |
|  inner_ipv6.trafficClass  |  8  |    True   |  -  |  -   |     -     |    2     |     1      |
|    inner_ipv6.flowLabel   |  20 |    True   |  -  |  -   |     -     |    3     |     1      |
|   inner_ipv6.payloadLen   |  16 |    True   |  -  |  -   |     -     |    2     |     1      |
|     inner_ipv6.nextHdr    |  8  |    True   |  -  |  -   |     -     |    1     |     1      |
|    inner_ipv6.hopLimit    |  8  |    True   |  -  |  -   |     -     |    1     |     1      |
|     inner_ipv6.srcAddr    |  16 |    True   |  -  |  -   |     -     |    16    |     1      |
|  l3_metadata.lkp_ip_proto |  8  |   False   |  -  |  -   |     -     |    1     |     3      |
|   l3_metadata.lkp_ip_ttl  |  8  |   False   |  -  |  -   |     -     |    1     |     3      |
| ipv6_metadata.lkp_ipv6_sa | 128 |   False   |  -  |  -   |     -     |    16    |     2      |
-------------------------------------------------------------------------------------------------

Packing options: 44716
MAU containers available:
  8-bit: 35
  16-bit: 40
  32-bit: 38
Tagalong containers available:
  8-bit: 28
  16-bit: 42
  32-bit: 28
Initial packing options: 5547

Packing option 0:  [16, 32, 32, 8, 8, 32, 32, 32, 32]
MAU containers after:
  8-bit: 29
  16-bit: 35
  32-bit: 30
+------------------------------------+
|  inner_ipv6.version [3:0]          |
|  inner_ipv6.trafficClass [7:0]     |
|  inner_ipv6.flowLabel [19:16]      |
+------------------------------------+
|  inner_ipv6.flowLabel [15:0]       |
|  inner_ipv6.payloadLen [15:0]      |
+------------------------------------+
|  inner_ipv6.nextHdr [7:0]          |
|  inner_ipv6.hopLimit [7:0]         |
|  inner_ipv6.srcAddr [127:112]      |
+------------------------------------+
|  l3_metadata.lkp_ip_proto [7:0]    |
+------------------------------------+
|  l3_metadata.lkp_ip_ttl [7:0]      |
+------------------------------------+
|  ipv6_metadata.lkp_ipv6_sa [127:96]|
+------------------------------------+
|  ipv6_metadata.lkp_ipv6_sa [95:64] |
+------------------------------------+
|  ipv6_metadata.lkp_ipv6_sa [63:32] |
+------------------------------------+
|  ipv6_metadata.lkp_ipv6_sa [31:0]  |
+------------------------------------+

Looking at inner_ipv6.version (ingress) [3:0], with test_alloc = True
----> inner_ipv6.version (ingress) is allocated? False
Looking at inner_ipv6.trafficClass (ingress) [7:0], with test_alloc = True
Looking at inner_ipv6.flowLabel (ingress) [19:16], with test_alloc = True

MAU groups: 3
  Group 11 16 bits -- avail 8 -- ingress avail 8 and remain 5 and promised 3 and req 2 -- egress avail 8 and remain 6 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv184
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 13 and promised 3 and req 2 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 13 and promised 3 and req 2 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv208
***Allocating phv184[15:12] for inner_ipv6.version[3:0]
***Allocating phv184[11:4] for inner_ipv6.trafficClass[7:0]
***Allocating phv184[3:0] for inner_ipv6.flowLabel[19:16]
Looking at inner_ipv6.flowLabel (ingress) [15:0], with test_alloc = True
----> inner_ipv6.flowLabel (ingress) is allocated? False
Looking at inner_ipv6.payloadLen (ingress) [15:0], with test_alloc = True
***Allocating phv260[31:16] for inner_ipv6.flowLabel[15:0]
***Allocating phv260[15:0] for inner_ipv6.payloadLen[15:0]
Looking at inner_ipv6.nextHdr (ingress) [7:0], with test_alloc = True
----> inner_ipv6.nextHdr (ingress) is allocated? False
Looking at inner_ipv6.hopLimit (ingress) [7:0], with test_alloc = True
Looking at inner_ipv6.srcAddr (ingress) [127:112], with test_alloc = True
***Allocating phv261[31:24] for inner_ipv6.nextHdr[7:0]
***Allocating phv261[23:16] for inner_ipv6.hopLimit[7:0]
***Allocating phv261[15:0] for inner_ipv6.srcAddr[127:112]
Looking at l3_metadata.lkp_ip_proto (ingress) [7:0], with test_alloc = True
----> l3_metadata.lkp_ip_proto (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 4 8 bits -- deparsed False -- avail 11 and promised 2 -- ingress promised 2 and remain 1 and req 9 -- egress promised 0 and remain 8 and req 0 -- act like deparsed False -- container_to_use phv70 -- fails False
  Group 5 8 bits -- deparsed False -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- deparsed False -- avail 13 and promised 2 -- ingress promised 2 and remain 1 and req 6 -- egress promised 0 and remain 5 and req 0 -- act like deparsed False -- container_to_use phv105 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 6 8 bits -- avail 13 -- ingress avail 8 and remain 6 and promised 2 and req 1 -- egress avail 13 and remain 5 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv105
  Group 4 8 bits -- avail 11 -- ingress avail 11 and remain 9 and promised 2 and req 1 -- egress avail 8 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv70
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv113
***Allocating phv105[7:0] for l3_metadata.lkp_ip_proto[7:0]
Looking at l3_metadata.lkp_ip_ttl (ingress) [7:0], with test_alloc = True
----> l3_metadata.lkp_ip_ttl (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 4 8 bits -- deparsed False -- avail 11 and promised 2 -- ingress promised 2 and remain 1 and req 9 -- egress promised 0 and remain 8 and req 0 -- act like deparsed False -- container_to_use phv70 -- fails False
  Group 5 8 bits -- deparsed False -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- deparsed False -- avail 12 and promised 3 -- ingress promised 3 and remain 2 and req 4 -- egress promised 0 and remain 5 and req 0 -- act like deparsed False -- container_to_use phv107 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 6 8 bits -- avail 12 -- ingress avail 7 and remain 4 and promised 3 and req 2 -- egress avail 12 and remain 5 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv107
  Group 4 8 bits -- avail 11 -- ingress avail 11 and remain 9 and promised 2 and req 1 -- egress avail 8 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv70
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv113
***Allocating phv107[7:0] for l3_metadata.lkp_ip_ttl[7:0]
Looking at ipv6_metadata.lkp_ipv6_sa (ingress) [127:96], with test_alloc = True
----> ipv6_metadata.lkp_ipv6_sa (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 0 32 bits -- deparsed False -- avail 13 and promised 5 -- ingress promised 5 and remain 1 and req 8 -- egress promised 0 and remain 8 and req 0 -- act like deparsed False -- container_to_use phv7 -- fails False
  Group 1 32 bits -- deparsed False -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- deparsed False -- avail 12 and promised 2 -- ingress promised 2 and remain 1 and req 10 -- egress promised 0 and remain 8 and req 0 -- act like deparsed False -- container_to_use phv37 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 2 32 bits -- avail 12 -- ingress avail 12 and remain 10 and promised 2 and req 1 -- egress avail 12 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv37
  Group 0 32 bits -- avail 13 -- ingress avail 13 and remain 8 and promised 5 and req 1 -- egress avail 12 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv7
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 12 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv49
***Allocating phv37[31:0] for ipv6_metadata.lkp_ipv6_sa[127:96]
Looking at ipv6_metadata.lkp_ipv6_sa (ingress) [95:64], with test_alloc = True
----> ipv6_metadata.lkp_ipv6_sa (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 0 32 bits -- deparsed False -- avail 13 and promised 5 -- ingress promised 5 and remain 1 and req 8 -- egress promised 0 and remain 8 and req 0 -- act like deparsed False -- container_to_use phv7 -- fails False
  Group 1 32 bits -- deparsed False -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- deparsed False -- avail 11 and promised 3 -- ingress promised 3 and remain 2 and req 8 -- egress promised 0 and remain 8 and req 0 -- act like deparsed False -- container_to_use phv39 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 2 32 bits -- avail 11 -- ingress avail 11 and remain 8 and promised 3 and req 2 -- egress avail 11 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv39
  Group 0 32 bits -- avail 13 -- ingress avail 13 and remain 8 and promised 5 and req 1 -- egress avail 12 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv7
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 12 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv49
***Allocating phv39[31:0] for ipv6_metadata.lkp_ipv6_sa[95:64]
Looking at ipv6_metadata.lkp_ipv6_sa (ingress) [63:32], with test_alloc = True
----> ipv6_metadata.lkp_ipv6_sa (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 0 32 bits -- deparsed False -- avail 13 and promised 5 -- ingress promised 5 and remain 1 and req 8 -- egress promised 0 and remain 8 and req 0 -- act like deparsed False -- container_to_use phv7 -- fails False
  Group 1 32 bits -- deparsed False -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- deparsed False -- avail 10 and promised 4 -- ingress promised 4 and remain 3 and req 6 -- egress promised 0 and remain 4 and req 0 -- act like deparsed False -- container_to_use phv41 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 2 32 bits -- avail 10 -- ingress avail 10 and remain 6 and promised 4 and req 3 -- egress avail 10 and remain 4 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv41
  Group 0 32 bits -- avail 13 -- ingress avail 13 and remain 8 and promised 5 and req 1 -- egress avail 12 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv7
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 12 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv49
***Allocating phv41[31:0] for ipv6_metadata.lkp_ipv6_sa[63:32]
Looking at ipv6_metadata.lkp_ipv6_sa (ingress) [31:0], with test_alloc = True
----> ipv6_metadata.lkp_ipv6_sa (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 0 32 bits -- deparsed False -- avail 13 and promised 5 -- ingress promised 5 and remain 1 and req 8 -- egress promised 0 and remain 8 and req 0 -- act like deparsed False -- container_to_use phv7 -- fails False
  Group 1 32 bits -- deparsed False -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- deparsed False -- avail 9 and promised 5 -- ingress promised 5 and remain 4 and req 4 -- egress promised 0 and remain 4 and req 0 -- act like deparsed False -- container_to_use phv43 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 2 32 bits -- avail 9 -- ingress avail 9 and remain 4 and promised 5 and req 4 -- egress avail 9 and remain 4 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv43
  Group 0 32 bits -- avail 13 -- ingress avail 13 and remain 8 and promised 5 and req 1 -- egress avail 12 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv7
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 12 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv49
***Allocating phv43[31:0] for ipv6_metadata.lkp_ipv6_sa[31:0]
Packing options tried: 1
Packing options skipped: 0

Parse state 1 (216 bits)
  inner_ipv6.srcAddr [111:0]
  inner_ipv6.dstAddr [127:120]
  ipv6_metadata.lkp_ipv6_da [127:32]
-------------------------------------------------------------------------------------------------
|            Name           |  BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------
|     inner_ipv6.srcAddr    | 112 |    True   |  -  |  -   |     -     |    16    |     1      |
|     inner_ipv6.dstAddr    |  8  |    True   |  -  |  -   |     -     |    16    |     1      |
| ipv6_metadata.lkp_ipv6_da |  96 |   False   |  -  |  -   |     -     |    16    |     2      |
-------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 14714
MAU containers available:
  8-bit: 31
  16-bit: 37
  32-bit: 30
Tagalong containers available:
  8-bit: 28
  16-bit: 42
  32-bit: 26
Initial packing options: 14714

Packing option 12:  [8, 16, 32, 32, 32, 32, 32, 32]
MAU containers after:
  8-bit: 31
  16-bit: 37
  32-bit: 24
+------------------------------------+
|  inner_ipv6.srcAddr [111:104]      |
+------------------------------------+
|  inner_ipv6.srcAddr [103:88]       |
+------------------------------------+
|  inner_ipv6.srcAddr [87:56]        |
+------------------------------------+
|  inner_ipv6.srcAddr [55:24]        |
+------------------------------------+
|  inner_ipv6.srcAddr [23:0]         |
|  inner_ipv6.dstAddr [127:120]      |
+------------------------------------+
|  ipv6_metadata.lkp_ipv6_da [127:96]|
+------------------------------------+
|  ipv6_metadata.lkp_ipv6_da [95:64] |
+------------------------------------+
|  ipv6_metadata.lkp_ipv6_da [63:32] |
+------------------------------------+

Looking at inner_ipv6.srcAddr (ingress) [111:104], with test_alloc = True
----> inner_ipv6.srcAddr (ingress) is allocated? False
***Allocating phv292[7:0] for inner_ipv6.srcAddr[111:104]
Looking at inner_ipv6.srcAddr (ingress) [103:88], with test_alloc = True
----> inner_ipv6.srcAddr (ingress) is allocated? False
***Allocating phv326[15:0] for inner_ipv6.srcAddr[103:88]
Looking at inner_ipv6.srcAddr (ingress) [87:56], with test_alloc = True
----> inner_ipv6.srcAddr (ingress) is allocated? False
***Allocating phv262[31:0] for inner_ipv6.srcAddr[87:56]
Looking at inner_ipv6.srcAddr (ingress) [55:24], with test_alloc = True
----> inner_ipv6.srcAddr (ingress) is allocated? False
***Allocating phv263[31:0] for inner_ipv6.srcAddr[55:24]
Looking at inner_ipv6.srcAddr (ingress) [23:0], with test_alloc = True
----> inner_ipv6.srcAddr (ingress) is allocated? False
Looking at inner_ipv6.dstAddr (ingress) [127:120], with test_alloc = True
***Allocating phv264[31:8] for inner_ipv6.srcAddr[23:0]
***Allocating phv264[7:0] for inner_ipv6.dstAddr[127:120]
Looking at ipv6_metadata.lkp_ipv6_da (ingress) [127:96], with test_alloc = True
----> ipv6_metadata.lkp_ipv6_da (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 0 32 bits -- deparsed False -- avail 13 and promised 5 -- ingress promised 5 and remain 1 and req 8 -- egress promised 0 and remain 8 and req 0 -- act like deparsed False -- container_to_use phv7 -- fails False
  Group 1 32 bits -- deparsed False -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- deparsed False -- avail 8 and promised 6 -- ingress promised 6 and remain 5 and req 2 -- egress promised 0 and remain 0 and req 0 -- act like deparsed False -- container_to_use phv45 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 2 32 bits -- avail 8 -- ingress avail 8 and remain 2 and promised 6 and req 5 -- egress avail 8 and remain 0 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv45
  Group 0 32 bits -- avail 13 -- ingress avail 13 and remain 8 and promised 5 and req 1 -- egress avail 12 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv7
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 12 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv49
***Allocating phv45[31:0] for ipv6_metadata.lkp_ipv6_da[127:96]
Looking at ipv6_metadata.lkp_ipv6_da (ingress) [95:64], with test_alloc = True
----> ipv6_metadata.lkp_ipv6_da (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 0 32 bits -- deparsed False -- avail 13 and promised 5 -- ingress promised 5 and remain 1 and req 8 -- egress promised 0 and remain 8 and req 0 -- act like deparsed False -- container_to_use phv7 -- fails False
  Group 1 32 bits -- deparsed False -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- deparsed False -- avail 7 and promised 7 -- ingress promised 7 and remain 6 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed False -- container_to_use phv47 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 2 32 bits -- avail 7 -- ingress avail 7 and remain 0 and promised 7 and req 6 -- egress avail 7 and remain 0 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv47
  Group 0 32 bits -- avail 13 -- ingress avail 13 and remain 8 and promised 5 and req 1 -- egress avail 12 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv7
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 12 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv49
***Allocating phv47[31:0] for ipv6_metadata.lkp_ipv6_da[95:64]
Looking at ipv6_metadata.lkp_ipv6_da (ingress) [63:32], with test_alloc = True
----> ipv6_metadata.lkp_ipv6_da (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 0 32 bits -- deparsed False -- avail 13 and promised 5 -- ingress promised 5 and remain 1 and req 8 -- egress promised 0 and remain 8 and req 0 -- act like deparsed False -- container_to_use phv7 -- fails False
  Group 1 32 bits -- deparsed False -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- deparsed False -- avail 6 and promised 7 -- ingress promised 8 and remain 7 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed False -- container_to_use None -- fails True
Could not find container to overlay in.

MAU groups: 2
  Group 0 32 bits -- avail 13 -- ingress avail 13 and remain 8 and promised 5 and req 1 -- egress avail 12 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv7
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 12 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv49
***Allocating phv7[31:0] for ipv6_metadata.lkp_ipv6_da[63:32]
Packing options tried: 13
Packing options skipped: 0
Failure Reasons:
  Field violates separation of metadata and headers (case 7a) -- tried 12 variants
    field: inner_ipv6.srcAddr meta: ipv6_metadata.lkp_ipv6_da

Parse state 2 (152 bits)
  inner_ipv6.dstAddr [119:0]
  ipv6_metadata.lkp_ipv6_da [31:0]
-------------------------------------------------------------------------------------------------
|            Name           |  BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------
|     inner_ipv6.dstAddr    | 120 |    True   |  -  |  -   |     -     |    16    |     1      |
| ipv6_metadata.lkp_ipv6_da |  32 |   False   |  -  |  -   |     -     |    16    |     2      |
-------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 2025
MAU containers available:
  8-bit: 31
  16-bit: 37
  32-bit: 24
Tagalong containers available:
  8-bit: 27
  16-bit: 41
  32-bit: 23
Initial packing options: 2025

Packing option 60:  [8, 8, 8, 16, 16, 32, 32, 32]
MAU containers after:
  8-bit: 31
  16-bit: 37
  32-bit: 22
+------------------------------------+
|  inner_ipv6.dstAddr [119:112]      |
+------------------------------------+
|  inner_ipv6.dstAddr [111:104]      |
+------------------------------------+
|  inner_ipv6.dstAddr [103:96]       |
+------------------------------------+
|  inner_ipv6.dstAddr [95:80]        |
+------------------------------------+
|  inner_ipv6.dstAddr [79:64]        |
+------------------------------------+
|  inner_ipv6.dstAddr [63:32]        |
+------------------------------------+
|  inner_ipv6.dstAddr [31:0]         |
+------------------------------------+
|  ipv6_metadata.lkp_ipv6_da [31:0]  |
+------------------------------------+

Looking at inner_ipv6.dstAddr (ingress) [119:112], with test_alloc = True
----> inner_ipv6.dstAddr (ingress) is allocated? False
***Allocating phv293[7:0] for inner_ipv6.dstAddr[119:112]
Looking at inner_ipv6.dstAddr (ingress) [111:104], with test_alloc = True
----> inner_ipv6.dstAddr (ingress) is allocated? False
***Allocating phv294[7:0] for inner_ipv6.dstAddr[111:104]
Looking at inner_ipv6.dstAddr (ingress) [103:96], with test_alloc = True
----> inner_ipv6.dstAddr (ingress) is allocated? False
***Allocating phv295[7:0] for inner_ipv6.dstAddr[103:96]
Looking at inner_ipv6.dstAddr (ingress) [95:80], with test_alloc = True
----> inner_ipv6.dstAddr (ingress) is allocated? False
***Allocating phv327[15:0] for inner_ipv6.dstAddr[95:80]
Looking at inner_ipv6.dstAddr (ingress) [79:64], with test_alloc = True
----> inner_ipv6.dstAddr (ingress) is allocated? False
***Allocating phv328[15:0] for inner_ipv6.dstAddr[79:64]
Looking at inner_ipv6.dstAddr (ingress) [63:32], with test_alloc = True
----> inner_ipv6.dstAddr (ingress) is allocated? False
***Allocating phv265[31:0] for inner_ipv6.dstAddr[63:32]
Looking at inner_ipv6.dstAddr (ingress) [31:0], with test_alloc = True
----> inner_ipv6.dstAddr (ingress) is allocated? False
***Allocating phv266[31:0] for inner_ipv6.dstAddr[31:0]
Looking at ipv6_metadata.lkp_ipv6_da (ingress) [31:0], with test_alloc = True
----> ipv6_metadata.lkp_ipv6_da (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 0 32 bits -- deparsed False -- avail 12 and promised 6 -- ingress promised 6 and remain 2 and req 6 -- egress promised 0 and remain 6 and req 0 -- act like deparsed False -- container_to_use phv9 -- fails False
  Group 1 32 bits -- deparsed False -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- deparsed False -- avail 6 and promised 7 -- ingress promised 8 and remain 7 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed False -- container_to_use None -- fails True
Could not find container to overlay in.

MAU groups: 2
  Group 0 32 bits -- avail 12 -- ingress avail 12 and remain 6 and promised 6 and req 2 -- egress avail 11 and remain 6 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv9
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 12 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv49
***Allocating phv9[31:0] for ipv6_metadata.lkp_ipv6_da[31:0]
Packing options tried: 61
Packing options skipped: 0
Failure Reasons:
  Field violates separation of metadata and headers (case 7a) -- tried 60 variants
    field: inner_ipv6.dstAddr meta: ipv6_metadata.lkp_ipv6_da

>> parse_inner_ipv6 (ingress) took 10.42 seconds
Working on parse node parse_ipv6 (20) (ingress)

-------------------------------------------
Allocating parsed header: pkt fields (8) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 320
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 320
unused_metadata_container_bits = 0
min_parse_states = 2
bits_per_state = 160
Parse state 0 (224 bits)
  ipv6.version [3:0]
  ipv6.trafficClass [7:0]
  ipv6.flowLabel [19:0]
  ipv6.payloadLen [15:0]
  ipv6.nextHdr [7:0]
  ipv6.hopLimit [7:0]
  ipv6.srcAddr [127:0]
  ipv6.dstAddr [127:96]
------------------------------------------------------------------------------------------------------------------------------
|        Name       |  BW | Tagalong? |                   Req                    | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------------------------------------
|    ipv6.version   |  4  |   False   |                [(16, 4)]                 |  -   |     -     |    1     |     5      |
| ipv6.trafficClass |  8  |    True   |                    -                     |  -   |     -     |    2     |     1      |
|   ipv6.flowLabel  |  20 |    True   |                    -                     |  -   |     -     |    3     |     1      |
|  ipv6.payloadLen  |  16 |    True   |                    -                     |  -   |     -     |    2     |     1      |
|    ipv6.nextHdr   |  8  |   False   |                 [(8, 8)]                 |  -   |     -     |    1     |     3      |
|   ipv6.hopLimit   |  8  |   False   |                 [(8, 8)]                 |  -   |     -     |    1     |     3      |
|    ipv6.srcAddr   | 128 |   False   | [(32, 32), (32, 32), (32, 32), (32, 32)] |  -   |     -     |    16    |     2      |
|    ipv6.dstAddr   |  32 |   False   |                [(32, 32)]                |  -   |     -     |    16    |     2      |
------------------------------------------------------------------------------------------------------------------------------

Packing options: 44716
MAU containers available:
  8-bit: 31
  16-bit: 37
  32-bit: 22
Tagalong containers available:
  8-bit: 24
  16-bit: 39
  32-bit: 21
Initial packing options: 44716

Packing option 200:  [16, 32, 8, 8, 32, 32, 32, 32, 32]
MAU containers after:
  8-bit: 31
  16-bit: 37
  32-bit: 22
+----------------------------+
|  ipv6.version [3:0]        |
|  ipv6.trafficClass [7:0]   |
|  ipv6.flowLabel [19:16]    |
+----------------------------+
|  ipv6.flowLabel [15:0]     |
|  ipv6.payloadLen [15:0]    |
+----------------------------+
|  ipv6.nextHdr [7:0]        |
+----------------------------+
|  ipv6.hopLimit [7:0]       |
+----------------------------+
|  ipv6.srcAddr [127:96]     |
+----------------------------+
|  ipv6.srcAddr [95:64]      |
+----------------------------+
|  ipv6.srcAddr [63:32]      |
+----------------------------+
|  ipv6.srcAddr [31:0]       |
+----------------------------+
|  ipv6.dstAddr [127:96]     |
+----------------------------+

Looking at ipv6.version (ingress) [3:0], with test_alloc = True
----> ipv6.version (ingress) is allocated? False
Looking at ipv6.trafficClass (ingress) [7:0], with test_alloc = True
Looking at ipv6.flowLabel (ingress) [19:16], with test_alloc = True
  Group 11 16 bits -- deparsed True -- promised 3 -- ingress promised 2 and remain 1 and req 5 -- egress promised 1 and remain 6 and req 1 -- act like deparsed True -- container_to_use phv185 -- fails False
  treat as deparsed? True
Required PHV group: Group 11 16 bits
Found new container in required group phv185
***Allocating phv185[15:12] for ipv6.version[3:0]
***Allocating phv185[11:4] for ipv6.trafficClass[7:0]
***Allocating phv185[3:0] for ipv6.flowLabel[19:16]
Looking at ipv6.flowLabel (ingress) [15:0], with test_alloc = True
----> ipv6.flowLabel (ingress) is allocated? False
Looking at ipv6.payloadLen (ingress) [15:0], with test_alloc = True
***Allocating phv267[31:16] for ipv6.flowLabel[15:0]
***Allocating phv267[15:0] for ipv6.payloadLen[15:0]
Looking at ipv6.nextHdr (ingress) [7:0], with test_alloc = True
----> ipv6.nextHdr (ingress) is allocated? False
  Group 6 8 bits -- deparsed True -- promised 2 -- ingress promised 2 and remain 2 and req 4 -- egress promised 0 and remain 5 and req 0 -- act like deparsed True -- container_to_use phv104 -- fails False
  treat as deparsed? True
Required PHV group: Group 6 8 bits
Found new container in required group phv104
***Allocating phv104[7:0] for ipv6.nextHdr[7:0]
Looking at ipv6.hopLimit (ingress) [7:0], with test_alloc = True
----> ipv6.hopLimit (ingress) is allocated? False
  Group 6 8 bits -- deparsed True -- promised 1 -- ingress promised 1 and remain 1 and req 4 -- egress promised 0 and remain 5 and req 0 -- act like deparsed True -- container_to_use phv106 -- fails False
  treat as deparsed? True
Required PHV group: Group 6 8 bits
Found new container in required group phv106
***Allocating phv106[7:0] for ipv6.hopLimit[7:0]
Looking at ipv6.srcAddr (ingress) [127:96], with test_alloc = True
----> ipv6.srcAddr (ingress) is allocated? False
  Group 2 32 bits -- deparsed True -- promised 6 -- ingress promised 6 and remain 6 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed True -- container_to_use phv36 -- fails False
  treat as deparsed? True
Required PHV group: Group 2 32 bits
Found new container in required group phv36
***Allocating phv36[31:0] for ipv6.srcAddr[127:96]
Looking at ipv6.srcAddr (ingress) [95:64], with test_alloc = True
----> ipv6.srcAddr (ingress) is allocated? False
  Group 2 32 bits -- deparsed True -- promised 5 -- ingress promised 5 and remain 5 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed True -- container_to_use phv38 -- fails False
  treat as deparsed? True
Required PHV group: Group 2 32 bits
Found new container in required group phv38
***Allocating phv38[31:0] for ipv6.srcAddr[95:64]
Looking at ipv6.srcAddr (ingress) [63:32], with test_alloc = True
----> ipv6.srcAddr (ingress) is allocated? False
  Group 2 32 bits -- deparsed True -- promised 4 -- ingress promised 4 and remain 4 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed True -- container_to_use phv40 -- fails False
  treat as deparsed? True
Required PHV group: Group 2 32 bits
Found new container in required group phv40
***Allocating phv40[31:0] for ipv6.srcAddr[63:32]
Looking at ipv6.srcAddr (ingress) [31:0], with test_alloc = True
----> ipv6.srcAddr (ingress) is allocated? False
  Group 2 32 bits -- deparsed True -- promised 3 -- ingress promised 3 and remain 3 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed True -- container_to_use phv42 -- fails False
  treat as deparsed? True
Required PHV group: Group 2 32 bits
Found new container in required group phv42
***Allocating phv42[31:0] for ipv6.srcAddr[31:0]
Looking at ipv6.dstAddr (ingress) [127:96], with test_alloc = True
----> ipv6.dstAddr (ingress) is allocated? False
  Group 2 32 bits -- deparsed True -- promised 2 -- ingress promised 2 and remain 2 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed True -- container_to_use phv44 -- fails False
  treat as deparsed? True
Required PHV group: Group 2 32 bits
Found new container in required group phv44
***Allocating phv44[31:0] for ipv6.dstAddr[127:96]
Packing options tried: 201
Packing options skipped: 0
Failure Reasons:
  Inconsistent field pack (case 2) -- tried 200 variants

Parse state 1 (96 bits)
  ipv6.dstAddr [95:0]
--------------------------------------------------------------------------------------------------------------
|     Name     | BW | Tagalong? |              Req               | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------------------------
| ipv6.dstAddr | 96 |   False   | [(32, 32), (32, 32), (32, 32)] |  -   |     -     |    16    |     2      |
--------------------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 4
Packing options: 292
MAU containers available:
  8-bit: 31
  16-bit: 37
  32-bit: 22
Tagalong containers available:
  8-bit: 24
  16-bit: 39
  32-bit: 20
Initial packing options: 292

Packing option 286:  [32, 32, 32]
MAU containers after:
  8-bit: 31
  16-bit: 37
  32-bit: 22
+-----------------------+
|  ipv6.dstAddr [95:64] |
+-----------------------+
|  ipv6.dstAddr [63:32] |
+-----------------------+
|  ipv6.dstAddr [31:0]  |
+-----------------------+

Looking at ipv6.dstAddr (ingress) [95:64], with test_alloc = True
----> ipv6.dstAddr (ingress) is allocated? False
  Group 2 32 bits -- deparsed True -- promised 1 -- ingress promised 1 and remain 1 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed True -- container_to_use phv46 -- fails False
  treat as deparsed? True
Required PHV group: Group 2 32 bits
Found new container in required group phv46
***Allocating phv46[31:0] for ipv6.dstAddr[95:64]
Looking at ipv6.dstAddr (ingress) [63:32], with test_alloc = True
----> ipv6.dstAddr (ingress) is allocated? False
  Group 0 32 bits -- deparsed True -- promised 5 -- ingress promised 5 and remain 2 and req 6 -- egress promised 0 and remain 6 and req 0 -- act like deparsed True -- container_to_use phv3 -- fails False
  treat as deparsed? True
Required PHV group: Group 0 32 bits
Found new container in required group phv3
***Allocating phv3[31:0] for ipv6.dstAddr[63:32]
Looking at ipv6.dstAddr (ingress) [31:0], with test_alloc = True
----> ipv6.dstAddr (ingress) is allocated? False
  Group 0 32 bits -- deparsed True -- promised 4 -- ingress promised 4 and remain 1 and req 6 -- egress promised 0 and remain 6 and req 0 -- act like deparsed True -- container_to_use phv4 -- fails False
  treat as deparsed? True
Required PHV group: Group 0 32 bits
Found new container in required group phv4
***Allocating phv4[31:0] for ipv6.dstAddr[31:0]
Packing options tried: 287
Packing options skipped: 0
Failure Reasons:
  Field pack does not fit (case 1) -- tried 286 variants

>> parse_ipv6 (ingress) took 121.27 seconds
Working on parse node parse_inner_ethernet (41) (ingress)

-------------------------------------------
Allocating parsed header: pkt fields (3) / meta fields (2) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 112
Set metadata bits: 96
Gress: ingress
bits_will_need_to_parse = 208
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 208

 >>>> meta field l2_metadata.lkp_mac_sa (ingress) is allocated?  False

 >>>> meta field l2_metadata.lkp_mac_da (ingress) is allocated?  False
Parse state 0 (208 bits)
  inner_ethernet.dstAddr [47:0]
  inner_ethernet.srcAddr [47:0]
  inner_ethernet.etherType [15:0]
  l2_metadata.lkp_mac_da [47:0]
  l2_metadata.lkp_mac_sa [47:0]
-----------------------------------------------------------------------------------------------
|           Name           | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------
|  inner_ethernet.dstAddr  | 48 |    True   |  -  |  -   |     -     |    6     |     1      |
|  inner_ethernet.srcAddr  | 48 |    True   |  -  |  -   |     -     |    6     |     1      |
| inner_ethernet.etherType | 16 |   False   |  -  |  -   |    [32]   |    2     |     5      |
|  l2_metadata.lkp_mac_da  | 48 |   False   |  -  |  -   |     -     |    6     |     2      |
|  l2_metadata.lkp_mac_sa  | 48 |   False   |  -  |  -   |     -     |    6     |     2      |
-----------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 16807
MAU containers available:
  8-bit: 31
  16-bit: 37
  32-bit: 22
Tagalong containers available:
  8-bit: 24
  16-bit: 39
  32-bit: 20
Initial packing options: 7152

Packing option 120:  [8, 32, 16, 8, 32, 16, 8, 8, 32, 16, 32]
MAU containers after:
  8-bit: 27
  16-bit: 30
  32-bit: 18
+-----------------------------------+
|  inner_ethernet.dstAddr [47:40]   |
+-----------------------------------+
|  inner_ethernet.dstAddr [39:8]    |
+-----------------------------------+
|  inner_ethernet.dstAddr [7:0]     |
|  inner_ethernet.srcAddr [47:40]   |
+-----------------------------------+
|  inner_ethernet.srcAddr [39:32]   |
+-----------------------------------+
|  inner_ethernet.srcAddr [31:0]    |
+-----------------------------------+
|  inner_ethernet.etherType [15:0]  |
+-----------------------------------+
|  l2_metadata.lkp_mac_da [47:40]   |
+-----------------------------------+
|  l2_metadata.lkp_mac_da [39:32]   |
+-----------------------------------+
|  l2_metadata.lkp_mac_da [31:0]    |
+-----------------------------------+
|  l2_metadata.lkp_mac_sa [47:32]   |
+-----------------------------------+
|  l2_metadata.lkp_mac_sa [31:0]    |
+-----------------------------------+

Looking at inner_ethernet.dstAddr (ingress) [47:40], with test_alloc = True
----> inner_ethernet.dstAddr (ingress) is allocated? False
***Allocating phv296[7:0] for inner_ethernet.dstAddr[47:40]
Looking at inner_ethernet.dstAddr (ingress) [39:8], with test_alloc = True
----> inner_ethernet.dstAddr (ingress) is allocated? False
***Allocating phv268[31:0] for inner_ethernet.dstAddr[39:8]
Looking at inner_ethernet.dstAddr (ingress) [7:0], with test_alloc = True
----> inner_ethernet.dstAddr (ingress) is allocated? False
Looking at inner_ethernet.srcAddr (ingress) [47:40], with test_alloc = True
***Allocating phv329[15:8] for inner_ethernet.dstAddr[7:0]
***Allocating phv329[7:0] for inner_ethernet.srcAddr[47:40]
Looking at inner_ethernet.srcAddr (ingress) [39:32], with test_alloc = True
----> inner_ethernet.srcAddr (ingress) is allocated? False
***Allocating phv297[7:0] for inner_ethernet.srcAddr[39:32]
Looking at inner_ethernet.srcAddr (ingress) [31:0], with test_alloc = True
----> inner_ethernet.srcAddr (ingress) is allocated? False
***Allocating phv269[31:0] for inner_ethernet.srcAddr[31:0]
Looking at inner_ethernet.etherType (ingress) [15:0], with test_alloc = True
----> inner_ethernet.etherType (ingress) is allocated? False

MAU groups: 3
  Group 11 16 bits -- avail 6 -- ingress avail 6 and remain 0 and promised 6 and req 4 -- egress avail 0 and remain 6 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv186
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 11 and promised 5 and req 4 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 11 and promised 5 and req 4 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv208
***Allocating phv186[15:0] for inner_ethernet.etherType[15:0]
Looking at l2_metadata.lkp_mac_da (ingress) [47:40], with test_alloc = True
----> l2_metadata.lkp_mac_da (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 4 8 bits -- deparsed False -- avail 11 and promised 2 -- ingress promised 2 and remain 1 and req 9 -- egress promised 0 and remain 8 and req 0 -- act like deparsed False -- container_to_use phv70 -- fails False
  Group 5 8 bits -- deparsed False -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- deparsed False -- avail 9 and promised 2 -- ingress promised 2 and remain 1 and req 2 -- egress promised 0 and remain 5 and req 0 -- act like deparsed False -- container_to_use phv109 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 6 8 bits -- avail 9 -- ingress avail 4 and remain 2 and promised 2 and req 1 -- egress avail 5 and remain 5 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv109
  Group 4 8 bits -- avail 11 -- ingress avail 11 and remain 9 and promised 2 and req 1 -- egress avail 8 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv70
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv113
***Allocating phv109[7:0] for l2_metadata.lkp_mac_da[47:40]
Looking at l2_metadata.lkp_mac_da (ingress) [39:32], with test_alloc = True
----> l2_metadata.lkp_mac_da (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 4 8 bits -- deparsed False -- avail 11 and promised 2 -- ingress promised 2 and remain 1 and req 9 -- egress promised 0 and remain 8 and req 0 -- act like deparsed False -- container_to_use phv70 -- fails False
  Group 5 8 bits -- deparsed False -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- deparsed False -- avail 8 and promised 3 -- ingress promised 3 and remain 2 and req 0 -- egress promised 0 and remain 5 and req 0 -- act like deparsed False -- container_to_use phv111 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 6 8 bits -- avail 8 -- ingress avail 3 and remain 0 and promised 3 and req 2 -- egress avail 5 and remain 5 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv111
  Group 4 8 bits -- avail 11 -- ingress avail 11 and remain 9 and promised 2 and req 1 -- egress avail 8 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv70
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv113
***Allocating phv111[7:0] for l2_metadata.lkp_mac_da[39:32]
Looking at l2_metadata.lkp_mac_da (ingress) [31:0], with test_alloc = True
----> l2_metadata.lkp_mac_da (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 0 32 bits -- deparsed False -- avail 9 and promised 5 -- ingress promised 5 and remain 1 and req 4 -- egress promised 0 and remain 4 and req 0 -- act like deparsed False -- container_to_use phv11 -- fails False
  Group 1 32 bits -- deparsed False -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- deparsed False -- avail 0 and promised 1 -- ingress promised 2 and remain 1 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed False -- container_to_use None -- fails True
Could not find container to overlay in.

MAU groups: 2
  Group 0 32 bits -- avail 9 -- ingress avail 9 and remain 4 and promised 5 and req 1 -- egress avail 7 and remain 4 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv11
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 1 -- egress avail 16 and remain 12 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv49
***Allocating phv11[31:0] for l2_metadata.lkp_mac_da[31:0]
Looking at l2_metadata.lkp_mac_sa (ingress) [47:32], with test_alloc = True
----> l2_metadata.lkp_mac_sa (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 8 16 bits -- deparsed True -- avail 9 and promised 10 -- ingress promised 11 and remain 5 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed True -- container_to_use phv138 -- fails True
  Group 11 16 bits -- deparsed True -- avail 12 and promised 8 -- ingress promised 7 and remain 5 and req 0 -- egress promised 1 and remain 4 and req 1 -- act like deparsed True -- container_to_use phv190 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 11 16 bits -- avail 5 -- ingress avail 5 and remain 0 and promised 7 and req 5 -- egress avail 0 and remain 4 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv190
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 2 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 2 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv208
***Allocating phv190[15:0] for l2_metadata.lkp_mac_sa[47:32]
Looking at l2_metadata.lkp_mac_sa (ingress) [31:0], with test_alloc = True
----> l2_metadata.lkp_mac_sa (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 0 32 bits -- deparsed True -- avail 8 and promised 6 -- ingress promised 6 and remain 3 and req 2 -- egress promised 0 and remain 2 and req 0 -- act like deparsed True -- container_to_use phv6 -- fails False
  Group 2 32 bits -- deparsed True -- avail 0 and promised 1 -- ingress promised 2 and remain 2 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed True -- container_to_use None -- fails True
Could not find container to overlay in.

MAU groups: 2
  Group 0 32 bits -- avail 8 -- ingress avail 8 and remain 2 and promised 6 and req 3 -- egress avail 6 and remain 2 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv6
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 2 -- egress avail 16 and remain 12 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv48
***Allocating phv6[31:0] for l2_metadata.lkp_mac_sa[31:0]
Packing options tried: 121
Packing options skipped: 0
Failure Reasons:
  Field in disallowed list (case 3) -- tried 120 variants
    field: inner_ethernet.etherType
    with constraints: [
      ParsedAlignment Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- lsb bit: 0
      MaxFieldSplit Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- max split: 6
      NoTagalong Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W>
      ContainerAlignment Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- field_bit: 0 -- bits_list: [0, 8, 16, 24]
      DifferentContainer Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: tunnel_metadata.tunnel_terminate <1 bits ingress imeta R W>
      DifferentContainer Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: l3_metadata.vrf <14 bits ingress imeta R W>
      DifferentContainer Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: ipv4_metadata.ipv4_unicast_enabled <1 bits ingress meta R W>
      DifferentContainer Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: ipv4_metadata.ipv4_urpf_mode <2 bits ingress meta R W>
      DifferentContainer Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: l3_metadata.rmac_group <10 bits ingress meta R W>
      DifferentContainer Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: l3_metadata.lkp_ip_type <2 bits ingress meta R W>
      DifferentContainer Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: multicast_metadata.bd_mrpf_group <14 bits ingress meta R W>
      DifferentContainer Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: multicast_metadata.ipv4_multicast_enabled <1 bits ingress meta R W>
      MauGroup Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: ethernet.srcAddr <48 bits ingress parsed R>
      DifferentContainer Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: ipv6_metadata.ipv6_unicast_enabled <1 bits ingress meta R W>
      DifferentContainer Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: ipv6_metadata.ipv6_urpf_mode <2 bits ingress meta R W>
      DifferentContainer Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: multicast_metadata.ipv6_multicast_enabled <1 bits ingress meta R W>
      DifferentContainer Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: tunnel_metadata.ingress_tunnel_type <5 bits ingress parsed imeta R W>
      DifferentContainer Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: ingress_metadata.egress_ifindex <14 bits ingress imeta R W>
      DifferentContainer Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: l3_metadata.fib_nexthop <16 bits ingress meta R W>
      DifferentContainer Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: l3_metadata.fib_nexthop_type <1 bits ingress meta R W>
      DifferentContainer Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: l3_metadata.fib_hit <1 bits ingress meta R W>
      DifferentContainer Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: l2_metadata.non_ip_packet <1 bits ingress meta R W>
      DifferentContainer Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W> -- other field instance: ethernet.etherType <16 bits ingress parsed R W>
      SolitaryExceptDigest Constraint: l2_metadata.lkp_mac_sa <48 bits ingress parsed meta R W>
]

>> parse_inner_ethernet (ingress) took 3.60 seconds
Working on parse node parse_ethernet (4) (ingress)

-------------------------------------------
Allocating parsed header: pkt fields (3) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 112
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 112
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 112
Parse state 0 (112 bits)
  ethernet.dstAddr [47:0]
  ethernet.srcAddr [47:0]
  ethernet.etherType [15:0]
----------------------------------------------------------------------------------------------------------------
|        Name        | BW | Tagalong? |            Req             | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------------------
|  ethernet.dstAddr  | 48 |   False   | [(8, 8), (8, 8), (32, 32)] |  -   |     -     |    6     |     2      |
|  ethernet.srcAddr  | 48 |   False   |    [(16, 16), (32, 32)]    |  -   |     -     |    6     |     2      |
| ethernet.etherType | 16 |   False   |         [(16, 16)]         |  -   |    [32]   |    2     |     5      |
----------------------------------------------------------------------------------------------------------------

min_extracts[8] = 3
min_extracts[16] = 3
min_extracts[32] = 3
Packing options: 604
MAU containers available:
  8-bit: 27
  16-bit: 32
  32-bit: 18
Tagalong containers available:
  8-bit: 22
  16-bit: 38
  32-bit: 18
Initial packing options: 604

Packing option 46:  [8, 8, 32, 16, 32, 16]
MAU containers after:
  8-bit: 27
  16-bit: 32
  32-bit: 18
+-----------------------------+
|  ethernet.dstAddr [47:40]   |
+-----------------------------+
|  ethernet.dstAddr [39:32]   |
+-----------------------------+
|  ethernet.dstAddr [31:0]    |
+-----------------------------+
|  ethernet.srcAddr [47:32]   |
+-----------------------------+
|  ethernet.srcAddr [31:0]    |
+-----------------------------+
|  ethernet.etherType [15:0]  |
+-----------------------------+

Looking at ethernet.dstAddr (ingress) [47:40], with test_alloc = True
----> ethernet.dstAddr (ingress) is allocated? False
  Group 6 8 bits -- deparsed True -- promised 2 -- ingress promised 2 and remain 2 and req 0 -- egress promised 0 and remain 5 and req 0 -- act like deparsed True -- container_to_use phv108 -- fails False
  treat as deparsed? True
Required PHV group: Group 6 8 bits
Found new container in required group phv108
***Allocating phv108[7:0] for ethernet.dstAddr[47:40]
Looking at ethernet.dstAddr (ingress) [39:32], with test_alloc = True
----> ethernet.dstAddr (ingress) is allocated? False
  Group 6 8 bits -- deparsed True -- promised 1 -- ingress promised 1 and remain 1 and req 0 -- egress promised 0 and remain 5 and req 0 -- act like deparsed True -- container_to_use phv110 -- fails False
  treat as deparsed? True
Required PHV group: Group 6 8 bits
Found new container in required group phv110
***Allocating phv110[7:0] for ethernet.dstAddr[39:32]
Looking at ethernet.dstAddr (ingress) [31:0], with test_alloc = True
----> ethernet.dstAddr (ingress) is allocated? False
  Group 0 32 bits -- deparsed True -- promised 5 -- ingress promised 5 and remain 2 and req 2 -- egress promised 0 and remain 2 and req 0 -- act like deparsed True -- container_to_use phv5 -- fails False
  treat as deparsed? True
Required PHV group: Group 0 32 bits
Found new container in required group phv5
***Allocating phv5[31:0] for ethernet.dstAddr[31:0]
Looking at ethernet.srcAddr (ingress) [47:32], with test_alloc = True
----> ethernet.srcAddr (ingress) is allocated? False
  Group 11 16 bits -- deparsed True -- promised 7 -- ingress promised 6 and remain 4 and req 0 -- egress promised 1 and remain 4 and req 1 -- act like deparsed True -- container_to_use phv191 -- fails False
  treat as deparsed? True
Required PHV group: Group 11 16 bits
Found new container in required group phv191
***Allocating phv191[15:0] for ethernet.srcAddr[47:32]
Looking at ethernet.srcAddr (ingress) [31:0], with test_alloc = True
----> ethernet.srcAddr (ingress) is allocated? False
  Group 0 32 bits -- deparsed True -- promised 4 -- ingress promised 4 and remain 1 and req 2 -- egress promised 0 and remain 2 and req 0 -- act like deparsed True -- container_to_use phv8 -- fails False
  treat as deparsed? True
Required PHV group: Group 0 32 bits
Found new container in required group phv8
***Allocating phv8[31:0] for ethernet.srcAddr[31:0]
Looking at ethernet.etherType (ingress) [15:0], with test_alloc = True
----> ethernet.etherType (ingress) is allocated? False
  Group 11 16 bits -- deparsed True -- promised 6 -- ingress promised 5 and remain 3 and req 0 -- egress promised 1 and remain 4 and req 1 -- act like deparsed True -- container_to_use phv188 -- fails False
  treat as deparsed? True
Required PHV group: Group 11 16 bits
Found new container in required group phv188
***Allocating phv188[15:0] for ethernet.etherType[15:0]
Packing options tried: 47
Packing options skipped: 0
Failure Reasons:
  Field pack does not fit (case 1) -- tried 18 variants
  Inconsistent field pack (case 2) -- tried 28 variants

>> parse_ethernet (ingress) took 0.65 seconds
Working on parse node parse_ethernet (4) (egress)

-------------------------------------------
Allocating parsed header: pkt fields (3) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 112
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 112
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 112
Parse state 0 (112 bits)
  ethernet.dstAddr [47:0]
  ethernet.srcAddr [47:0]
  ethernet.etherType [15:0]
-----------------------------------------------------------------------------------------
|        Name        | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------
|  ethernet.dstAddr  | 48 |   False   |  -  |  -   |     -     |    6     |     3      |
|  ethernet.srcAddr  | 48 |   False   |  -  |  -   |     -     |    6     |     2      |
| ethernet.etherType | 16 |   False   |  -  |  -   |    [32]   |    2     |     5      |
-----------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 604
MAU containers available:
  8-bit: 21
  16-bit: 37
  32-bit: 16
Tagalong containers available:
  8-bit: 19
  16-bit: 30
  32-bit: 20
Initial packing options: 161

Packing option 0:  [8, 8, 32, 16, 32, 16]
MAU containers after:
  8-bit: 15
  16-bit: 30
  32-bit: 11
+-----------------------------+
|  ethernet.dstAddr [47:40]   |
+-----------------------------+
|  ethernet.dstAddr [39:32]   |
+-----------------------------+
|  ethernet.dstAddr [31:0]    |
+-----------------------------+
|  ethernet.srcAddr [47:32]   |
+-----------------------------+
|  ethernet.srcAddr [31:0]    |
+-----------------------------+
|  ethernet.etherType [15:0]  |
+-----------------------------+

Looking at ethernet.dstAddr (egress) [47:40], with test_alloc = True
----> ethernet.dstAddr (egress) is allocated? False

MAU groups: 2
  Group 6 8 bits -- avail 5 -- ingress avail 0 and remain 0 and promised 0 and req 0 -- egress avail 5 and remain 2 and promised 3 and req 2 -- as if deparsed True -- container_to_use phv99
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 13 and promised 3 and req 2 -- as if deparsed True -- container_to_use phv112
***Allocating phv99[7:0] for ethernet.dstAddr[47:40]
Looking at ethernet.dstAddr (egress) [39:32], with test_alloc = True
----> ethernet.dstAddr (egress) is allocated? False

MAU groups: 1
  Group 7 8 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 13 and promised 3 and req 2 -- as if deparsed True -- container_to_use phv112
***Allocating phv112[7:0] for ethernet.dstAddr[39:32]
Looking at ethernet.dstAddr (egress) [31:0], with test_alloc = True
----> ethernet.dstAddr (egress) is allocated? False

MAU groups: 1
  Group 3 32 bits -- avail 16 -- ingress avail 16 and remain 12 and promised 0 and req 0 -- egress avail 16 and remain 13 and promised 3 and req 2 -- as if deparsed True -- container_to_use phv48
***Allocating phv48[31:0] for ethernet.dstAddr[31:0]
Looking at ethernet.srcAddr (egress) [47:32], with test_alloc = True
----> ethernet.srcAddr (egress) is allocated? False

MAU groups: 3
  Group 11 16 bits -- avail 7 -- ingress avail 0 and remain 0 and promised 4 and req 2 -- egress avail 7 and remain 2 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv178
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208
***Allocating phv178[15:0] for ethernet.srcAddr[47:32]
Looking at ethernet.srcAddr (egress) [31:0], with test_alloc = True
----> ethernet.srcAddr (egress) is allocated? False

MAU groups: 1
  Group 3 32 bits -- avail 15 -- ingress avail 12 and remain 11 and promised 0 and req 0 -- egress avail 15 and remain 11 and promised 4 and req 3 -- as if deparsed True -- container_to_use phv50
***Allocating phv50[31:0] for ethernet.srcAddr[31:0]
Looking at ethernet.etherType (egress) [15:0], with test_alloc = True
----> ethernet.etherType (egress) is allocated? False

MAU groups: 2
  Group 12 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 11 and promised 5 and req 5 -- as if deparsed True -- container_to_use phv192
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 11 and promised 5 and req 5 -- as if deparsed True -- container_to_use phv208
***Allocating phv192[15:0] for ethernet.etherType[15:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_ethernet (egress) took 1.55 seconds
Working on parse node parse_inner_ethernet (41) (egress)

-------------------------------------------
Allocating parsed header: pkt fields (3) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 112
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 112
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 112
Parse state 0 (112 bits)
  inner_ethernet.dstAddr [47:0]
  inner_ethernet.srcAddr [47:0]
  inner_ethernet.etherType [15:0]
----------------------------------------------------------------------------------------------------------------------
|           Name           | BW | Tagalong? |            Req             | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------------------------
|  inner_ethernet.dstAddr  | 48 |   False   | [(8, 8), (8, 8), (32, 32)] |  -   |     -     |    6     |     3      |
|  inner_ethernet.srcAddr  | 48 |   False   |    [(16, 16), (32, 32)]    |  -   |     -     |    6     |     2      |
| inner_ethernet.etherType | 16 |   False   |         [(16, 16)]         |  -   |    [32]   |    2     |     5      |
----------------------------------------------------------------------------------------------------------------------

min_extracts[8] = 3
min_extracts[16] = 3
min_extracts[32] = 3
Packing options: 604
MAU containers available:
  8-bit: 15
  16-bit: 30
  32-bit: 11
Tagalong containers available:
  8-bit: 19
  16-bit: 30
  32-bit: 20
Initial packing options: 604

Packing option 46:  [8, 8, 32, 16, 32, 16]
MAU containers after:
  8-bit: 15
  16-bit: 30
  32-bit: 11
+-----------------------------------+
|  inner_ethernet.dstAddr [47:40]   |
+-----------------------------------+
|  inner_ethernet.dstAddr [39:32]   |
+-----------------------------------+
|  inner_ethernet.dstAddr [31:0]    |
+-----------------------------------+
|  inner_ethernet.srcAddr [47:32]   |
+-----------------------------------+
|  inner_ethernet.srcAddr [31:0]    |
+-----------------------------------+
|  inner_ethernet.etherType [15:0]  |
+-----------------------------------+

Looking at inner_ethernet.dstAddr (egress) [47:40], with test_alloc = True
----> inner_ethernet.dstAddr (egress) is allocated? False
  Group 6 8 bits -- deparsed True -- promised 2 -- ingress promised 0 and remain 0 and req 0 -- egress promised 2 and remain 2 and req 1 -- act like deparsed True -- container_to_use phv100 -- fails False
  treat as deparsed? True
Required PHV group: Group 6 8 bits
Found new container in required group phv100
***Allocating phv100[7:0] for inner_ethernet.dstAddr[47:40]
Looking at inner_ethernet.dstAddr (egress) [39:32], with test_alloc = True
----> inner_ethernet.dstAddr (egress) is allocated? False
  Group 7 8 bits -- deparsed True -- promised 2 -- ingress promised 0 and remain 0 and req 8 -- egress promised 2 and remain 13 and req 1 -- act like deparsed True -- container_to_use phv113 -- fails False
  treat as deparsed? True
Required PHV group: Group 7 8 bits
Found new container in required group phv113
***Allocating phv113[7:0] for inner_ethernet.dstAddr[39:32]
Looking at inner_ethernet.dstAddr (egress) [31:0], with test_alloc = True
----> inner_ethernet.dstAddr (egress) is allocated? False
  Group 3 32 bits -- deparsed True -- promised 3 -- ingress promised 0 and remain 0 and req 11 -- egress promised 3 and remain 11 and req 2 -- act like deparsed True -- container_to_use phv49 -- fails False
  treat as deparsed? True
Required PHV group: Group 3 32 bits
Found new container in required group phv49
***Allocating phv49[31:0] for inner_ethernet.dstAddr[31:0]
Looking at inner_ethernet.srcAddr (egress) [47:32], with test_alloc = True
----> inner_ethernet.srcAddr (egress) is allocated? False
  Group 11 16 bits -- deparsed True -- promised 6 -- ingress promised 4 and remain 2 and req 0 -- egress promised 2 and remain 2 and req 2 -- act like deparsed True -- container_to_use phv179 -- fails False
  treat as deparsed? True
Required PHV group: Group 11 16 bits
Found new container in required group phv179
***Allocating phv179[15:0] for inner_ethernet.srcAddr[47:32]
Looking at inner_ethernet.srcAddr (egress) [31:0], with test_alloc = True
----> inner_ethernet.srcAddr (egress) is allocated? False
  Group 3 32 bits -- deparsed True -- promised 2 -- ingress promised 0 and remain 0 and req 11 -- egress promised 2 and remain 11 and req 1 -- act like deparsed True -- container_to_use phv51 -- fails False
  treat as deparsed? True
Required PHV group: Group 3 32 bits
Found new container in required group phv51
***Allocating phv51[31:0] for inner_ethernet.srcAddr[31:0]
Looking at inner_ethernet.etherType (egress) [15:0], with test_alloc = True
----> inner_ethernet.etherType (egress) is allocated? False
  Group 12 16 bits -- deparsed True -- promised 4 -- ingress promised 0 and remain 0 and req 8 -- egress promised 4 and remain 11 and req 4 -- act like deparsed True -- container_to_use phv195 -- fails False
  treat as deparsed? True
Required PHV group: Group 12 16 bits
Found new container in required group phv195
***Allocating phv195[15:0] for inner_ethernet.etherType[15:0]
Packing options tried: 47
Packing options skipped: 0
Failure Reasons:
  Field pack does not fit (case 1) -- tried 18 variants
  Inconsistent field pack (case 2) -- tried 28 variants

>> parse_inner_ethernet (egress) took 0.80 seconds
Working on parse node parse_qinq (8) (ingress)

-------------------------------------------
Allocating parsed header: pkt fields (4) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  vlan_tag_[0].pcp [2:0]
  vlan_tag_[0].cfi [0:0]
  vlan_tag_[0].vid [11:0]
  vlan_tag_[0].etherType [15:0]
----------------------------------------------------------------------------------------------------
|          Name          | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------
|    vlan_tag_[0].pcp    | 3  |    True   |     -      |  -   |     -     |    1     |     1      |
|    vlan_tag_[0].cfi    | 1  |    True   |     -      |  -   |     -     |    1     |     1      |
|    vlan_tag_[0].vid    | 12 |   False   |     -      |  -   |     -     |    2     |     1      |
| vlan_tag_[0].etherType | 16 |   False   | [(16, 16)] |  -   |    [32]   |    2     |     5      |
----------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 2
min_extracts[32] = 1
Packing options: 6
MAU containers available:
  8-bit: 19
  16-bit: 24
  32-bit: 13
Tagalong containers available:
  8-bit: 22
  16-bit: 38
  32-bit: 18
Initial packing options: 6

Packing option 2:  [16, 16]
MAU containers after:
  8-bit: 19
  16-bit: 23
  32-bit: 13
+---------------------------------+
|  vlan_tag_[0].pcp [2:0]         |
|  vlan_tag_[0].cfi [0:0]         |
|  vlan_tag_[0].vid [11:0]        |
+---------------------------------+
|  vlan_tag_[0].etherType [15:0]  |
+---------------------------------+

Looking at vlan_tag_[0].pcp (ingress) [2:0], with test_alloc = True
----> vlan_tag_[0].pcp (ingress) is allocated? False
Looking at vlan_tag_[0].cfi (ingress) [0:0], with test_alloc = True
Looking at vlan_tag_[0].vid (ingress) [11:0], with test_alloc = True

MAU groups: 2
  Group 12 16 bits -- avail 8 -- ingress avail 8 and remain 7 and promised 1 and req 1 -- egress avail 8 and remain 3 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv200
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 1 -- egress avail 16 and remain 8 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv208
***Allocating phv200[15:13] for vlan_tag_[0].pcp[2:0]
***Allocating phv200[12:12] for vlan_tag_[0].cfi[0:0]
***Allocating phv200[11:0] for vlan_tag_[0].vid[11:0]
Looking at vlan_tag_[0].etherType (ingress) [15:0], with test_alloc = True
----> vlan_tag_[0].etherType (ingress) is allocated? False
  Group 11 16 bits -- deparsed True -- promised 5 -- ingress promised 4 and remain 2 and req 0 -- egress promised 1 and remain 2 and req 1 -- act like deparsed True -- container_to_use phv187 -- fails False
  treat as deparsed? True
Required PHV group: Group 11 16 bits
Found new container in required group phv187
***Allocating phv187[15:0] for vlan_tag_[0].etherType[15:0]
Packing options tried: 3
Packing options skipped: 0
Failure Reasons:
  Field pack does not fit (case 1) -- tried 1 variants
  Inconsistent field pack (case 2) -- tried 1 variants

>> parse_qinq (ingress) took 0.39 seconds
Working on parse node parse_qinq (8) (egress)

-------------------------------------------
Allocating parsed header: pkt fields (4) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  vlan_tag_[0].pcp [2:0]
  vlan_tag_[0].cfi [0:0]
  vlan_tag_[0].vid [11:0]
  vlan_tag_[0].etherType [15:0]
----------------------------------------------------------------------------------------------------
|          Name          | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------
|    vlan_tag_[0].pcp    | 3  |   False   |     -      |  -   |     -     |    1     |     1      |
|    vlan_tag_[0].cfi    | 1  |    True   |     -      |  -   |     -     |    1     |     1      |
|    vlan_tag_[0].vid    | 12 |   False   |     -      |  -   |     -     |    2     |     1      |
| vlan_tag_[0].etherType | 16 |   False   | [(16, 16)] |  -   |    [32]   |    2     |     5      |
----------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 2
min_extracts[32] = 1
Packing options: 6
MAU containers available:
  8-bit: 15
  16-bit: 22
  32-bit: 11
Tagalong containers available:
  8-bit: 19
  16-bit: 30
  32-bit: 20
Initial packing options: 4

Packing option 1:  [16, 16]
MAU containers after:
  8-bit: 15
  16-bit: 21
  32-bit: 11
+---------------------------------+
|  vlan_tag_[0].pcp [2:0]         |
|  vlan_tag_[0].cfi [0:0]         |
|  vlan_tag_[0].vid [11:0]        |
+---------------------------------+
|  vlan_tag_[0].etherType [15:0]  |
+---------------------------------+

Looking at vlan_tag_[0].pcp (egress) [2:0], with test_alloc = True
----> vlan_tag_[0].pcp (egress) is allocated? False
Looking at vlan_tag_[0].cfi (egress) [0:0], with test_alloc = True
Looking at vlan_tag_[0].vid (egress) [11:0], with test_alloc = True

MAU groups: 4
  Group 11 16 bits -- avail 5 -- ingress avail 0 and remain 0 and promised 3 and req 1 -- egress avail 5 and remain 1 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv180
  Group 10 16 bits -- avail 5 -- ingress avail 0 and remain 0 and promised 0 and req 0 -- egress avail 5 and remain 0 and promised 5 and req 3 -- as if deparsed True -- container_to_use phv173
  Group 12 16 bits -- avail 6 -- ingress avail 0 and remain 7 and promised 0 and req 0 -- egress avail 6 and remain 2 and promised 4 and req 4 -- as if deparsed True -- container_to_use phv197
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv208
***Allocating phv180[15:13] for vlan_tag_[0].pcp[2:0]
***Allocating phv180[12:12] for vlan_tag_[0].cfi[0:0]
***Allocating phv180[11:0] for vlan_tag_[0].vid[11:0]
Looking at vlan_tag_[0].etherType (egress) [15:0], with test_alloc = True
----> vlan_tag_[0].etherType (egress) is allocated? False
  Group 12 16 bits -- deparsed True -- promised 3 -- ingress promised 0 and remain 0 and req 7 -- egress promised 3 and remain 3 and req 3 -- act like deparsed True -- container_to_use phv194 -- fails False
  treat as deparsed? True
Required PHV group: Group 12 16 bits
Found new container in required group phv194
***Allocating phv194[15:0] for vlan_tag_[0].etherType[15:0]
Packing options tried: 2
Packing options skipped: 0
Failure Reasons:
  Field pack does not fit (case 1) -- tried 1 variants

>> parse_qinq (egress) took 0.41 seconds
Working on parse node parse_fabric_payload_header (46) (ingress)

-------------------------------------------
Allocating parsed header: pkt fields (1) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 16
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (16 bits)
  fabric_payload_header.etherType [15:0]
-------------------------------------------------------------------------------------------------------------
|               Name              | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------------------
| fabric_payload_header.etherType | 16 |   False   | [(16, 16)] |  -   |    [32]   |    2     |     5      |
-------------------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 2
min_extracts[32] = 1
Packing options: 2
MAU containers available:
  8-bit: 19
  16-bit: 23
  32-bit: 13
Tagalong containers available:
  8-bit: 22
  16-bit: 38
  32-bit: 18
Initial packing options: 2

Packing option 0:  [16]
MAU containers after:
  8-bit: 19
  16-bit: 23
  32-bit: 13
+------------------------------------------+
|  fabric_payload_header.etherType [15:0]  |
+------------------------------------------+

Looking at fabric_payload_header.etherType (ingress) [15:0], with test_alloc = True
----> fabric_payload_header.etherType (ingress) is allocated? False
  Group 11 16 bits -- deparsed True -- promised 4 -- ingress promised 3 and remain 1 and req 0 -- egress promised 1 and remain 1 and req 1 -- act like deparsed True -- container_to_use phv189 -- fails False
  treat as deparsed? True
Required PHV group: Group 11 16 bits
Found new container in required group phv189
***Allocating phv189[15:0] for fabric_payload_header.etherType[15:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_fabric_payload_header (ingress) took 0.27 seconds
Working on parse node parse_fabric_payload_header (46) (egress)

-------------------------------------------
Allocating parsed header: pkt fields (1) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 16
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (16 bits)
  fabric_payload_header.etherType [15:0]
-------------------------------------------------------------------------------------------------------------
|               Name              | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------------------
| fabric_payload_header.etherType | 16 |   False   | [(16, 16)] |  -   |    [32]   |    2     |     5      |
-------------------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 2
min_extracts[32] = 1
Packing options: 2
MAU containers available:
  8-bit: 15
  16-bit: 21
  32-bit: 11
Tagalong containers available:
  8-bit: 19
  16-bit: 30
  32-bit: 20
Initial packing options: 2

Packing option 0:  [16]
MAU containers after:
  8-bit: 15
  16-bit: 21
  32-bit: 11
+------------------------------------------+
|  fabric_payload_header.etherType [15:0]  |
+------------------------------------------+

Looking at fabric_payload_header.etherType (egress) [15:0], with test_alloc = True
----> fabric_payload_header.etherType (egress) is allocated? False
  Group 12 16 bits -- deparsed True -- promised 2 -- ingress promised 0 and remain 0 and req 7 -- egress promised 2 and remain 3 and req 2 -- act like deparsed True -- container_to_use phv193 -- fails False
  treat as deparsed? True
Required PHV group: Group 12 16 bits
Found new container in required group phv193
***Allocating phv193[15:0] for fabric_payload_header.etherType[15:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_fabric_payload_header (egress) took 0.29 seconds
Working on parse node parse_vxlan (34) (egress)

-------------------------------------------
Allocating parsed header: pkt fields (4) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 64
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 64
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 64
Parse state 0 (64 bits)
  vxlan.flags [7:0]
  vxlan.reserved [23:0]
  vxlan.vni [23:0]
  vxlan.reserved2 [7:0]
--------------------------------------------------------------------------------------
|       Name      | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------
|   vxlan.flags   | 8  |   False   |  -  |  -   |     -     |    1     |     1      |
|  vxlan.reserved | 24 |   False   |  -  |  -   |     -     |    3     |     1      |
|    vxlan.vni    | 24 |   False   |  -  |  -   |     -     |    3     |     4      |
| vxlan.reserved2 | 8  |   False   |  -  |  -   |     -     |    1     |     1      |
--------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 47
MAU containers available:
  8-bit: 15
  16-bit: 21
  32-bit: 11
Tagalong containers available:
  8-bit: 19
  16-bit: 30
  32-bit: 20
Initial packing options: 12

Packing option 0:  [32, 8, 16, 8]
MAU containers after:
  8-bit: 10
  16-bit: 17
  32-bit: 10
+--------------------------+
|  vxlan.flags [7:0]       |
|  vxlan.reserved [23:0]   |
+--------------------------+
|  vxlan.vni [23:16]       |
+--------------------------+
|  vxlan.vni [15:0]        |
+--------------------------+
|  vxlan.reserved2 [7:0]   |
+--------------------------+

Looking at vxlan.flags (egress) [7:0], with test_alloc = True
----> vxlan.flags (egress) is allocated? False
Looking at vxlan.reserved (egress) [23:0], with test_alloc = True

MAU groups: 2
  Group 1 32 bits -- avail 2 -- ingress avail 0 and remain 0 and promised 0 and req 0 -- egress avail 2 and remain 0 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv18
  Group 3 32 bits -- avail 12 -- ingress avail 12 and remain 8 and promised 0 and req 0 -- egress avail 12 and remain 10 and promised 2 and req 1 -- as if deparsed True -- container_to_use phv52
***Allocating phv18[31:24] for vxlan.flags[7:0]
***Allocating phv18[23:0] for vxlan.reserved[23:0]
Looking at vxlan.vni (egress) [23:16], with test_alloc = True
----> vxlan.vni (egress) is allocated? False

MAU groups: 2
  Group 6 8 bits -- avail 3 -- ingress avail 0 and remain 0 and promised 0 and req 0 -- egress avail 3 and remain 0 and promised 3 and req 1 -- as if deparsed True -- container_to_use phv101
  Group 7 8 bits -- avail 14 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 14 and remain 11 and promised 3 and req 1 -- as if deparsed True -- container_to_use phv114
***Allocating phv101[7:0] for vxlan.vni[23:16]
Looking at vxlan.vni (egress) [15:0], with test_alloc = True
----> vxlan.vni (egress) is allocated? False

MAU groups: 2
  Group 12 16 bits -- avail 4 -- ingress avail 0 and remain 7 and promised 0 and req 0 -- egress avail 4 and remain 1 and promised 3 and req 2 -- as if deparsed True -- container_to_use phv196
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 1 -- as if deparsed True -- container_to_use phv208
***Allocating phv196[15:0] for vxlan.vni[15:0]
Looking at vxlan.reserved2 (egress) [7:0], with test_alloc = True
----> vxlan.reserved2 (egress) is allocated? False

MAU groups: 1
  Group 7 8 bits -- avail 14 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 14 and remain 12 and promised 2 and req 1 -- as if deparsed True -- container_to_use phv114
***Allocating phv114[7:0] for vxlan.reserved2[7:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_vxlan (egress) took 2.49 seconds
Working on parse node parse_inner_tcp (38) (ingress)

-------------------------------------------
Allocating parsed header: pkt fields (10) / meta fields (3) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 160
Set metadata bits: 40
Gress: ingress
bits_will_need_to_parse = 200
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 200

 >>>> meta field l3_metadata.lkp_l4_sport (ingress) is allocated?  False

 >>>> meta field l3_metadata.lkp_l4_dport (ingress) is allocated?  False

 >>>> meta field l3_metadata.lkp_tcp_flags (ingress) is allocated?  False
Parse state 0 (200 bits)
  inner_tcp.srcPort [15:0]
  inner_tcp.dstPort [15:0]
  inner_tcp.seqNo [31:0]
  inner_tcp.ackNo [31:0]
  inner_tcp.dataOffset [3:0]
  inner_tcp.res [3:0]
  inner_tcp.flags [7:0]
  inner_tcp.window [15:0]
  inner_tcp.checksum [15:0]
  inner_tcp.urgentPtr [15:0]
  l3_metadata.lkp_l4_sport [15:0]
  l3_metadata.lkp_l4_dport [15:0]
  l3_metadata.lkp_tcp_flags [7:0]
------------------------------------------------------------------------------------------------
|            Name           | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------
|     inner_tcp.srcPort     | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
|     inner_tcp.dstPort     | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
|      inner_tcp.seqNo      | 32 |    True   |  -  |  -   |     -     |    4     |     1      |
|      inner_tcp.ackNo      | 32 |    True   |  -  |  -   |     -     |    4     |     1      |
|    inner_tcp.dataOffset   | 4  |    True   |  -  |  -   |     -     |    1     |     1      |
|       inner_tcp.res       | 4  |    True   |  -  |  -   |     -     |    1     |     1      |
|      inner_tcp.flags      | 8  |    True   |  -  |  -   |     -     |    1     |     1      |
|      inner_tcp.window     | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
|     inner_tcp.checksum    | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
|    inner_tcp.urgentPtr    | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
|  l3_metadata.lkp_l4_sport | 16 |   False   |  -  |  -   |     -     |    2     |     2      |
|  l3_metadata.lkp_l4_dport | 16 |   False   |  -  |  -   |     -     |    2     |     2      |
| l3_metadata.lkp_tcp_flags | 8  |   False   |  -  |  -   |     -     |    1     |     3      |
------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 5509
MAU containers available:
  8-bit: 19
  16-bit: 23
  32-bit: 13
Tagalong containers available:
  8-bit: 22
  16-bit: 38
  32-bit: 18
Initial packing options: 523

Packing option 315:  [8, 8, 16, 32, 32, 32, 32, 16, 16, 8]
MAU containers after:
  8-bit: 16
  16-bit: 19
  32-bit: 13
+------------------------------------+
|  inner_tcp.srcPort [15:8]          |
+------------------------------------+
|  inner_tcp.srcPort [7:0]           |
+------------------------------------+
|  inner_tcp.dstPort [15:0]          |
+------------------------------------+
|  inner_tcp.seqNo [31:0]            |
+------------------------------------+
|  inner_tcp.ackNo [31:0]            |
+------------------------------------+
|  inner_tcp.dataOffset [3:0]        |
|  inner_tcp.res [3:0]               |
|  inner_tcp.flags [7:0]             |
|  inner_tcp.window [15:0]           |
+------------------------------------+
|  inner_tcp.checksum [15:0]         |
|  inner_tcp.urgentPtr [15:0]        |
+------------------------------------+
|  l3_metadata.lkp_l4_sport [15:0]   |
+------------------------------------+
|  l3_metadata.lkp_l4_dport [15:0]   |
+------------------------------------+
|  l3_metadata.lkp_tcp_flags [7:0]   |
+------------------------------------+

Looking at inner_tcp.srcPort (ingress) [15:8], with test_alloc = True
----> inner_tcp.srcPort (ingress) is allocated? False
***Allocating phv298[7:0] for inner_tcp.srcPort[15:8]
Looking at inner_tcp.srcPort (ingress) [7:0], with test_alloc = True
----> inner_tcp.srcPort (ingress) is allocated? False
***Allocating phv299[7:0] for inner_tcp.srcPort[7:0]
Looking at inner_tcp.dstPort (ingress) [15:0], with test_alloc = True
----> inner_tcp.dstPort (ingress) is allocated? False
***Allocating phv330[15:0] for inner_tcp.dstPort[15:0]
Looking at inner_tcp.seqNo (ingress) [31:0], with test_alloc = True
----> inner_tcp.seqNo (ingress) is allocated? False
***Allocating phv270[31:0] for inner_tcp.seqNo[31:0]
Looking at inner_tcp.ackNo (ingress) [31:0], with test_alloc = True
----> inner_tcp.ackNo (ingress) is allocated? False
***Allocating phv271[31:0] for inner_tcp.ackNo[31:0]
Looking at inner_tcp.dataOffset (ingress) [3:0], with test_alloc = True
----> inner_tcp.dataOffset (ingress) is allocated? False
Looking at inner_tcp.res (ingress) [3:0], with test_alloc = True
Looking at inner_tcp.flags (ingress) [7:0], with test_alloc = True
Looking at inner_tcp.window (ingress) [15:0], with test_alloc = True
***Allocating phv272[31:28] for inner_tcp.dataOffset[3:0]
***Allocating phv272[27:24] for inner_tcp.res[3:0]
***Allocating phv272[23:16] for inner_tcp.flags[7:0]
***Allocating phv272[15:0] for inner_tcp.window[15:0]
Looking at inner_tcp.checksum (ingress) [15:0], with test_alloc = True
----> inner_tcp.checksum (ingress) is allocated? False
Looking at inner_tcp.urgentPtr (ingress) [15:0], with test_alloc = True
***Allocating phv273[31:16] for inner_tcp.checksum[15:0]
***Allocating phv273[15:0] for inner_tcp.urgentPtr[15:0]
Looking at l3_metadata.lkp_l4_sport (ingress) [15:0], with test_alloc = True
----> l3_metadata.lkp_l4_sport (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 8 16 bits -- deparsed False -- avail 9 and promised 10 -- ingress promised 11 and remain 3 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- deparsed False -- avail 5 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- deparsed False -- avail 5 and promised 6 -- ingress promised 2 and remain 0 and req 0 -- egress promised 4 and remain 0 and req 2 -- act like deparsed False -- container_to_use phv175 -- fails True
  Group 11 16 bits -- deparsed False -- avail 4 and promised 5 -- ingress promised 4 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 1 -- act like deparsed False -- container_to_use phv183 -- fails True
  Group 12 16 bits -- deparsed False -- avail 10 and promised 4 -- ingress promised 2 and remain 0 and req 5 -- egress promised 2 and remain 1 and req 1 -- act like deparsed False -- container_to_use phv201 -- fails False
Could not find container to overlay in.

MAU groups: 2
  Group 12 16 bits -- avail 10 -- ingress avail 7 and remain 5 and promised 2 and req 0 -- egress avail 3 and remain 1 and promised 2 and req 1 -- as if deparsed False -- container_to_use phv201
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 0 -- egress avail 16 and remain 14 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv208
***Allocating phv201[15:0] for l3_metadata.lkp_l4_sport[15:0]
Looking at l3_metadata.lkp_l4_dport (ingress) [15:0], with test_alloc = True
----> l3_metadata.lkp_l4_dport (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 8 16 bits -- deparsed False -- avail 9 and promised 10 -- ingress promised 11 and remain 3 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- deparsed False -- avail 5 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- deparsed False -- avail 5 and promised 6 -- ingress promised 2 and remain 0 and req 0 -- egress promised 4 and remain 0 and req 2 -- act like deparsed False -- container_to_use phv175 -- fails True
  Group 11 16 bits -- deparsed False -- avail 4 and promised 5 -- ingress promised 4 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 1 -- act like deparsed False -- container_to_use phv183 -- fails True
  Group 12 16 bits -- deparsed False -- avail 9 and promised 5 -- ingress promised 3 and remain 0 and req 3 -- egress promised 2 and remain 1 and req 1 -- act like deparsed False -- container_to_use phv203 -- fails False
Could not find container to overlay in.

MAU groups: 2
  Group 12 16 bits -- avail 9 -- ingress avail 6 and remain 3 and promised 3 and req 0 -- egress avail 3 and remain 1 and promised 2 and req 1 -- as if deparsed False -- container_to_use phv203
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 14 and promised 2 and req 0 -- egress avail 16 and remain 14 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv208
***Allocating phv203[15:0] for l3_metadata.lkp_l4_dport[15:0]
Looking at l3_metadata.lkp_tcp_flags (ingress) [7:0], with test_alloc = True
----> l3_metadata.lkp_tcp_flags (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 4 8 bits -- deparsed False -- avail 11 and promised 3 -- ingress promised 3 and remain 1 and req 8 -- egress promised 0 and remain 8 and req 0 -- act like deparsed False -- container_to_use phv70 -- fails False
  Group 5 8 bits -- deparsed False -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- deparsed False -- avail 2 and promised 3 -- ingress promised 3 and remain 1 and req 0 -- egress promised 2 and remain 0 and req 0 -- act like deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- deparsed False -- avail 13 and promised 4 -- ingress promised 3 and remain 1 and req 5 -- egress promised 1 and remain 9 and req 0 -- act like deparsed False -- container_to_use phv121 -- fails False
Could not find container to overlay in.

MAU groups: 2
  Group 7 8 bits -- avail 13 -- ingress avail 8 and remain 5 and promised 3 and req 1 -- egress avail 13 and remain 9 and promised 1 and req 0 -- as if deparsed False -- container_to_use phv121
  Group 4 8 bits -- avail 11 -- ingress avail 11 and remain 8 and promised 3 and req 1 -- egress avail 8 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv70
***Allocating phv121[7:0] for l3_metadata.lkp_tcp_flags[7:0]
Packing options tried: 316
Packing options skipped: 0
Failure Reasons:
  Field violates separation of metadata and headers (case 7a) -- tried 315 variants
    field: inner_tcp.checksum meta: l3_metadata.lkp_l4_sport
    field: inner_tcp.urgentPtr meta: l3_metadata.lkp_l4_sport

>> parse_inner_tcp (ingress) took 3.76 seconds
Working on parse node parse_fabric_timestamp_header (45) (egress)

-------------------------------------------
Allocating parsed header: pkt fields (2) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 48
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 48
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 48
Parse state 0 (48 bits)
  fabric_header_timestamp.arrival_time_hi [15:0]
  fabric_header_timestamp.arrival_time [31:0]
---------------------------------------------------------------------------------------------------------------------
|                   Name                  | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------------------------
| fabric_header_timestamp.arrival_time_hi | 16 |   False   | [(16, 16)] |  -   |     -     |    2     |     2      |
|   fabric_header_timestamp.arrival_time  | 32 |   False   | [(32, 32)] |  -   |     -     |    4     |     3      |
---------------------------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 2
min_extracts[32] = 2
Packing options: 17
MAU containers available:
  8-bit: 9
  16-bit: 19
  32-bit: 11
Tagalong containers available:
  8-bit: 15
  16-bit: 24
  32-bit: 16
Initial packing options: 17

Packing option 0:  [16, 32]
MAU containers after:
  8-bit: 9
  16-bit: 19
  32-bit: 11
+--------------------------------------------------+
|  fabric_header_timestamp.arrival_time_hi [15:0]  |
+--------------------------------------------------+
|  fabric_header_timestamp.arrival_time [31:0]     |
+--------------------------------------------------+

Looking at fabric_header_timestamp.arrival_time_hi (egress) [15:0], with test_alloc = True
----> fabric_header_timestamp.arrival_time_hi (egress) is allocated? False
  Group 9 16 bits -- deparsed True -- promised 5 -- ingress promised 0 and remain 0 and req 0 -- egress promised 5 and remain 0 and req 1 -- act like deparsed True -- container_to_use phv153 -- fails False
  treat as deparsed? True
Required PHV group: Group 9 16 bits
Found new container in required group phv153
***Allocating phv153[15:0] for fabric_header_timestamp.arrival_time_hi[15:0]
Looking at fabric_header_timestamp.arrival_time (egress) [31:0], with test_alloc = True
----> fabric_header_timestamp.arrival_time (egress) is allocated? False
  Group 1 32 bits -- deparsed True -- promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 1 -- act like deparsed True -- container_to_use phv17 -- fails False
  treat as deparsed? True
Required PHV group: Group 1 32 bits
Found new container in required group phv17
***Allocating phv17[31:0] for fabric_header_timestamp.arrival_time[31:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_fabric_timestamp_header (egress) took 0.27 seconds
Working on parse node parse_inner_tcp (38) (egress)

-------------------------------------------
Allocating parsed header: pkt fields (10) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 160
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 160
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 160
Parse state 0 (160 bits)
  inner_tcp.srcPort [15:0]
  inner_tcp.dstPort [15:0]
  inner_tcp.seqNo [31:0]
  inner_tcp.ackNo [31:0]
  inner_tcp.dataOffset [3:0]
  inner_tcp.res [3:0]
  inner_tcp.flags [7:0]
  inner_tcp.window [15:0]
  inner_tcp.checksum [15:0]
  inner_tcp.urgentPtr [15:0]
-------------------------------------------------------------------------------------------
|         Name         | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------
|  inner_tcp.srcPort   | 16 |   False   |  -  |  -   |     -     |    2     |     2      |
|  inner_tcp.dstPort   | 16 |   False   |  -  |  -   |     -     |    2     |     2      |
|   inner_tcp.seqNo    | 32 |   False   |  -  |  -   |     -     |    4     |     2      |
|   inner_tcp.ackNo    | 32 |   False   |  -  |  -   |     -     |    4     |     2      |
| inner_tcp.dataOffset | 4  |   False   |  -  |  -   |     -     |    1     |     2      |
|    inner_tcp.res     | 4  |   False   |  -  |  -   |     -     |    1     |     2      |
|   inner_tcp.flags    | 8  |   False   |  -  |  -   |     -     |    1     |     2      |
|   inner_tcp.window   | 16 |   False   |  -  |  -   |     -     |    2     |     2      |
|  inner_tcp.checksum  | 16 |   False   |  -  |  -   |     -     |    2     |     2      |
| inner_tcp.urgentPtr  | 16 |   False   |  -  |  -   |     -     |    2     |     2      |
-------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 5196
MAU containers available:
  8-bit: 9
  16-bit: 19
  32-bit: 11
Tagalong containers available:
  8-bit: 15
  16-bit: 24
  32-bit: 16
Initial packing options: 5196

Packing option 0:  [8, 8, 16, 16, 16, 32, 32, 32]
MAU containers after:
  8-bit: 5
  16-bit: 13
  32-bit: 5
+-------------------------------+
|  inner_tcp.srcPort [15:8]     |
+-------------------------------+
|  inner_tcp.srcPort [7:0]      |
+-------------------------------+
|  inner_tcp.dstPort [15:0]     |
+-------------------------------+
|  inner_tcp.seqNo [31:16]      |
+-------------------------------+
|  inner_tcp.seqNo [15:0]       |
+-------------------------------+
|  inner_tcp.ackNo [31:0]       |
+-------------------------------+
|  inner_tcp.dataOffset [3:0]   |
|  inner_tcp.res [3:0]          |
|  inner_tcp.flags [7:0]        |
|  inner_tcp.window [15:0]      |
+-------------------------------+
|  inner_tcp.checksum [15:0]    |
|  inner_tcp.urgentPtr [15:0]   |
+-------------------------------+

Looking at inner_tcp.srcPort (egress) [15:8], with test_alloc = True
----> inner_tcp.srcPort (egress) is allocated? False

MAU groups: 2
  Group 5 8 bits -- avail 2 -- ingress avail 0 and remain 0 and promised 0 and req 0 -- egress avail 2 and remain 0 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv84
  Group 7 8 bits -- avail 12 -- ingress avail 7 and remain 5 and promised 2 and req 1 -- egress avail 12 and remain 8 and promised 2 and req 1 -- as if deparsed True -- container_to_use phv115
***Allocating phv84[7:0] for inner_tcp.srcPort[15:8]
Looking at inner_tcp.srcPort (egress) [7:0], with test_alloc = True
----> inner_tcp.srcPort (egress) is allocated? False

MAU groups: 2
  Group 5 8 bits -- avail 1 -- ingress avail 0 and remain 0 and promised 0 and req 0 -- egress avail 1 and remain 0 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv87
  Group 7 8 bits -- avail 12 -- ingress avail 7 and remain 5 and promised 2 and req 1 -- egress avail 12 and remain 8 and promised 2 and req 1 -- as if deparsed True -- container_to_use phv115
***Allocating phv87[7:0] for inner_tcp.srcPort[7:0]
Looking at inner_tcp.dstPort (egress) [15:0], with test_alloc = True
----> inner_tcp.dstPort (egress) is allocated? False

MAU groups: 4
  Group 11 16 bits -- avail 4 -- ingress avail 0 and remain 0 and promised 2 and req 0 -- egress avail 4 and remain 1 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv177
  Group 12 16 bits -- avail 3 -- ingress avail 0 and remain 3 and promised 2 and req 0 -- egress avail 3 and remain 0 and promised 3 and req 2 -- as if deparsed True -- container_to_use phv198
  Group 10 16 bits -- avail 5 -- ingress avail 0 and remain 0 and promised 0 and req 0 -- egress avail 5 and remain 1 and promised 4 and req 2 -- as if deparsed True -- container_to_use phv163
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv208
***Allocating phv177[15:0] for inner_tcp.dstPort[15:0]
Looking at inner_tcp.seqNo (egress) [31:16], with test_alloc = True
----> inner_tcp.seqNo (egress) is allocated? False

MAU groups: 4
  Group 11 16 bits -- avail 3 -- ingress avail 0 and remain 0 and promised 2 and req 0 -- egress avail 3 and remain 0 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv181
  Group 12 16 bits -- avail 3 -- ingress avail 0 and remain 3 and promised 2 and req 0 -- egress avail 3 and remain 0 and promised 3 and req 2 -- as if deparsed True -- container_to_use phv198
  Group 10 16 bits -- avail 5 -- ingress avail 0 and remain 0 and promised 0 and req 0 -- egress avail 5 and remain 1 and promised 4 and req 2 -- as if deparsed True -- container_to_use phv163
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv208
***Allocating phv181[15:0] for inner_tcp.seqNo[31:16]
Looking at inner_tcp.seqNo (egress) [15:0], with test_alloc = True
----> inner_tcp.seqNo (egress) is allocated? False

MAU groups: 3
  Group 12 16 bits -- avail 3 -- ingress avail 0 and remain 3 and promised 2 and req 0 -- egress avail 3 and remain 0 and promised 3 and req 2 -- as if deparsed True -- container_to_use phv198
  Group 10 16 bits -- avail 5 -- ingress avail 0 and remain 0 and promised 0 and req 0 -- egress avail 5 and remain 1 and promised 4 and req 2 -- as if deparsed True -- container_to_use phv163
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv208
***Allocating phv198[15:0] for inner_tcp.seqNo[15:0]
Looking at inner_tcp.ackNo (egress) [31:0], with test_alloc = True
----> inner_tcp.ackNo (egress) is allocated? False

MAU groups: 1
  Group 3 32 bits -- avail 12 -- ingress avail 12 and remain 8 and promised 0 and req 0 -- egress avail 12 and remain 10 and promised 2 and req 1 -- as if deparsed True -- container_to_use phv52
***Allocating phv52[31:0] for inner_tcp.ackNo[31:0]
Looking at inner_tcp.dataOffset (egress) [3:0], with test_alloc = True
----> inner_tcp.dataOffset (egress) is allocated? False
Looking at inner_tcp.res (egress) [3:0], with test_alloc = True
Looking at inner_tcp.flags (egress) [7:0], with test_alloc = True
Looking at inner_tcp.window (egress) [15:0], with test_alloc = True

MAU groups: 1
  Group 3 32 bits -- avail 11 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 11 and remain 9 and promised 2 and req 1 -- as if deparsed True -- container_to_use phv53
***Allocating phv53[31:28] for inner_tcp.dataOffset[3:0]
***Allocating phv53[27:24] for inner_tcp.res[3:0]
***Allocating phv53[23:16] for inner_tcp.flags[7:0]
***Allocating phv53[15:0] for inner_tcp.window[15:0]
Looking at inner_tcp.checksum (egress) [15:0], with test_alloc = True
----> inner_tcp.checksum (egress) is allocated? False
Looking at inner_tcp.urgentPtr (egress) [15:0], with test_alloc = True

MAU groups: 1
  Group 3 32 bits -- avail 10 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 10 and remain 8 and promised 2 and req 1 -- as if deparsed True -- container_to_use phv54
***Allocating phv54[31:16] for inner_tcp.checksum[15:0]
***Allocating phv54[15:0] for inner_tcp.urgentPtr[15:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_inner_tcp (egress) took 3.02 seconds
Working on parse node parse_udp (24) (ingress)

-------------------------------------------
Allocating parsed header: pkt fields (4) / meta fields (2) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 64
Set metadata bits: 32
Gress: ingress
bits_will_need_to_parse = 96
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 96

 >>>> meta field l3_metadata.lkp_outer_l4_sport (ingress) is allocated?  False

 >>>> meta field l3_metadata.lkp_outer_l4_dport (ingress) is allocated?  False
Parse state 0 (96 bits)
  udp.srcPort [15:0]
  udp.dstPort [15:0]
  udp.length_ [15:0]
  udp.checksum [15:0]
  l3_metadata.lkp_outer_l4_sport [15:0]
  l3_metadata.lkp_outer_l4_dport [15:0]
------------------------------------------------------------------------------------------------------------
|              Name              | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------------------
|          udp.srcPort           | 16 |    True   |     -      |  -   |     -     |    2     |     1      |
|          udp.dstPort           | 16 |    True   |     -      |  -   |     -     |    2     |     1      |
|          udp.length_           | 16 |    True   |     -      |  -   |     -     |    2     |     1      |
|          udp.checksum          | 16 |    True   |     -      |  -   |     -     |    2     |     1      |
| l3_metadata.lkp_outer_l4_sport | 16 |   False   | [(16, 16)] |  -   |     -     |    2     |     2      |
| l3_metadata.lkp_outer_l4_dport | 16 |   False   | [(16, 16)] |  -   |     -     |    2     |     2      |
------------------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 3
min_extracts[32] = 1
Packing options: 292
MAU containers available:
  8-bit: 16
  16-bit: 19
  32-bit: 10
Tagalong containers available:
  8-bit: 20
  16-bit: 37
  32-bit: 14
Initial packing options: 166

Packing option 0:  [8, 8, 16, 32, 16, 16]
MAU containers after:
  8-bit: 16
  16-bit: 19
  32-bit: 10
+-----------------------------------------+
|  udp.srcPort [15:8]                     |
+-----------------------------------------+
|  udp.srcPort [7:0]                      |
+-----------------------------------------+
|  udp.dstPort [15:0]                     |
+-----------------------------------------+
|  udp.length_ [15:0]                     |
|  udp.checksum [15:0]                    |
+-----------------------------------------+
|  l3_metadata.lkp_outer_l4_sport [15:0]  |
+-----------------------------------------+
|  l3_metadata.lkp_outer_l4_dport [15:0]  |
+-----------------------------------------+

Looking at udp.srcPort (ingress) [15:8], with test_alloc = True
----> udp.srcPort (ingress) is allocated? False
***Allocating phv300[7:0] for udp.srcPort[15:8]
Looking at udp.srcPort (ingress) [7:0], with test_alloc = True
----> udp.srcPort (ingress) is allocated? False
***Allocating phv301[7:0] for udp.srcPort[7:0]
Looking at udp.dstPort (ingress) [15:0], with test_alloc = True
----> udp.dstPort (ingress) is allocated? False
***Allocating phv331[15:0] for udp.dstPort[15:0]
Looking at udp.length_ (ingress) [15:0], with test_alloc = True
----> udp.length_ (ingress) is allocated? False
Looking at udp.checksum (ingress) [15:0], with test_alloc = True
***Allocating phv274[31:16] for udp.length_[15:0]
***Allocating phv274[15:0] for udp.checksum[15:0]
Looking at l3_metadata.lkp_outer_l4_sport (ingress) [15:0], with test_alloc = True
----> l3_metadata.lkp_outer_l4_sport (ingress) is allocated? False
Checking if can overlay metadata field.
Required PHV group: Group 12 16 bits
  Group 12 16 bits -- deparsed False -- avail 7 and promised 4 -- ingress promised 2 and remain 0 and req 3 -- egress promised 2 and remain 0 and req 1 -- act like deparsed False -- container_to_use phv202 -- fails False
Could not find container to overlay in.
  Group 12 16 bits -- deparsed False -- promised 4 -- ingress promised 2 and remain 0 and req 3 -- egress promised 2 and remain 0 and req 1 -- act like deparsed False -- container_to_use phv202 -- fails False
  treat as deparsed? False
Required PHV group: Group 12 16 bits
Found new container in required group phv202
***Allocating phv202[15:0] for l3_metadata.lkp_outer_l4_sport[15:0]
Looking at l3_metadata.lkp_outer_l4_dport (ingress) [15:0], with test_alloc = True
----> l3_metadata.lkp_outer_l4_dport (ingress) is allocated? False
Checking if can overlay metadata field.
Required PHV group: Group 12 16 bits
  Group 12 16 bits -- deparsed False -- avail 6 and promised 3 -- ingress promised 1 and remain 0 and req 3 -- egress promised 2 and remain 0 and req 1 -- act like deparsed False -- container_to_use phv204 -- fails False
Could not find container to overlay in.
  Group 12 16 bits -- deparsed False -- promised 3 -- ingress promised 1 and remain 0 and req 3 -- egress promised 2 and remain 0 and req 1 -- act like deparsed False -- container_to_use phv204 -- fails False
  treat as deparsed? False
Required PHV group: Group 12 16 bits
Found new container in required group phv204
***Allocating phv204[15:0] for l3_metadata.lkp_outer_l4_dport[15:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_udp (ingress) took 0.66 seconds
Working on parse node parse_fabric_header_cpu (44) (ingress)

-------------------------------------------
Allocating parsed header: pkt fields (8) / meta fields (1) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 72
Set metadata bits: 8
Gress: ingress
bits_will_need_to_parse = 80
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 80

 >>>> meta field ingress_metadata.bypass_lookups (ingress) is allocated?  False
Parse state 0 (80 bits)
  fabric_header_cpu.egressQueue [4:0]
  fabric_header_cpu.txBypass [0:0]
  fabric_header_cpu.capture_tstamp_on_tx [0:0]
  fabric_header_cpu.reserved [0:0]
  fabric_header_cpu.ingressPort [15:0]
  fabric_header_cpu.ingressIfindex [15:0]
  fabric_header_cpu.ingressBd [15:0]
  fabric_header_cpu.reasonCode [15:0]
  ingress_metadata.bypass_lookups [7:0]
-------------------------------------------------------------------------------------------------------------------
|                  Name                  | BW | Tagalong? |    Req    | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------------------------
|     fabric_header_cpu.egressQueue      | 5  |    True   |     -     |  -   |     -     |    1     |     1      |
|       fabric_header_cpu.txBypass       | 1  |   False   | [(16, 1)] |  -   |     -     |    1     |     2      |
| fabric_header_cpu.capture_tstamp_on_tx | 1  |   False   | [(16, 1)] |  -   |     -     |    1     |     2      |
|       fabric_header_cpu.reserved       | 1  |    True   |     -     |  -   |     -     |    1     |     1      |
|     fabric_header_cpu.ingressPort      | 16 |    True   |     -     |  -   |     -     |    2     |     1      |
|    fabric_header_cpu.ingressIfindex    | 16 |    True   |     -     |  -   |     -     |    2     |     1      |
|      fabric_header_cpu.ingressBd       | 16 |   False   |     -     |  -   |     -     |    2     |     1      |
|      fabric_header_cpu.reasonCode      | 16 |    True   |     -     |  -   |     -     |    2     |     1      |
|    ingress_metadata.bypass_lookups     | 8  |   False   |     -     |  -   |     -     |    1     |     1      |
-------------------------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 3
min_extracts[32] = 1
Packing options: 123
MAU containers available:
  8-bit: 16
  16-bit: 19
  32-bit: 10
Tagalong containers available:
  8-bit: 18
  16-bit: 36
  32-bit: 13
Initial packing options: 123

Packing option 7:  [16, 8, 16, 32, 8]
MAU containers after:
  8-bit: 15
  16-bit: 18
  32-bit: 9
+-------------------------------------------------+
|  fabric_header_cpu.egressQueue [4:0]            |
|  fabric_header_cpu.txBypass [0:0]               |
|  fabric_header_cpu.capture_tstamp_on_tx [0:0]   |
|  fabric_header_cpu.reserved [0:0]               |
|  fabric_header_cpu.ingressPort [15:8]           |
+-------------------------------------------------+
|  fabric_header_cpu.ingressPort [7:0]            |
+-------------------------------------------------+
|  fabric_header_cpu.ingressIfindex [15:0]        |
+-------------------------------------------------+
|  fabric_header_cpu.ingressBd [15:0]             |
|  fabric_header_cpu.reasonCode [15:0]            |
+-------------------------------------------------+
|  ingress_metadata.bypass_lookups [7:0]          |
+-------------------------------------------------+

Looking at fabric_header_cpu.egressQueue (ingress) [4:0], with test_alloc = True
----> fabric_header_cpu.egressQueue (ingress) is allocated? False
Looking at fabric_header_cpu.txBypass (ingress) [0:0], with test_alloc = True
Looking at fabric_header_cpu.capture_tstamp_on_tx (ingress) [0:0], with test_alloc = True
Looking at fabric_header_cpu.reserved (ingress) [0:0], with test_alloc = True
Looking at fabric_header_cpu.ingressPort (ingress) [15:8], with test_alloc = True
  Group 8 16 bits -- deparsed True -- promised 9 -- ingress promised 9 and remain 3 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed True -- container_to_use phv133 -- fails False
  treat as deparsed? True
Required PHV group: Group 8 16 bits
Found new container in required group phv133
***Allocating phv133[15:11] for fabric_header_cpu.egressQueue[4:0]
***Allocating phv133[10:10] for fabric_header_cpu.txBypass[0:0]
***Allocating phv133[9:9] for fabric_header_cpu.capture_tstamp_on_tx[0:0]
***Allocating phv133[8:8] for fabric_header_cpu.reserved[0:0]
***Allocating phv133[7:0] for fabric_header_cpu.ingressPort[15:8]
Looking at fabric_header_cpu.ingressPort (ingress) [7:0], with test_alloc = True
----> fabric_header_cpu.ingressPort (ingress) is allocated? False
***Allocating phv302[7:0] for fabric_header_cpu.ingressPort[7:0]
Looking at fabric_header_cpu.ingressIfindex (ingress) [15:0], with test_alloc = True
----> fabric_header_cpu.ingressIfindex (ingress) is allocated? False
***Allocating phv332[15:0] for fabric_header_cpu.ingressIfindex[15:0]
Looking at fabric_header_cpu.ingressBd (ingress) [15:0], with test_alloc = True
----> fabric_header_cpu.ingressBd (ingress) is allocated? False
Looking at fabric_header_cpu.reasonCode (ingress) [15:0], with test_alloc = True

MAU groups: 2
  Group 0 32 bits -- avail 5 -- ingress avail 5 and remain 1 and promised 4 and req 1 -- egress avail 4 and remain 1 and promised 0 and req 0 -- as if deparsed True -- container_to_use phv10
  Group 3 32 bits -- avail 8 -- ingress avail 8 and remain 7 and promised 1 and req 1 -- egress avail 8 and remain 4 and promised 1 and req 0 -- as if deparsed True -- container_to_use phv56
***Allocating phv10[31:16] for fabric_header_cpu.ingressBd[15:0]
***Allocating phv10[15:0] for fabric_header_cpu.reasonCode[15:0]
Looking at ingress_metadata.bypass_lookups (ingress) [7:0], with test_alloc = True
----> ingress_metadata.bypass_lookups (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 4 8 bits -- deparsed False -- avail 11 and promised 1 -- ingress promised 1 and remain 0 and req 10 -- egress promised 0 and remain 8 and req 0 -- act like deparsed False -- container_to_use phv69 -- fails False
  Group 5 8 bits -- deparsed False -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- deparsed False -- avail 2 and promised 3 -- ingress promised 1 and remain 0 and req 0 -- egress promised 2 and remain 0 and req 0 -- act like deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- deparsed False -- avail 12 and promised 4 -- ingress promised 3 and remain 1 and req 4 -- egress promised 1 and remain 8 and req 0 -- act like deparsed False -- container_to_use phv123 -- fails False
Could not find container to overlay in.

MAU groups: 2
  Group 7 8 bits -- avail 12 -- ingress avail 7 and remain 4 and promised 3 and req 1 -- egress avail 12 and remain 8 and promised 1 and req 0 -- as if deparsed False -- container_to_use phv123
  Group 4 8 bits -- avail 11 -- ingress avail 11 and remain 10 and promised 1 and req 0 -- egress avail 8 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv69
***Allocating phv123[7:0] for ingress_metadata.bypass_lookups[7:0]
Packing options tried: 8
Packing options skipped: 0
Failure Reasons:
  Field violates separation of metadata and headers (case 7a) -- tried 1 variants
    field: fabric_header_cpu.ingressBd meta: ingress_metadata.bypass_lookups
  Inconsistent field pack (case 2) -- tried 6 variants

>> parse_fabric_header_cpu (ingress) took 0.64 seconds
Working on parse node ingress_intrinsic_metadata (49) (ingress)

-------------------------------------------
Allocating parsed header: pkt fields (6) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 64
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 64
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 64
Parse state 0 (64 bits)
  ig_intr_md.resubmit_flag [0:0]
  ig_intr_md._pad1 [0:0]
  ig_intr_md._pad2 [1:0]
  ig_intr_md._pad3 [2:0]
  ig_intr_md.ingress_port [8:0]
  ig_intr_md.ingress_mac_tstamp [47:0]
-----------------------------------------------------------------------------------------------------------
|              Name             | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------------
|    ig_intr_md.resubmit_flag   | 1  |   False   |     -      |  -   |     -     |    1     |     1      |
|        ig_intr_md._pad1       | 1  |   False   |     -      |  -   |     -     |    1     |     1      |
|        ig_intr_md._pad2       | 2  |   False   |     -      |  -   |     -     |    1     |     1      |
|        ig_intr_md._pad3       | 3  |   False   |     -      |  -   |     -     |    1     |     1      |
|    ig_intr_md.ingress_port    | 9  |   False   | [(16, 9)]  |  -   |     -     |    2     |     2      |
| ig_intr_md.ingress_mac_tstamp | 48 |   False   | [(32, 32)] |  -   |     -     |    6     |     2      |
-----------------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 2
min_extracts[32] = 2
Packing options: 47
MAU containers available:
  8-bit: 15
  16-bit: 19
  32-bit: 9
Tagalong containers available:
  8-bit: 17
  16-bit: 35
  32-bit: 13
Initial packing options: 47

Packing option 20:  [16, 16, 32]
MAU containers after:
  8-bit: 15
  16-bit: 16
  32-bit: 9
+----------------------------------------+
|  ig_intr_md.resubmit_flag [0:0]        |
|  ig_intr_md._pad1 [0:0]                |
|  ig_intr_md._pad2 [1:0]                |
|  ig_intr_md._pad3 [2:0]                |
|  ig_intr_md.ingress_port [8:0]         |
+----------------------------------------+
|  ig_intr_md.ingress_mac_tstamp [47:32] |
+----------------------------------------+
|  ig_intr_md.ingress_mac_tstamp [31:0]  |
+----------------------------------------+

Looking at ig_intr_md.resubmit_flag (ingress) [0:0], with test_alloc = True
----> ig_intr_md.resubmit_flag (ingress) is allocated? False
Looking at ig_intr_md._pad1 (ingress) [0:0], with test_alloc = True
Looking at ig_intr_md._pad2 (ingress) [1:0], with test_alloc = True
Looking at ig_intr_md._pad3 (ingress) [2:0], with test_alloc = True
Looking at ig_intr_md.ingress_port (ingress) [8:0], with test_alloc = True
Checking if can overlay metadata field.
Required PHV group: Group 8 16 bits
  Group 8 16 bits -- deparsed False -- avail 8 and promised 8 -- ingress promised 8 and remain 2 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed False -- container_to_use phv141 -- fails False
Could not find container to overlay in.
  Group 8 16 bits -- deparsed False -- promised 8 -- ingress promised 8 and remain 2 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed False -- container_to_use phv141 -- fails False
  treat as deparsed? False
Required PHV group: Group 8 16 bits
Found new container in required group phv141
***Allocating phv141[15:15] for ig_intr_md.resubmit_flag[0:0]
***Allocating phv141[14:14] for ig_intr_md._pad1[0:0]
***Allocating phv141[13:12] for ig_intr_md._pad2[1:0]
***Allocating phv141[11:9] for ig_intr_md._pad3[2:0]
***Allocating phv141[8:0] for ig_intr_md.ingress_port[8:0]
Looking at ig_intr_md.ingress_mac_tstamp (ingress) [47:32], with test_alloc = True
----> ig_intr_md.ingress_mac_tstamp (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 8 16 bits -- deparsed False -- avail 7 and promised 7 -- ingress promised 7 and remain 2 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed True -- container_to_use phv130 -- fails False
  Group 9 16 bits -- deparsed False -- avail 4 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- deparsed False -- avail 5 and promised 5 -- ingress promised 1 and remain 0 and req 0 -- egress promised 4 and remain 0 and req 2 -- act like deparsed False -- container_to_use phv175 -- fails False
  Group 11 16 bits -- deparsed False -- avail 2 and promised 3 -- ingress promised 3 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- deparsed False -- avail 5 and promised 3 -- ingress promised 1 and remain 0 and req 2 -- egress promised 2 and remain 0 and req 1 -- act like deparsed False -- container_to_use phv205 -- fails False
Found overlay container to use None
***Allocating phv130[15:0] for ig_intr_md.ingress_mac_tstamp[47:32]
Looking at ig_intr_md.ingress_mac_tstamp (ingress) [31:0], with test_alloc = True
----> ig_intr_md.ingress_mac_tstamp (ingress) is allocated? False
Checking if can overlay metadata field.
Required PHV group: Group 0 32 bits
  Group 0 32 bits -- deparsed False -- avail 4 and promised 3 -- ingress promised 3 and remain 0 and req 1 -- egress promised 0 and remain 1 and req 0 -- act like deparsed True -- container_to_use phv1 -- fails False
Found overlay container to use None
***Allocating phv1[31:0] for ig_intr_md.ingress_mac_tstamp[31:0]
Packing options tried: 21
Packing options skipped: 0
Failure Reasons:
  Field pack does not fit (case 1) -- tried 14 variants
  Inconsistent field pack (case 2) -- tried 6 variants

>> ingress_intrinsic_metadata (ingress) took 13.94 seconds
Working on parse node parse_fabric_header (43) (ingress)

-------------------------------------------
Allocating parsed header: pkt fields (8) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 40
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 40
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 40
Parse state 0 (40 bits)
  fabric_header.packetType [2:0]
  fabric_header.headerVersion [1:0]
  fabric_header.packetVersion [1:0]
  fabric_header.pad1 [0:0]
  fabric_header.fabricColor [2:0]
  fabric_header.fabricQos [4:0]
  fabric_header.dstDevice [7:0]
  fabric_header.dstPortOrGroup [15:0]
---------------------------------------------------------------------------------------------------
|             Name             | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------
|   fabric_header.packetType   | 3  |    True   |  -  |  -   |     -     |    1     |     1      |
| fabric_header.headerVersion  | 2  |    True   |  -  |  -   |     -     |    1     |     1      |
| fabric_header.packetVersion  | 2  |    True   |  -  |  -   |     -     |    1     |     1      |
|      fabric_header.pad1      | 1  |    True   |  -  |  -   |     -     |    1     |     1      |
|  fabric_header.fabricColor   | 3  |    True   |  -  |  -   |     -     |    1     |     1      |
|   fabric_header.fabricQos    | 5  |    True   |  -  |  -   |     -     |    1     |     1      |
|   fabric_header.dstDevice    | 8  |   False   |  -  |  -   |     -     |    1     |     1      |
| fabric_header.dstPortOrGroup | 16 |   False   |  -  |  -   |    [8]    |    2     |     2      |
---------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 9
MAU containers available:
  8-bit: 15
  16-bit: 19
  32-bit: 9
Tagalong containers available:
  8-bit: 17
  16-bit: 35
  32-bit: 13
Initial packing options: 9

Packing option 0:  [8, 32]
MAU containers after:
  8-bit: 15
  16-bit: 19
  32-bit: 7
+---------------------------------------+
|  fabric_header.packetType [2:0]       |
|  fabric_header.headerVersion [1:0]    |
|  fabric_header.packetVersion [1:0]    |
|  fabric_header.pad1 [0:0]             |
+---------------------------------------+
|  fabric_header.fabricColor [2:0]      |
|  fabric_header.fabricQos [4:0]        |
|  fabric_header.dstDevice [7:0]        |
|  fabric_header.dstPortOrGroup [15:0]  |
+---------------------------------------+

Looking at fabric_header.packetType (ingress) [2:0], with test_alloc = True
----> fabric_header.packetType (ingress) is allocated? False
Looking at fabric_header.headerVersion (ingress) [1:0], with test_alloc = True
Looking at fabric_header.packetVersion (ingress) [1:0], with test_alloc = True
Looking at fabric_header.pad1 (ingress) [0:0], with test_alloc = True
***Allocating phv303[7:5] for fabric_header.packetType[2:0]
***Allocating phv303[4:3] for fabric_header.headerVersion[1:0]
***Allocating phv303[2:1] for fabric_header.packetVersion[1:0]
***Allocating phv303[0:0] for fabric_header.pad1[0:0]
Looking at fabric_header.fabricColor (ingress) [2:0], with test_alloc = True
----> fabric_header.fabricColor (ingress) is allocated? False
Looking at fabric_header.fabricQos (ingress) [4:0], with test_alloc = True
Looking at fabric_header.dstDevice (ingress) [7:0], with test_alloc = True
Looking at fabric_header.dstPortOrGroup (ingress) [15:0], with test_alloc = True

MAU groups: 1
  Group 3 32 bits -- avail 8 -- ingress avail 8 and remain 6 and promised 2 and req 2 -- egress avail 8 and remain 4 and promised 1 and req 0 -- as if deparsed True -- container_to_use phv56
***Allocating phv56[31:29] for fabric_header.fabricColor[2:0]
***Allocating phv56[28:24] for fabric_header.fabricQos[4:0]
***Allocating phv56[23:16] for fabric_header.dstDevice[7:0]
***Allocating phv56[15:0] for fabric_header.dstPortOrGroup[15:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_fabric_header (ingress) took 0.40 seconds
Working on parse node parse_vxlan (34) (ingress)

-------------------------------------------
Allocating parsed header: pkt fields (4) / meta fields (2) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 64
Set metadata bits: 29
Gress: ingress
bits_will_need_to_parse = 96
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 96

 >>>> meta field tunnel_metadata.ingress_tunnel_type (ingress) is allocated?  True

 >>>> meta field tunnel_metadata.tunnel_vni (ingress) is allocated?  False
Parse state 0 (96 bits)
  vxlan.flags [7:0]
  vxlan.reserved [23:0]
  vxlan.vni [23:0]
  vxlan.reserved2 [7:0]
  tunnel_metadata.tunnel_vni [23:0]
  -pad-7- [2:0]
  tunnel_metadata.ingress_tunnel_type [4:0]
---------------------------------------------------------------------------------------------------------------
|                 Name                | BW | Tagalong? |   Req    | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------------------
|             vxlan.flags             | 8  |    True   |    -     |  -   |     -     |    1     |     1      |
|            vxlan.reserved           | 24 |    True   |    -     |  -   |     -     |    3     |     1      |
|              vxlan.vni              | 24 |    True   |    -     |  -   |     -     |    3     |     1      |
|           vxlan.reserved2           | 8  |    True   |    -     |  -   |     -     |    1     |     1      |
|      tunnel_metadata.tunnel_vni     | 24 |   False   |    -     |  -   |     -     |    3     |     1      |
|               -pad-7-               | 3  |    True   |    -     |  -   |     -     |   None   |     1      |
| tunnel_metadata.ingress_tunnel_type | 5  |   False   | [(8, 5)] |  -   |     -     |    1     |     1      |
---------------------------------------------------------------------------------------------------------------

min_extracts[8] = 2
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 292
MAU containers available:
  8-bit: 15
  16-bit: 19
  32-bit: 7
Tagalong containers available:
  8-bit: 16
  16-bit: 35
  32-bit: 13
Initial packing options: 137

Packing option 19:  [8, 8, 16, 32, 8, 16, 8]
MAU containers after:
  8-bit: 14
  16-bit: 18
  32-bit: 7
+----------------------------------------------+
|  vxlan.flags [7:0]                           |
+----------------------------------------------+
|  vxlan.reserved [23:16]                      |
+----------------------------------------------+
|  vxlan.reserved [15:0]                       |
+----------------------------------------------+
|  vxlan.vni [23:0]                            |
|  vxlan.reserved2 [7:0]                       |
+----------------------------------------------+
|  tunnel_metadata.tunnel_vni [23:16]          |
+----------------------------------------------+
|  tunnel_metadata.tunnel_vni [15:0]           |
+----------------------------------------------+
|  -pad-7- [2:0]                               |
|  tunnel_metadata.ingress_tunnel_type [4:0]   |
+----------------------------------------------+

Looking at vxlan.flags (ingress) [7:0], with test_alloc = True
----> vxlan.flags (ingress) is allocated? False
***Allocating phv304[7:0] for vxlan.flags[7:0]
Looking at vxlan.reserved (ingress) [23:16], with test_alloc = True
----> vxlan.reserved (ingress) is allocated? False
***Allocating phv305[7:0] for vxlan.reserved[23:16]
Looking at vxlan.reserved (ingress) [15:0], with test_alloc = True
----> vxlan.reserved (ingress) is allocated? False
***Allocating phv333[15:0] for vxlan.reserved[15:0]
Looking at vxlan.vni (ingress) [23:0], with test_alloc = True
----> vxlan.vni (ingress) is allocated? False
Looking at vxlan.reserved2 (ingress) [7:0], with test_alloc = True
***Allocating phv275[31:8] for vxlan.vni[23:0]
***Allocating phv275[7:0] for vxlan.reserved2[7:0]
Looking at tunnel_metadata.tunnel_vni (ingress) [23:16], with test_alloc = True
----> tunnel_metadata.tunnel_vni (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 4 8 bits -- deparsed False -- avail 11 and promised 1 -- ingress promised 1 and remain 0 and req 10 -- egress promised 0 and remain 8 and req 0 -- act like deparsed False -- container_to_use phv69 -- fails False
  Group 5 8 bits -- deparsed False -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- deparsed False -- avail 2 and promised 3 -- ingress promised 1 and remain 0 and req 0 -- egress promised 2 and remain 0 and req 0 -- act like deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- deparsed False -- avail 11 and promised 4 -- ingress promised 3 and remain 1 and req 3 -- egress promised 1 and remain 7 and req 0 -- act like deparsed False -- container_to_use phv124 -- fails False
Could not find container to overlay in.

MAU groups: 2
  Group 7 8 bits -- avail 11 -- ingress avail 6 and remain 3 and promised 3 and req 1 -- egress avail 11 and remain 7 and promised 1 and req 0 -- as if deparsed False -- container_to_use phv124
  Group 4 8 bits -- avail 11 -- ingress avail 11 and remain 10 and promised 1 and req 0 -- egress avail 8 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv69
***Allocating phv124[7:0] for tunnel_metadata.tunnel_vni[23:16]
Looking at tunnel_metadata.tunnel_vni (ingress) [15:0], with test_alloc = True
----> tunnel_metadata.tunnel_vni (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 8 16 bits -- deparsed False -- avail 7 and promised 8 -- ingress promised 8 and remain 2 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- deparsed False -- avail 4 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- deparsed False -- avail 5 and promised 5 -- ingress promised 1 and remain 0 and req 0 -- egress promised 4 and remain 0 and req 2 -- act like deparsed False -- container_to_use phv175 -- fails False
  Group 11 16 bits -- deparsed False -- avail 2 and promised 3 -- ingress promised 3 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- deparsed False -- avail 5 and promised 3 -- ingress promised 1 and remain 0 and req 2 -- egress promised 2 and remain 0 and req 1 -- act like deparsed False -- container_to_use phv205 -- fails False
Could not find container to overlay in.

MAU groups: 3
  Group 10 16 bits -- avail 5 -- ingress avail 0 and remain 0 and promised 1 and req 0 -- egress avail 5 and remain 0 and promised 4 and req 2 -- as if deparsed False -- container_to_use phv175
  Group 12 16 bits -- avail 5 -- ingress avail 3 and remain 2 and promised 1 and req 0 -- egress avail 2 and remain 0 and promised 2 and req 1 -- as if deparsed False -- container_to_use phv205
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 15 and promised 1 and req 0 -- egress avail 16 and remain 15 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv208
***Allocating phv175[15:0] for tunnel_metadata.tunnel_vni[15:0]
Looking at -pad-7- (ingress) [2:0], with test_alloc = False
Looking at tunnel_metadata.ingress_tunnel_type (ingress) [4:0], with test_alloc = True
----> tunnel_metadata.ingress_tunnel_type (ingress) is allocated? True
Fields for container 8 at index 6 already allocated.  No need to overlay or allocate new.
  -pad-7-[2:0]
  tunnel_metadata.ingress_tunnel_type[4:0]
Packing options tried: 20
Packing options skipped: 0
Failure Reasons:
  Field violates separation of metadata and headers (case 7a) -- tried 12 variants
    field: vxlan.reserved2 meta: tunnel_metadata.tunnel_vni
  Field violates max container sum (case 60) -- tried 7 variants
    field: tunnel_metadata.tunnel_vni violations: pack_size 48 > 32

>> parse_vxlan (ingress) took 2.07 seconds
Working on parse node parse_fabric_timestamp_header (45) (ingress)

-------------------------------------------
Allocating parsed header: pkt fields (2) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 48
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 48
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 48
Parse state 0 (48 bits)
  fabric_header_timestamp.arrival_time_hi [15:0]
  fabric_header_timestamp.arrival_time [31:0]
--------------------------------------------------------------------------------------------------------------
|                   Name                  | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------------------------
| fabric_header_timestamp.arrival_time_hi | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
|   fabric_header_timestamp.arrival_time  | 32 |    True   |  -  |  -   |     -     |    4     |     1      |
--------------------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 17
MAU containers available:
  8-bit: 14
  16-bit: 19
  32-bit: 7
Tagalong containers available:
  8-bit: 14
  16-bit: 34
  32-bit: 12
Initial packing options: 17

Packing option 0:  [8, 8, 16, 16]
MAU containers after:
  8-bit: 14
  16-bit: 19
  32-bit: 7
+--------------------------------------------------+
|  fabric_header_timestamp.arrival_time_hi [15:8]  |
+--------------------------------------------------+
|  fabric_header_timestamp.arrival_time_hi [7:0]   |
+--------------------------------------------------+
|  fabric_header_timestamp.arrival_time [31:16]    |
+--------------------------------------------------+
|  fabric_header_timestamp.arrival_time [15:0]     |
+--------------------------------------------------+

Looking at fabric_header_timestamp.arrival_time_hi (ingress) [15:8], with test_alloc = True
----> fabric_header_timestamp.arrival_time_hi (ingress) is allocated? False
***Allocating phv306[7:0] for fabric_header_timestamp.arrival_time_hi[15:8]
Looking at fabric_header_timestamp.arrival_time_hi (ingress) [7:0], with test_alloc = True
----> fabric_header_timestamp.arrival_time_hi (ingress) is allocated? False
***Allocating phv307[7:0] for fabric_header_timestamp.arrival_time_hi[7:0]
Looking at fabric_header_timestamp.arrival_time (ingress) [31:16], with test_alloc = True
----> fabric_header_timestamp.arrival_time (ingress) is allocated? False
***Allocating phv334[15:0] for fabric_header_timestamp.arrival_time[31:16]
Looking at fabric_header_timestamp.arrival_time (ingress) [15:0], with test_alloc = True
----> fabric_header_timestamp.arrival_time (ingress) is allocated? False
***Allocating phv335[15:0] for fabric_header_timestamp.arrival_time[15:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_fabric_timestamp_header (ingress) took 0.60 seconds
Working on parse node parse_snap_header (6) (ingress)

-------------------------------------------
Allocating parsed header: pkt fields (2) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 40
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 40
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 40
Parse state 0 (40 bits)
  snap_header.oui [23:0]
  snap_header.type_ [15:0]
----------------------------------------------------------------------------------------
|        Name       | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------
|  snap_header.oui  | 24 |    True   |  -  |  -   |     -     |    3     |     1      |
| snap_header.type_ | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
----------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 9
MAU containers available:
  8-bit: 14
  16-bit: 19
  32-bit: 7
Tagalong containers available:
  8-bit: 12
  16-bit: 32
  32-bit: 12
Initial packing options: 9

Packing option 0:  [8, 16, 16]
MAU containers after:
  8-bit: 14
  16-bit: 19
  32-bit: 7
+----------------------------+
|  snap_header.oui [23:16]   |
+----------------------------+
|  snap_header.oui [15:0]    |
+----------------------------+
|  snap_header.type_ [15:0]  |
+----------------------------+

Looking at snap_header.oui (ingress) [23:16], with test_alloc = True
----> snap_header.oui (ingress) is allocated? False
***Allocating phv308[7:0] for snap_header.oui[23:16]
Looking at snap_header.oui (ingress) [15:0], with test_alloc = True
----> snap_header.oui (ingress) is allocated? False
***Allocating phv336[15:0] for snap_header.oui[15:0]
Looking at snap_header.type_ (ingress) [15:0], with test_alloc = True
----> snap_header.type_ (ingress) is allocated? False
***Allocating phv337[15:0] for snap_header.type_[15:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_snap_header (ingress) took 0.94 seconds
Working on parse node parse_fabric_header (43) (egress)

-------------------------------------------
Allocating parsed header: pkt fields (8) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 40
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 40
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 40
Parse state 0 (40 bits)
  fabric_header.packetType [2:0]
  fabric_header.headerVersion [1:0]
  fabric_header.packetVersion [1:0]
  fabric_header.pad1 [0:0]
  fabric_header.fabricColor [2:0]
  fabric_header.fabricQos [4:0]
  fabric_header.dstDevice [7:0]
  fabric_header.dstPortOrGroup [15:0]
---------------------------------------------------------------------------------------------------
|             Name             | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------
|   fabric_header.packetType   | 3  |   False   |  -  |  -   |     -     |    1     |     1      |
| fabric_header.headerVersion  | 2  |   False   |  -  |  -   |     -     |    1     |     1      |
| fabric_header.packetVersion  | 2  |   False   |  -  |  -   |     -     |    1     |     1      |
|      fabric_header.pad1      | 1  |   False   |  -  |  -   |     -     |    1     |     1      |
|  fabric_header.fabricColor   | 3  |    True   |  -  |  -   |     -     |    1     |     1      |
|   fabric_header.fabricQos    | 5  |    True   |  -  |  -   |     -     |    1     |     1      |
|   fabric_header.dstDevice    | 8  |    True   |  -  |  -   |     -     |    1     |     1      |
| fabric_header.dstPortOrGroup | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
---------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 9
MAU containers available:
  8-bit: 7
  16-bit: 16
  32-bit: 4
Tagalong containers available:
  8-bit: 11
  16-bit: 18
  32-bit: 12
Initial packing options: 9

Packing option 0:  [8, 32]
MAU containers after:
  8-bit: 6
  16-bit: 16
  32-bit: 4
+---------------------------------------+
|  fabric_header.packetType [2:0]       |
|  fabric_header.headerVersion [1:0]    |
|  fabric_header.packetVersion [1:0]    |
|  fabric_header.pad1 [0:0]             |
+---------------------------------------+
|  fabric_header.fabricColor [2:0]      |
|  fabric_header.fabricQos [4:0]        |
|  fabric_header.dstDevice [7:0]        |
|  fabric_header.dstPortOrGroup [15:0]  |
+---------------------------------------+

Looking at fabric_header.packetType (egress) [2:0], with test_alloc = True
----> fabric_header.packetType (egress) is allocated? False
Looking at fabric_header.headerVersion (egress) [1:0], with test_alloc = True
Looking at fabric_header.packetVersion (egress) [1:0], with test_alloc = True
Looking at fabric_header.pad1 (egress) [0:0], with test_alloc = True

MAU groups: 1
  Group 7 8 bits -- avail 10 -- ingress avail 5 and remain 3 and promised 2 and req 1 -- egress avail 10 and remain 6 and promised 2 and req 1 -- as if deparsed True -- container_to_use phv115
***Allocating phv115[7:5] for fabric_header.packetType[2:0]
***Allocating phv115[4:3] for fabric_header.headerVersion[1:0]
***Allocating phv115[2:1] for fabric_header.packetVersion[1:0]
***Allocating phv115[0:0] for fabric_header.pad1[0:0]
Looking at fabric_header.fabricColor (egress) [2:0], with test_alloc = True
----> fabric_header.fabricColor (egress) is allocated? False
Looking at fabric_header.fabricQos (egress) [4:0], with test_alloc = True
Looking at fabric_header.dstDevice (egress) [7:0], with test_alloc = True
Looking at fabric_header.dstPortOrGroup (egress) [15:0], with test_alloc = True
***Allocating phv256[31:29] for fabric_header.fabricColor[2:0]
***Allocating phv256[28:24] for fabric_header.fabricQos[4:0]
***Allocating phv256[23:16] for fabric_header.dstDevice[7:0]
***Allocating phv256[15:0] for fabric_header.dstPortOrGroup[15:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_fabric_header (egress) took 0.41 seconds
Working on parse node parse_snap_header (6) (egress)

-------------------------------------------
Allocating parsed header: pkt fields (2) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 40
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 40
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 40
Parse state 0 (40 bits)
  snap_header.oui [23:0]
  snap_header.type_ [15:0]
----------------------------------------------------------------------------------------
|        Name       | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------
|  snap_header.oui  | 24 |    True   |  -  |  -   |     -     |    3     |     1      |
| snap_header.type_ | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
----------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 9
MAU containers available:
  8-bit: 6
  16-bit: 16
  32-bit: 4
Tagalong containers available:
  8-bit: 11
  16-bit: 18
  32-bit: 11
Initial packing options: 9

Packing option 0:  [8, 16, 16]
MAU containers after:
  8-bit: 6
  16-bit: 16
  32-bit: 4
+----------------------------+
|  snap_header.oui [23:16]   |
+----------------------------+
|  snap_header.oui [15:0]    |
+----------------------------+
|  snap_header.type_ [15:0]  |
+----------------------------+

Looking at snap_header.oui (egress) [23:16], with test_alloc = True
----> snap_header.oui (egress) is allocated? False
***Allocating phv289[7:0] for snap_header.oui[23:16]
Looking at snap_header.oui (egress) [15:0], with test_alloc = True
----> snap_header.oui (egress) is allocated? False
***Allocating phv320[15:0] for snap_header.oui[15:0]
Looking at snap_header.type_ (egress) [15:0], with test_alloc = True
----> snap_header.type_ (egress) is allocated? False
***Allocating phv321[15:0] for snap_header.type_[15:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_snap_header (egress) took 0.54 seconds
Working on parse node parse_qinq_vlan (9) (ingress)

-------------------------------------------
Allocating parsed header: pkt fields (4) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  vlan_tag_[1].pcp [2:0]
  vlan_tag_[1].cfi [0:0]
  vlan_tag_[1].vid [11:0]
  vlan_tag_[1].etherType [15:0]
---------------------------------------------------------------------------------------------
|          Name          | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------
|    vlan_tag_[1].pcp    | 3  |    True   |  -  |  -   |     -     |    1     |     1      |
|    vlan_tag_[1].cfi    | 1  |    True   |  -  |  -   |     -     |    1     |     1      |
|    vlan_tag_[1].vid    | 12 |    True   |  -  |  -   |     -     |    2     |     1      |
| vlan_tag_[1].etherType | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
---------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 6
MAU containers available:
  8-bit: 14
  16-bit: 19
  32-bit: 7
Tagalong containers available:
  8-bit: 11
  16-bit: 30
  32-bit: 12
Initial packing options: 6

Packing option 0:  [8, 8, 16]
MAU containers after:
  8-bit: 14
  16-bit: 19
  32-bit: 7
+---------------------------------+
|  vlan_tag_[1].pcp [2:0]         |
|  vlan_tag_[1].cfi [0:0]         |
|  vlan_tag_[1].vid [11:8]        |
+---------------------------------+
|  vlan_tag_[1].vid [7:0]         |
+---------------------------------+
|  vlan_tag_[1].etherType [15:0]  |
+---------------------------------+

Looking at vlan_tag_[1].pcp (ingress) [2:0], with test_alloc = True
----> vlan_tag_[1].pcp (ingress) is allocated? False
Looking at vlan_tag_[1].cfi (ingress) [0:0], with test_alloc = True
Looking at vlan_tag_[1].vid (ingress) [11:8], with test_alloc = True
***Allocating phv309[7:5] for vlan_tag_[1].pcp[2:0]
***Allocating phv309[4:4] for vlan_tag_[1].cfi[0:0]
***Allocating phv309[3:0] for vlan_tag_[1].vid[11:8]
Looking at vlan_tag_[1].vid (ingress) [7:0], with test_alloc = True
----> vlan_tag_[1].vid (ingress) is allocated? False
***Allocating phv310[7:0] for vlan_tag_[1].vid[7:0]
Looking at vlan_tag_[1].etherType (ingress) [15:0], with test_alloc = True
----> vlan_tag_[1].etherType (ingress) is allocated? False
***Allocating phv338[15:0] for vlan_tag_[1].etherType[15:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_qinq_vlan (ingress) took 0.49 seconds
Working on parse node parse_qinq_vlan (9) (egress)

-------------------------------------------
Allocating parsed header: pkt fields (4) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  vlan_tag_[1].pcp [2:0]
  vlan_tag_[1].cfi [0:0]
  vlan_tag_[1].vid [11:0]
  vlan_tag_[1].etherType [15:0]
---------------------------------------------------------------------------------------------
|          Name          | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------
|    vlan_tag_[1].pcp    | 3  |    True   |  -  |  -   |     -     |    1     |     1      |
|    vlan_tag_[1].cfi    | 1  |    True   |  -  |  -   |     -     |    1     |     1      |
|    vlan_tag_[1].vid    | 12 |    True   |  -  |  -   |     -     |    2     |     1      |
| vlan_tag_[1].etherType | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
---------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 6
MAU containers available:
  8-bit: 6
  16-bit: 16
  32-bit: 4
Tagalong containers available:
  8-bit: 10
  16-bit: 16
  32-bit: 11
Initial packing options: 6

Packing option 0:  [16, 16]
MAU containers after:
  8-bit: 6
  16-bit: 16
  32-bit: 4
+---------------------------------+
|  vlan_tag_[1].pcp [2:0]         |
|  vlan_tag_[1].cfi [0:0]         |
|  vlan_tag_[1].vid [11:0]        |
+---------------------------------+
|  vlan_tag_[1].etherType [15:0]  |
+---------------------------------+

Looking at vlan_tag_[1].pcp (egress) [2:0], with test_alloc = True
----> vlan_tag_[1].pcp (egress) is allocated? False
Looking at vlan_tag_[1].cfi (egress) [0:0], with test_alloc = True
Looking at vlan_tag_[1].vid (egress) [11:0], with test_alloc = True
***Allocating phv322[15:13] for vlan_tag_[1].pcp[2:0]
***Allocating phv322[12:12] for vlan_tag_[1].cfi[0:0]
***Allocating phv322[11:0] for vlan_tag_[1].vid[11:0]
Looking at vlan_tag_[1].etherType (egress) [15:0], with test_alloc = True
----> vlan_tag_[1].etherType (egress) is allocated? False
***Allocating phv323[15:0] for vlan_tag_[1].etherType[15:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_qinq_vlan (egress) took 0.47 seconds
Working on parse node parse_llc_header (5) (ingress)

-------------------------------------------
Allocating parsed header: pkt fields (3) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 24
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 24
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 24
Parse state 0 (24 bits)
  llc_header.dsap [7:0]
  llc_header.ssap [7:0]
  llc_header.control_ [7:0]
------------------------------------------------------------------------------------------
|         Name        | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------
|   llc_header.dsap   | 8  |    True   |  -  |  -   |    [32]   |    1     |     1      |
|   llc_header.ssap   | 8  |    True   |  -  |  -   |    [32]   |    1     |     1      |
| llc_header.control_ | 8  |    True   |  -  |  -   |    [32]   |    1     |     1      |
------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 3
MAU containers available:
  8-bit: 14
  16-bit: 19
  32-bit: 7
Tagalong containers available:
  8-bit: 9
  16-bit: 29
  32-bit: 12
Initial packing options: 3

Packing option 0:  [8, 16]
MAU containers after:
  8-bit: 14
  16-bit: 19
  32-bit: 7
+------------------------------+
|  llc_header.dsap [7:0]       |
+------------------------------+
|  llc_header.ssap [7:0]       |
|  llc_header.control_ [7:0]   |
+------------------------------+

Looking at llc_header.dsap (ingress) [7:0], with test_alloc = True
----> llc_header.dsap (ingress) is allocated? False
***Allocating phv311[7:0] for llc_header.dsap[7:0]
Looking at llc_header.ssap (ingress) [7:0], with test_alloc = True
----> llc_header.ssap (ingress) is allocated? False
Looking at llc_header.control_ (ingress) [7:0], with test_alloc = True
***Allocating phv339[15:8] for llc_header.ssap[7:0]
***Allocating phv339[7:0] for llc_header.control_[7:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_llc_header (ingress) took 0.42 seconds
Working on parse node parse_llc_header (5) (egress)

-------------------------------------------
Allocating parsed header: pkt fields (3) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 24
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 24
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 24
Parse state 0 (24 bits)
  llc_header.dsap [7:0]
  llc_header.ssap [7:0]
  llc_header.control_ [7:0]
------------------------------------------------------------------------------------------
|         Name        | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------
|   llc_header.dsap   | 8  |    True   |  -  |  -   |    [32]   |    1     |     1      |
|   llc_header.ssap   | 8  |    True   |  -  |  -   |    [32]   |    1     |     1      |
| llc_header.control_ | 8  |    True   |  -  |  -   |    [32]   |    1     |     1      |
------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 3
MAU containers available:
  8-bit: 6
  16-bit: 16
  32-bit: 4
Tagalong containers available:
  8-bit: 10
  16-bit: 14
  32-bit: 11
Initial packing options: 3

Packing option 0:  [8, 16]
MAU containers after:
  8-bit: 6
  16-bit: 16
  32-bit: 4
+------------------------------+
|  llc_header.dsap [7:0]       |
+------------------------------+
|  llc_header.ssap [7:0]       |
|  llc_header.control_ [7:0]   |
+------------------------------+

Looking at llc_header.dsap (egress) [7:0], with test_alloc = True
----> llc_header.dsap (egress) is allocated? False
***Allocating phv290[7:0] for llc_header.dsap[7:0]
Looking at llc_header.ssap (egress) [7:0], with test_alloc = True
----> llc_header.ssap (egress) is allocated? False
Looking at llc_header.control_ (egress) [7:0], with test_alloc = True
***Allocating phv324[15:8] for llc_header.ssap[7:0]
***Allocating phv324[7:0] for llc_header.control_[7:0]
Packing options tried: 1
Packing options skipped: 0

>> parse_llc_header (egress) took 0.43 seconds
Working on parse node ingress_parse_aux (50) (ingress)

-------------------------------------------
Allocating parsed header: pkt fields (1) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 16
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (16 bits)
  ig_intr_md_from_parser_aux.ingress_parser_err [15:0]
--------------------------------------------------------------------------------------------------------------------
|                      Name                     | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------------------------------
| ig_intr_md_from_parser_aux.ingress_parser_err | 16 |   False   |  -  |  -   |    [8]    |    1     |     1      |
--------------------------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 2
MAU containers available:
  8-bit: 14
  16-bit: 19
  32-bit: 7
Tagalong containers available:
  8-bit: 8
  16-bit: 28
  32-bit: 12
Initial packing options: 2

Packing option 0:  [16]
MAU containers after:
  8-bit: 14
  16-bit: 18
  32-bit: 7
+--------------------------------------------------------+
|  ig_intr_md_from_parser_aux.ingress_parser_err [15:0]  |
+--------------------------------------------------------+

Looking at ig_intr_md_from_parser_aux.ingress_parser_err (ingress) [15:0], with test_alloc = True
----> ig_intr_md_from_parser_aux.ingress_parser_err (ingress) is allocated? False
Checking if can overlay metadata field.
No required PHV group.
  Group 8 16 bits -- deparsed False -- avail 7 and promised 7 -- ingress promised 7 and remain 2 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed True -- container_to_use phv135 -- fails False
  Group 9 16 bits -- deparsed False -- avail 4 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- deparsed False -- avail 4 and promised 5 -- ingress promised 1 and remain 0 and req 0 -- egress promised 4 and remain 0 and req 2 -- act like deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- deparsed False -- avail 2 and promised 3 -- ingress promised 3 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- act like deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- deparsed False -- avail 5 and promised 3 -- ingress promised 1 and remain 0 and req 2 -- egress promised 2 and remain 0 and req 1 -- act like deparsed False -- container_to_use phv205 -- fails False
Found overlay container to use None
***Allocating phv135[15:0] for ig_intr_md_from_parser_aux.ingress_parser_err[15:0]
Packing options tried: 1
Packing options skipped: 0

>> ingress_parse_aux (ingress) took 1.44 seconds
Working on parse node egress_for_mirror_buffer (53) (egress)

-------------------------------------------
Allocating parsed header: pkt fields (2) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 16
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (16 bits)
  eg_intr_md_for_mb._pad1 [5:0]
  eg_intr_md_for_mb.egress_mirror_id [9:0]
---------------------------------------------------------------------------------------------------------
|                Name                | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------------
|      eg_intr_md_for_mb._pad1       | 6  |   False   |  -  |  -   |     -     |    1     |     1      |
| eg_intr_md_for_mb.egress_mirror_id | 10 |   False   |  -  |  -   |    [8]    |    1     |     1      |
---------------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 2
MAU containers available:
  8-bit: 6
  16-bit: 16
  32-bit: 4
Tagalong containers available:
  8-bit: 9
  16-bit: 13
  32-bit: 11
Initial packing options: 2

Packing option 0:  [16]
MAU containers after:
  8-bit: 6
  16-bit: 15
  32-bit: 4
+---------------------------------------------+
|  eg_intr_md_for_mb._pad1 [5:0]              |
|  eg_intr_md_for_mb.egress_mirror_id [9:0]   |
+---------------------------------------------+

Looking at eg_intr_md_for_mb._pad1 (egress) [5:0], with test_alloc = True
----> eg_intr_md_for_mb._pad1 (egress) is allocated? False
Looking at eg_intr_md_for_mb.egress_mirror_id (egress) [9:0], with test_alloc = True
Checking if can overlay metadata field.
No required PHV group.
  Group 9 16 bits -- deparsed True -- avail 4 and promised 5 -- ingress promised 0 and remain 0 and req 0 -- egress promised 5 and remain 0 and req 1 -- act like deparsed True -- container_to_use phv154 -- fails True
  Group 10 16 bits -- deparsed True -- avail 4 and promised 5 -- ingress promised 0 and remain 0 and req 0 -- egress promised 5 and remain 0 and req 3 -- act like deparsed True -- container_to_use phv173 -- fails True
  Group 11 16 bits -- deparsed True -- avail 2 and promised 3 -- ingress promised 2 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 1 -- act like deparsed True -- container_to_use phv182 -- fails True
  Group 12 16 bits -- deparsed True -- avail 5 and promised 3 -- ingress promised 0 and remain 0 and req 2 -- egress promised 3 and remain 0 and req 2 -- act like deparsed True -- container_to_use phv199 -- fails False
  Group 8 16 bits -- deparsed True -- avail 7 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
Could not find container to overlay in.

MAU groups: 2
  Group 12 16 bits -- avail 2 -- ingress avail 0 and remain 2 and promised 0 and req 0 -- egress avail 2 and remain 0 and promised 3 and req 2 -- as if deparsed True -- container_to_use phv199
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 15 and promised 1 and req 1 -- as if deparsed True -- container_to_use phv208
***Allocating phv199[15:10] for eg_intr_md_for_mb._pad1[5:0]
***Allocating phv199[9:0] for eg_intr_md_for_mb.egress_mirror_id[9:0]
Packing options tried: 1
Packing options skipped: 0

>> egress_for_mirror_buffer (egress) took 0.97 seconds
Working on parse node egress_parse_aux (51) (egress)

-------------------------------------------
Allocating parsed header: pkt fields (2) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 8
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 8
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (8 bits)
  eg_intr_md_from_parser_aux.clone_digest_id [3:0]
  eg_intr_md_from_parser_aux.clone_src [3:0]
-----------------------------------------------------------------------------------------------------------------
|                    Name                    | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------------------
| eg_intr_md_from_parser_aux.clone_digest_id | 4  |   False   |  -  |  -   |     -     |    1     |     1      |
|    eg_intr_md_from_parser_aux.clone_src    | 4  |   False   |  -  |  -   |     -     |    1     |     1      |
-----------------------------------------------------------------------------------------------------------------

min_extracts[8] = 1
min_extracts[16] = 1
min_extracts[32] = 1
Packing options: 1
MAU containers available:
  8-bit: 6
  16-bit: 16
  32-bit: 4
Tagalong containers available:
  8-bit: 9
  16-bit: 13
  32-bit: 11
Initial packing options: 1

Packing option 0:  [8]
MAU containers after:
  8-bit: 5
  16-bit: 16
  32-bit: 4
+-----------------------------------------------------+
|  eg_intr_md_from_parser_aux.clone_digest_id [3:0]   |
|  eg_intr_md_from_parser_aux.clone_src [3:0]         |
+-----------------------------------------------------+

Looking at eg_intr_md_from_parser_aux.clone_digest_id (egress) [3:0], with test_alloc = True
----> eg_intr_md_from_parser_aux.clone_digest_id (egress) is allocated? False
Looking at eg_intr_md_from_parser_aux.clone_src (egress) [3:0], with test_alloc = True
Checking if can overlay metadata field.
No required PHV group.
  Group 4 8 bits -- deparsed False -- avail 11 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- act like deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- deparsed False -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- act like deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- deparsed False -- avail 2 and promised 3 -- ingress promised 0 and remain 0 and req 0 -- egress promised 3 and remain 0 and req 0 -- act like deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- deparsed False -- avail 9 and promised 4 -- ingress promised 2 and remain 1 and req 3 -- egress promised 2 and remain 5 and req 0 -- act like deparsed False -- container_to_use phv117 -- fails False
Could not find container to overlay in.

MAU groups: 1
  Group 7 8 bits -- avail 9 -- ingress avail 5 and remain 3 and promised 2 and req 1 -- egress avail 9 and remain 5 and promised 2 and req 0 -- as if deparsed False -- container_to_use phv117
***Allocating phv117[7:4] for eg_intr_md_from_parser_aux.clone_digest_id[3:0]
***Allocating phv117[3:0] for eg_intr_md_from_parser_aux.clone_src[3:0]
Packing options tried: 1
Packing options skipped: 0

>> egress_parse_aux (egress) took 1.00 seconds
Working on parse node start (1) ()
>> start () took 0.00 seconds
Working on parse node ingress_parser_control (51) ()
>> ingress_parser_control () took 0.00 seconds
Working on parse node --ingress-- (0) ()
>> --ingress-- () took 0.00 seconds
Working on parse node __super_start__ (49) ()
>> __super_start__ () took 0.00 seconds
Working on parse node egress_parser_control (52) ()
>> egress_parser_control () took 0.00 seconds
Working on parse node start (1) ()
>> start () took 0.00 seconds
Working on parse node --egress-- (0) ()
>> --egress-- () took 0.00 seconds

After allocating critical parse paths:
Allocation state: Final Allocation
------------------------------------------------------------------------------
|       PHV Group        | Containers Used |   Bits Used   | Bits Available |
| (container bit widths) |     (% used)    |    (% used)   |                |
------------------------------------------------------------------------------
|         0 (32)         |   12 (75.00%)   |  384 (75.00%) |      512       |
|         1 (32)         |   16 (100.00%)  | 512 (100.00%) |      512       |
|         2 (32)         |   16 (100.00%)  | 512 (100.00%) |      512       |
|         3 (32)         |    8 (50.00%)   |  256 (50.00%) |      512       |
|    Total for 32 bit    |   52 (81.25%)   | 1664 (81.25%) |      2048      |
|                        |                 |               |                |
|         4 (8)          |    5 (31.25%)   |  40 (31.25%)  |      128       |
|         5 (8)          |   16 (100.00%)  | 128 (100.00%) |      128       |
|         6 (8)          |   14 (87.50%)   |  112 (87.50%) |      128       |
|         7 (8)          |    8 (50.00%)   |  64 (50.00%)  |      128       |
|    Total for 8 bit     |   43 (67.19%)   |  344 (67.19%) |      512       |
|                        |                 |               |                |
|         8 (16)         |    9 (56.25%)   |  144 (56.25%) |      256       |
|         9 (16)         |   12 (75.00%)   |  192 (75.00%) |      256       |
|        10 (16)         |   12 (75.00%)   |  192 (75.00%) |      256       |
|        11 (16)         |   14 (87.50%)   |  224 (87.50%) |      256       |
|        12 (16)         |   12 (75.00%)   |  192 (75.00%) |      256       |
|        13 (16)         |    0 (0.00%)    |   0 (0.00%)   |      256       |
|    Total for 16 bit    |   59 (61.46%)   |  944 (61.46%) |      1536      |
|                        |                 |               |                |
|       14 (32) T        |   13 (81.25%)   |  416 (81.25%) |      512       |
|       15 (32) T        |    4 (25.00%)   |  128 (25.00%) |      512       |
|    Total for 32 bit    |   17 (53.12%)   |  544 (53.12%) |      1024      |
|                        |                 |               |                |
|        16 (8) T        |   15 (93.75%)   |  120 (93.75%) |      128       |
|        17 (8) T        |    8 (50.00%)   |  64 (50.00%)  |      128       |
|    Total for 8 bit     |   23 (71.88%)   |  184 (71.88%) |      256       |
|                        |                 |               |                |
|       18 (16) T        |   15 (93.75%)   |  240 (93.75%) |      256       |
|       19 (16) T        |    4 (25.00%)   |  64 (25.00%)  |      256       |
|       20 (16) T        |    0 (0.00%)    |   0 (0.00%)   |      256       |
|    Total for 16 bit    |   19 (39.58%)   |  304 (39.58%) |      768       |
|                        |                 |               |                |
|       MAU total        |   154 (68.75%)  | 2952 (72.07%) |      4096      |
|     Tagalong total     |   59 (52.68%)   | 1032 (50.39%) |      2048      |
|     Overall total      |   213 (63.39%)  | 3984 (64.84%) |      6144      |
------------------------------------------------------------------------------

>>Event 'pa_overlay' at time 1573972147.07
   Took 316.61 seconds

-----------------------------------------------
  Allocating remaining parsed fields
-----------------------------------------------
Allocation Step

All Sorted parse nodes (non-critical):
  parse_ipv4_option_32b (egress) with bits = 192 and max = 9
  parse_ipv4_no_options (egress) with bits = 160 and max = 9
  parse_ipv4_other (egress) with bits = 160 and max = 9
  parse_inner_ipv4 (egress) with bits = 160 and max = 9
  parse_inner_udp (egress) with bits = 64 and max = 9
  parse_inner_ipv4 (ingress) with bits = 240 and max = 5
  parse_ipv4_option_32b (ingress) with bits = 192 and max = 5
  parse_ipv4_no_options (ingress) with bits = 160 and max = 5
  parse_ipv4_other (ingress) with bits = 160 and max = 5
  start_e2e_mirrored (egress) with bits = 112 and max = 5
  start_i2e_mirrored (egress) with bits = 112 and max = 5
  parse_vlan (ingress) with bits = 32 and max = 5
  parse_vlan (egress) with bits = 32 and max = 5
  parse_gre (egress) with bits = 32 and max = 5
  parse_geneve (egress) with bits = 64 and max = 4
  parse_nvgre (egress) with bits = 32 and max = 4
  parse_tcp (ingress) with bits = 200 and max = 3
  parse_erspan_t3 (egress) with bits = 96 and max = 3
  parse_mpls (egress) with bits = 32 and max = 3
  parse_mpls (egress) with bits = 32 and max = 3
  parse_mpls (egress) with bits = 32 and max = 3
  parse_tcp (egress) with bits = 160 and max = 2
  parse_inner_udp (ingress) with bits = 96 and max = 2
  parse_icmp (ingress) with bits = 48 and max = 2
  parse_igmp (ingress) with bits = 48 and max = 2
  parse_inner_icmp (ingress) with bits = 48 and max = 2
  parse_icmp (egress) with bits = 32 and max = 2
  parse_inner_icmp (egress) with bits = 32 and max = 2
  parse_erspan_t3 (ingress) with bits = 96 and max = 1
  parse_geneve (ingress) with bits = 93 and max = 1
  parse_nvgre (ingress) with bits = 61 and max = 1
  parse_mpls (ingress) with bits = 32 and max = 1
  parse_mpls (ingress) with bits = 32 and max = 1
  parse_mpls (ingress) with bits = 32 and max = 1
  parse_gre (ingress) with bits = 32 and max = 1
  parse_igmp (egress) with bits = 32 and max = 1
  parse_ipv4_in_ip (ingress) with bits = 5 and max = 1
  parse_ipv6_in_ip (ingress) with bits = 5 and max = 1
  parse_mpls_inner_ipv4 (ingress) with bits = 5 and max = 1
  parse_mpls_inner_ipv6 (ingress) with bits = 5 and max = 1
  parse_eompls (ingress) with bits = 5 and max = 1
  parse_gre_ipv4 (ingress) with bits = 5 and max = 1
  parse_gre_ipv6 (ingress) with bits = 5 and max = 1
  parse_arp_rarp_req (ingress) with bits = 2 and max = 1
  parse_arp_rarp_res (ingress) with bits = 2 and max = 1
Total packet bits: 2912
Total meta bits: 297
Total bits: 3209
Working on parse node parse_ipv4_option_32b (16) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (13) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 192
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 192
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 192
Parse state 0 (192 bits)
  ipv4.version [3:0]
  ipv4.ihl [3:0]
  ipv4.diffserv [7:0]
  ipv4.totalLen [15:0]
  ipv4.identification [15:0]
  ipv4.flags [2:0]
  ipv4.fragOffset [12:0]
  ipv4.ttl [7:0]
  ipv4.protocol [7:0]
  ipv4.hdrChecksum [15:0]
  ipv4.srcAddr [31:0]
  ipv4.dstAddr [31:0]
  ipv4_option_32b.option_fields [31:0]
-----------------------------------------------------------------------------------------------------------
|              Name             | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------------
|          ipv4.version         | 4  |   False   |     -      |  -   |     -     |    1     |     2      |
|            ipv4.ihl           | 4  |   False   |     -      |  -   |     -     |    1     |     2      |
|         ipv4.diffserv         | 8  |   False   |     -      |  -   |     -     |    1     |     2      |
|         ipv4.totalLen         | 16 |   False   | [(16, 16)] | [16] |    [8]    |    1     |     9      |
|      ipv4.identification      | 16 |   False   |     -      |  -   |     -     |    2     |     2      |
|           ipv4.flags          | 3  |   False   |     -      |  -   |     -     |    1     |     2      |
|        ipv4.fragOffset        | 13 |   False   |     -      |  -   |     -     |    2     |     2      |
|            ipv4.ttl           | 8  |   False   |     -      | [8]  |     -     |    1     |     2      |
|         ipv4.protocol         | 8  |   False   |  [(8, 8)]  |  -   |     -     |    1     |     5      |
|        ipv4.hdrChecksum       | 16 |   False   |     -      | [16] |     -     |    2     |     2      |
|          ipv4.srcAddr         | 32 |   False   |     -      |  -   |     -     |    4     |     2      |
|          ipv4.dstAddr         | 32 |   False   |     -      |  -   |     -     |    4     |     2      |
| ipv4_option_32b.option_fields | 32 |    True   |     -      |  -   |     -     |    4     |     1      |
-----------------------------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 5
  16-bit: 16
  32-bit: 4
Packing options: 16030
Initial packing options: 28

Packing option 0:  [8, 8, 16, 16, 16, 8, 8, 16, 32, 32, 32]

MAU groups: 1
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208

MAU groups: 1
  Group 13 16 bits -- avail 15 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 15 and remain 12 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv210
Skipping overlaying ipv6.dstAddr (egress)[127:96] with ipv4.dstAddr (egress)[31:0], because they are both used by the match key for table l3_rewrite.
>>Can pack using [8, 8, 16, 16, 16, 8, 8, 16, 32, 32, 32] if open up 3 new containers.

Packing option 1:  [8, 8, 16, 32, 8, 8, 16, 16, 16, 32, 32]

MAU groups: 1
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208

MAU groups: 1
  Group 13 16 bits -- avail 15 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 15 and remain 12 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv210
Skipping overlaying ipv6.dstAddr (egress)[127:96] with ipv4.dstAddr (egress)[31:0], because they are both used by the match key for table l3_rewrite.
>>Can pack using [8, 8, 16, 32, 8, 8, 16, 16, 16, 32, 32] if open up 3 new containers.

Packing option 2:  [8, 8, 16, 32, 8, 8, 16, 32, 16, 16, 32]

MAU groups: 1
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208

MAU groups: 1
  Group 13 16 bits -- avail 15 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 15 and remain 12 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv210
>>Can pack using [8, 8, 16, 32, 8, 8, 16, 32, 16, 16, 32] if open up 3 new containers.

Packing option 3:  [8, 8, 16, 32, 8, 8, 16, 32, 32, 16, 16]
Skipping overlaying ipv6.dstAddr (egress)[127:96] with ipv4.dstAddr (egress)[31:0], because they are both used by the match key for table l3_rewrite.
>>Can pack using [8, 8, 16, 32, 8, 8, 16, 32, 32, 16, 16] if open up 2 new containers.

Packing option 8:  [16, 16, 8, 8, 16, 8, 8, 16, 32, 32, 32]

MAU groups: 1
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208

MAU groups: 1
  Group 13 16 bits -- avail 15 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 15 and remain 12 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv210
Skipping overlaying ipv6.dstAddr (egress)[127:96] with ipv4.dstAddr (egress)[31:0], because they are both used by the match key for table l3_rewrite.
>>Can pack using [16, 16, 8, 8, 16, 8, 8, 16, 32, 32, 32] if open up 3 new containers.

Packing option 10:  [16, 16, 16, 8, 8, 8, 8, 16, 32, 32, 32]

MAU groups: 1
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208

MAU groups: 1
  Group 13 16 bits -- avail 15 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 15 and remain 12 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv210
Skipping overlaying ipv6.dstAddr (egress)[127:96] with ipv4.dstAddr (egress)[31:0], because they are both used by the match key for table l3_rewrite.
>>Can pack using [16, 16, 16, 8, 8, 8, 8, 16, 32, 32, 32] if open up 3 new containers.

Packing option 11:  [16, 16, 16, 16, 8, 8, 8, 8, 32, 32, 32]

MAU groups: 1
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208

MAU groups: 1
  Group 13 16 bits -- avail 15 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 15 and remain 12 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv210
Skipping overlaying ipv6.dstAddr (egress)[127:96] with ipv4.dstAddr (egress)[31:0], because they are both used by the match key for table l3_rewrite.
>>Can pack using [16, 16, 16, 16, 8, 8, 8, 8, 32, 32, 32] if open up 3 new containers.

Packing option 12:  [16, 16, 32, 8, 8, 8, 8, 16, 16, 32, 32]

MAU groups: 1
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208

MAU groups: 1
  Group 13 16 bits -- avail 15 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 15 and remain 12 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv210
Skipping overlaying ipv6.dstAddr (egress)[127:96] with ipv4.dstAddr (egress)[31:0], because they are both used by the match key for table l3_rewrite.
>>Can pack using [16, 16, 32, 8, 8, 8, 8, 16, 16, 32, 32] if open up 3 new containers.

Packing option 13:  [16, 16, 32, 8, 8, 8, 8, 32, 16, 16, 32]

MAU groups: 1
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208

MAU groups: 1
  Group 13 16 bits -- avail 15 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 15 and remain 12 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv210
>>Can pack using [16, 16, 32, 8, 8, 8, 8, 32, 16, 16, 32] if open up 3 new containers.

Packing option 14:  [16, 16, 32, 8, 8, 8, 8, 32, 32, 16, 16]
Skipping overlaying ipv6.dstAddr (egress)[127:96] with ipv4.dstAddr (egress)[31:0], because they are both used by the match key for table l3_rewrite.
>>Can pack using [16, 16, 32, 8, 8, 8, 8, 32, 32, 16, 16] if open up 2 new containers.

Packing option 15:  [16, 16, 32, 8, 8, 16, 8, 8, 16, 32, 32]

MAU groups: 1
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208

MAU groups: 1
  Group 7 8 bits -- avail 8 -- ingress avail 5 and remain 3 and promised 2 and req 1 -- egress avail 8 and remain 3 and promised 3 and req 2 -- as if deparsed True -- container_to_use phv116

MAU groups: 1
  Group 7 8 bits -- avail 7 -- ingress avail 5 and remain 1 and promised 2 and req 1 -- egress avail 7 and remain 1 and promised 4 and req 3 -- as if deparsed True -- container_to_use phv119

MAU groups: 1
  Group 13 16 bits -- avail 15 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 15 and remain 12 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv210
Skipping overlaying ipv6.dstAddr (egress)[127:96] with ipv4.dstAddr (egress)[31:0], because they are both used by the match key for table l3_rewrite.
>>Can pack using [16, 16, 32, 8, 8, 16, 8, 8, 16, 32, 32] if open up 5 new containers.

Packing option 17:  [16, 16, 32, 8, 8, 16, 16, 8, 8, 32, 32]

MAU groups: 1
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208

MAU groups: 1
  Group 13 16 bits -- avail 15 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 15 and remain 12 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv210

MAU groups: 1
  Group 7 8 bits -- avail 8 -- ingress avail 5 and remain 3 and promised 2 and req 1 -- egress avail 8 and remain 3 and promised 3 and req 2 -- as if deparsed True -- container_to_use phv116

MAU groups: 1
  Group 7 8 bits -- avail 7 -- ingress avail 5 and remain 1 and promised 2 and req 1 -- egress avail 7 and remain 1 and promised 4 and req 3 -- as if deparsed True -- container_to_use phv119
Skipping overlaying ipv6.dstAddr (egress)[127:96] with ipv4.dstAddr (egress)[31:0], because they are both used by the match key for table l3_rewrite.
>>Can pack using [16, 16, 32, 8, 8, 16, 16, 8, 8, 32, 32] if open up 5 new containers.

Packing option 18:  [16, 16, 32, 8, 8, 16, 32, 8, 8, 16, 32]

MAU groups: 1
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208

MAU groups: 1
  Group 13 16 bits -- avail 15 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 15 and remain 12 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv210
>>Can pack using [16, 16, 32, 8, 8, 16, 32, 8, 8, 16, 32] if open up 3 new containers.

Packing option 20:  [16, 16, 32, 8, 8, 16, 32, 16, 8, 8, 32]

MAU groups: 1
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208

MAU groups: 1
  Group 13 16 bits -- avail 15 -- ingress avail 8 and remain 8 and promised 0 and req 0 -- egress avail 15 and remain 12 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv210
>>Can pack using [16, 16, 32, 8, 8, 16, 32, 16, 8, 8, 32] if open up 3 new containers.

Packing option 21:  [16, 16, 32, 8, 8, 16, 32, 32, 8, 8, 16]

MAU groups: 1
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208
Skipping overlaying ipv6.dstAddr (egress)[127:96] with ipv4.dstAddr (egress)[31:0], because they are both used by the match key for table l3_rewrite.
>>Can pack using [16, 16, 32, 8, 8, 16, 32, 32, 8, 8, 16] if open up 4 new containers.

Packing option 23:  [16, 16, 32, 8, 8, 16, 32, 32, 16, 8, 8]

MAU groups: 1
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208
Skipping overlaying ipv6.dstAddr (egress)[127:96] with ipv4.dstAddr (egress)[31:0], because they are both used by the match key for table l3_rewrite.
>>Can pack using [16, 16, 32, 8, 8, 16, 32, 32, 16, 8, 8] if open up 4 new containers.

Packing option 24:  [8, 8, 16, 32, 8, 8, 16, 32, 32, 32]
Skipping overlaying ipv6.dstAddr (egress)[127:96] with ipv4.dstAddr (egress)[31:0], because they are both used by the match key for table l3_rewrite.
>>Can pack using [8, 8, 16, 32, 8, 8, 16, 32, 32, 32] if open up 1 new containers.

Packing option 26:  [16, 16, 32, 8, 8, 8, 8, 32, 32, 32]
Skipping overlaying ipv6.dstAddr (egress)[127:96] with ipv4.dstAddr (egress)[31:0], because they are both used by the match key for table l3_rewrite.
>>Can pack using [16, 16, 32, 8, 8, 8, 8, 32, 32, 32] if open up 1 new containers.

Packing option 27:  [16, 16, 32, 8, 8, 16, 32, 32, 32]

MAU groups: 1
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 14 and promised 2 and req 2 -- as if deparsed True -- container_to_use phv208
Skipping overlaying ipv6.dstAddr (egress)[127:96] with ipv4.dstAddr (egress)[31:0], because they are both used by the match key for table l3_rewrite.
>>Can pack using [16, 16, 32, 8, 8, 16, 32, 32, 32] if open up 2 new containers.
Packing options tried: 28
Packing options skipped: 0
Trying to place using best packing [8, 8, 16, 32, 8, 8, 16, 32, 32, 32]
***Allocating phv91[7:4] for ipv4.version[3:0]
***Allocating phv91[3:0] for ipv4.ihl[3:0]
***Allocating phv93[7:0] for ipv4.diffserv[7:0]
***Allocating phv162[15:0] for ipv4.totalLen[15:0]
***Allocating phv20[31:16] for ipv4.identification[15:0]
***Allocating phv20[15:13] for ipv4.flags[2:0]
***Allocating phv20[12:0] for ipv4.fragOffset[12:0]
***Allocating phv96[7:0] for ipv4.ttl[7:0]
***Allocating phv89[7:0] for ipv4.protocol[7:0]
***Allocating phv165[15:0] for ipv4.hdrChecksum[15:0]
***Allocating phv22[31:0] for ipv4.srcAddr[31:0]
Skipping overlaying ipv6.dstAddr (egress)[127:96] with ipv4.dstAddr (egress)[31:0], because they are both used by the match key for table l3_rewrite.
***Allocating phv32[31:0] for ipv4.dstAddr[31:0]
***Allocating phv257[31:0] for ipv4_option_32b.option_fields[31:0]
>> parse_ipv4_option_32b (egress) took 34.52 seconds
Working on parse node parse_ipv4_no_options (17) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (12) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
All fields already allocated.
>> parse_ipv4_no_options (egress) took 0.00 seconds
Working on parse node parse_ipv4_other (15) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (12) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
All fields already allocated.
>> parse_ipv4_other (egress) took 0.00 seconds
Working on parse node parse_inner_ipv4 (36) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (12) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 160
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 160
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 160
Parse state 0 (160 bits)
  inner_ipv4.version [3:0]
  inner_ipv4.ihl [3:0]
  inner_ipv4.diffserv [7:0]
  inner_ipv4.totalLen [15:0]
  inner_ipv4.identification [15:0]
  inner_ipv4.flags [2:0]
  inner_ipv4.fragOffset [12:0]
  inner_ipv4.ttl [7:0]
  inner_ipv4.protocol [7:0]
  inner_ipv4.hdrChecksum [15:0]
  inner_ipv4.srcAddr [31:0]
  inner_ipv4.dstAddr [31:0]
-------------------------------------------------------------------------------------------------------
|            Name           | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------------
|     inner_ipv4.version    | 4  |   False   |  [(8, 4)]  |  -   |     -     |    1     |     2      |
|       inner_ipv4.ihl      | 4  |   False   |  [(8, 4)]  |  -   |     -     |    1     |     2      |
|    inner_ipv4.diffserv    | 8  |   False   |  [(8, 8)]  |  -   |     -     |    1     |     2      |
|    inner_ipv4.totalLen    | 16 |   False   | [(16, 16)] |  -   |    [8]    |    2     |     9      |
| inner_ipv4.identification | 16 |   False   | [(32, 16)] |  -   |     -     |    2     |     2      |
|      inner_ipv4.flags     | 3  |   False   | [(32, 3)]  |  -   |     -     |    1     |     2      |
|   inner_ipv4.fragOffset   | 13 |   False   | [(32, 13)] |  -   |     -     |    2     |     2      |
|       inner_ipv4.ttl      | 8  |   False   |  [(8, 8)]  |  -   |     -     |    1     |     2      |
|    inner_ipv4.protocol    | 8  |   False   |  [(8, 8)]  |  -   |     -     |    1     |     5      |
|   inner_ipv4.hdrChecksum  | 16 |   False   | [(16, 16)] | [16] |     -     |    2     |     2      |
|     inner_ipv4.srcAddr    | 32 |   False   | [(32, 32)] |  -   |     -     |    4     |     2      |
|     inner_ipv4.dstAddr    | 32 |   False   | [(32, 32)] |  -   |     -     |    4     |     2      |
-------------------------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 5
  16-bit: 16
  32-bit: 4
min_extracts[8] = 6
min_extracts[16] = 3
min_extracts[32] = 6
Packing options: 10656
Initial packing options: 2562

Packing option 905:  [8, 8, 16, 32, 8, 8, 16, 32, 32]
>>Can pack using [8, 8, 16, 32, 8, 8, 16, 32, 32] if open up 0 new containers.
Packing options tried: 906
Packing options skipped: 0
Trying to place using best packing [8, 8, 16, 32, 8, 8, 16, 32, 32]
***Allocating phv92[7:4] for inner_ipv4.version[3:0]
***Allocating phv92[3:0] for inner_ipv4.ihl[3:0]
***Allocating phv94[7:0] for inner_ipv4.diffserv[7:0]
***Allocating phv164[15:0] for inner_ipv4.totalLen[15:0]
***Allocating phv21[31:16] for inner_ipv4.identification[15:0]
***Allocating phv21[15:13] for inner_ipv4.flags[2:0]
***Allocating phv21[12:0] for inner_ipv4.fragOffset[12:0]
***Allocating phv97[7:0] for inner_ipv4.ttl[7:0]
***Allocating phv90[7:0] for inner_ipv4.protocol[7:0]
***Allocating phv166[15:0] for inner_ipv4.hdrChecksum[15:0]
***Allocating phv23[31:0] for inner_ipv4.srcAddr[31:0]
***Allocating phv33[31:0] for inner_ipv4.dstAddr[31:0]
>> parse_inner_ipv4 (egress) took 2.18 seconds
Working on parse node parse_inner_udp (39) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (4) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 64
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 64
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 64
Parse state 0 (64 bits)
  inner_udp.srcPort [15:0]
  inner_udp.dstPort [15:0]
  inner_udp.length_ [15:0]
  inner_udp.checksum [15:0]
------------------------------------------------------------------------------------------------------
|        Name        | BW | Tagalong? |       Req        | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------------
| inner_udp.srcPort  | 16 |   False   | [(8, 8), (8, 8)] |  -   |     -     |    2     |     4      |
| inner_udp.dstPort  | 16 |   False   |    [(16, 16)]    |  -   |     -     |    2     |     2      |
| inner_udp.length_  | 16 |   False   |    [(16, 16)]    |  -   |    [8]    |    2     |     9      |
| inner_udp.checksum | 16 |   False   |    [(16, 16)]    |  -   |     -     |    2     |     2      |
------------------------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 5
  16-bit: 16
  32-bit: 4
Packing options: 47
Initial packing options: 47

Packing option 15:  [8, 8, 16, 16, 16]
>>Can pack using [8, 8, 16, 16, 16] if open up 2 new containers.
Packing options tried: 47
Packing options skipped: 0
Trying to place using best packing [8, 8, 16, 16, 16]
***Allocating phv84[7:0] for inner_udp.srcPort[15:8]
***Allocating phv87[7:0] for inner_udp.srcPort[7:0]
***Allocating phv171[15:0] for inner_udp.dstPort[15:0]
***Allocating phv163[15:0] for inner_udp.length_[15:0]
***Allocating phv177[15:0] for inner_udp.checksum[15:0]
>> parse_inner_udp (egress) took 1.10 seconds
Working on parse node parse_inner_ipv4 (36) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (12) / meta fields (4) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 160
Set metadata bits: 80
Gress: ingress
bits_will_need_to_parse = 240
unused_metadata_container_bits = 0
min_parse_states = 2
bits_per_state = 120

 >>>> meta field ipv4_metadata.lkp_ipv4_sa (ingress) is allocated?  False

 >>>> meta field ipv4_metadata.lkp_ipv4_da (ingress) is allocated?  False

 >>>> meta field l3_metadata.lkp_ip_proto (ingress) is allocated?  True

 >>>> meta field l3_metadata.lkp_ip_ttl (ingress) is allocated?  True
Parse state 0 (224 bits)
  inner_ipv4.version [3:0]
  inner_ipv4.ihl [3:0]
  inner_ipv4.diffserv [7:0]
  inner_ipv4.totalLen [15:0]
  inner_ipv4.identification [15:0]
  inner_ipv4.flags [2:0]
  inner_ipv4.fragOffset [12:0]
  inner_ipv4.ttl [7:0]
  inner_ipv4.protocol [7:0]
  inner_ipv4.hdrChecksum [15:0]
  inner_ipv4.srcAddr [31:0]
  inner_ipv4.dstAddr [31:16]
  l3_metadata.lkp_ip_ttl [7:0]
  l3_metadata.lkp_ip_proto [7:0]
  ipv4_metadata.lkp_ipv4_sa [31:0]
  ipv4_metadata.lkp_ipv4_da [31:0]
------------------------------------------------------------------------------------------------------
|            Name           | BW | Tagalong? |    Req    | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------------
|     inner_ipv4.version    | 4  |   False   | [(16, 4)] |  -   |     -     |    1     |     5      |
|       inner_ipv4.ihl      | 4  |   False   |     -     |  -   |     -     |    1     |     1      |
|    inner_ipv4.diffserv    | 8  |    True   |     -     |  -   |     -     |    1     |     1      |
|    inner_ipv4.totalLen    | 16 |    True   |     -     |  -   |     -     |    2     |     1      |
| inner_ipv4.identification | 16 |    True   |     -     |  -   |     -     |    2     |     1      |
|      inner_ipv4.flags     | 3  |    True   |     -     |  -   |     -     |    1     |     1      |
|   inner_ipv4.fragOffset   | 13 |    True   |     -     |  -   |     -     |    2     |     1      |
|       inner_ipv4.ttl      | 8  |    True   |     -     |  -   |     -     |    1     |     1      |
|    inner_ipv4.protocol    | 8  |    True   |     -     |  -   |     -     |    1     |     1      |
|   inner_ipv4.hdrChecksum  | 16 |    True   |     -     |  -   |     -     |    2     |     1      |
|     inner_ipv4.srcAddr    | 32 |    True   |     -     |  -   |     -     |    4     |     1      |
|     inner_ipv4.dstAddr    | 16 |    True   |     -     |  -   |     -     |    4     |     1      |
|   l3_metadata.lkp_ip_ttl  | 8  |   False   |  [(8, 8)] |  -   |     -     |    1     |     3      |
|  l3_metadata.lkp_ip_proto | 8  |   False   |  [(8, 8)] |  -   |     -     |    1     |     3      |
| ipv4_metadata.lkp_ipv4_sa | 32 |   False   |     -     |  -   |     -     |    4     |     2      |
| ipv4_metadata.lkp_ipv4_da | 32 |   False   |     -     |  -   |     -     |    4     |     2      |
------------------------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 14
  16-bit: 18
  32-bit: 7
Packing options: 34650
Initial packing options: 3435

Packing option 1036:  [16, 8, 8, 16, 16, 16, 32, 32, 8, 8, 32, 32]
Fields for container 8 at index 8 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_ip_ttl[7:0]
Fields for container 8 at index 9 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_ip_proto[7:0]
Skipping overlaying ipv6_metadata.lkp_ipv6_sa (ingress)[127:96] with ipv4_metadata.lkp_ipv4_sa (ingress)[31:0], because they are both used by the match key for table validate_packet.
>>Can pack using [16, 8, 8, 16, 16, 16, 32, 32, 8, 8, 32, 32] if open up 0 new containers.
Packing options tried: 1037
Packing options skipped: 0
Trying to place using best packing [16, 8, 8, 16, 16, 16, 32, 32, 8, 8, 32, 32]
***Allocating phv184[15:12] for inner_ipv4.version[3:0]
***Allocating phv184[11:8] for inner_ipv4.ihl[3:0]
***Allocating phv184[7:0] for inner_ipv4.diffserv[7:0]
***Allocating phv292[7:0] for inner_ipv4.totalLen[15:8]
***Allocating phv293[7:0] for inner_ipv4.totalLen[7:0]
***Allocating phv326[15:0] for inner_ipv4.identification[15:0]
***Allocating phv327[15:13] for inner_ipv4.flags[2:0]
***Allocating phv327[12:0] for inner_ipv4.fragOffset[12:0]
***Allocating phv328[15:8] for inner_ipv4.ttl[7:0]
***Allocating phv328[7:0] for inner_ipv4.protocol[7:0]
***Allocating phv260[31:16] for inner_ipv4.hdrChecksum[15:0]
***Allocating phv260[15:0] for inner_ipv4.srcAddr[31:16]
***Allocating phv261[31:16] for inner_ipv4.srcAddr[15:0]
***Allocating phv261[15:0] for inner_ipv4.dstAddr[31:16]
Fields for container 8 at index 8 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_ip_ttl[7:0]
Fields for container 8 at index 9 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_ip_proto[7:0]
Skipping overlaying ipv6_metadata.lkp_ipv6_sa (ingress)[127:96] with ipv4_metadata.lkp_ipv4_sa (ingress)[31:0], because they are both used by the match key for table validate_packet.
***Allocating phv39[31:0] for ipv4_metadata.lkp_ipv4_sa[31:0]
***Allocating phv45[31:0] for ipv4_metadata.lkp_ipv4_da[31:0]
Parse state 1 (16 bits)
  inner_ipv4.dstAddr [15:0]
-----------------------------------------------------------------------------------------
|        Name        | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------
| inner_ipv4.dstAddr | 16 |    True   |  -  |  -   |     -     |    4     |     1      |
-----------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 14
  16-bit: 18
  32-bit: 7
Packing options: 2
Initial packing options: 2

Packing option 0:  [16]
>>Can pack using [16] if open up 1 new containers.

Packing option 1:  [8, 8]
>>Can pack using [8, 8] if open up 0 new containers.
Packing options tried: 2
Packing options skipped: 0
Trying to place using best packing [8, 8]
***Allocating phv294[7:0] for inner_ipv4.dstAddr[15:8]
***Allocating phv295[7:0] for inner_ipv4.dstAddr[7:0]
>> parse_inner_ipv4 (ingress) took 4.83 seconds
Working on parse node parse_ipv4_option_32b (16) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (13) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 192
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 192
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 192
Parse state 0 (192 bits)
  ipv4.version [3:0]
  ipv4.ihl [3:0]
  ipv4.diffserv [7:0]
  ipv4.totalLen [15:0]
  ipv4.identification [15:0]
  ipv4.flags [2:0]
  ipv4.fragOffset [12:0]
  ipv4.ttl [7:0]
  ipv4.protocol [7:0]
  ipv4.hdrChecksum [15:0]
  ipv4.srcAddr [31:0]
  ipv4.dstAddr [31:0]
  ipv4_option_32b.option_fields [31:0]
-----------------------------------------------------------------------------------------------------------
|              Name             | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------------
|          ipv4.version         | 4  |   False   | [(16, 4)]  |  -   |     -     |    1     |     5      |
|            ipv4.ihl           | 4  |   False   |     -      |  -   |     -     |    1     |     1      |
|         ipv4.diffserv         | 8  |    True   |     -      |  -   |     -     |    1     |     1      |
|         ipv4.totalLen         | 16 |    True   |     -      |  -   |     -     |    2     |     1      |
|      ipv4.identification      | 16 |    True   |     -      |  -   |     -     |    2     |     1      |
|           ipv4.flags          | 3  |    True   |     -      |  -   |     -     |    1     |     1      |
|        ipv4.fragOffset        | 13 |    True   |     -      |  -   |     -     |    2     |     1      |
|            ipv4.ttl           | 8  |   False   |  [(8, 8)]  |  -   |     -     |    1     |     3      |
|         ipv4.protocol         | 8  |   False   |  [(8, 8)]  |  -   |     -     |    1     |     3      |
|        ipv4.hdrChecksum       | 16 |    True   |     -      |  -   |     -     |    2     |     1      |
|          ipv4.srcAddr         | 32 |   False   | [(32, 32)] |  -   |     -     |    4     |     2      |
|          ipv4.dstAddr         | 32 |   False   | [(32, 32)] |  -   |     -     |    4     |     2      |
| ipv4_option_32b.option_fields | 32 |    True   |     -      |  -   |     -     |    4     |     1      |
-----------------------------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 14
  16-bit: 18
  32-bit: 7
Packing options: 16030
Initial packing options: 8545

Packing option 2354:  [16, 8, 8, 16, 16, 8, 8, 16, 32, 32, 32]
>>Can pack using [16, 8, 8, 16, 16, 8, 8, 16, 32, 32, 32] if open up 6 new containers.

Packing option 2435:  [16, 8, 8, 32, 8, 8, 16, 32, 32, 16, 16]
>>Can pack using [16, 8, 8, 32, 8, 8, 16, 32, 32, 16, 16] if open up 5 new containers.

Packing option 3154:  [16, 16, 8, 8, 16, 8, 8, 16, 32, 32, 32]
>>Can pack using [16, 16, 8, 8, 16, 8, 8, 16, 32, 32, 32] if open up 6 new containers.

Packing option 3410:  [16, 16, 16, 8, 8, 8, 8, 16, 32, 32, 32]
>>Can pack using [16, 16, 16, 8, 8, 8, 8, 16, 32, 32, 32] if open up 6 new containers.

Packing option 3482:  [16, 16, 16, 16, 8, 8, 8, 8, 32, 32, 32]
>>Can pack using [16, 16, 16, 16, 8, 8, 8, 8, 32, 32, 32] if open up 6 new containers.

Packing option 3552:  [16, 16, 32, 8, 8, 8, 8, 32, 32, 16, 16]
>>Can pack using [16, 16, 32, 8, 8, 8, 8, 32, 32, 16, 16] if open up 5 new containers.

Packing option 3576:  [16, 16, 32, 8, 8, 16, 32, 32, 8, 8, 16]
>>Can pack using [16, 16, 32, 8, 8, 16, 32, 32, 8, 8, 16] if open up 5 new containers.

Packing option 3578:  [16, 16, 32, 8, 8, 16, 32, 32, 16, 8, 8]
>>Can pack using [16, 16, 32, 8, 8, 16, 32, 32, 16, 8, 8] if open up 5 new containers.

Packing option 3768:  [16, 32, 8, 8, 8, 8, 16, 32, 32, 16, 16]
>>Can pack using [16, 32, 8, 8, 8, 8, 16, 32, 32, 16, 16] if open up 5 new containers.

Packing option 4058:  [16, 32, 16, 8, 8, 8, 8, 32, 32, 16, 16]
>>Can pack using [16, 32, 16, 8, 8, 8, 8, 32, 32, 16, 16] if open up 5 new containers.

Packing option 4082:  [16, 32, 16, 8, 8, 16, 32, 32, 8, 8, 16]
>>Can pack using [16, 32, 16, 8, 8, 16, 32, 32, 8, 8, 16] if open up 5 new containers.

Packing option 4084:  [16, 32, 16, 8, 8, 16, 32, 32, 16, 8, 8]
>>Can pack using [16, 32, 16, 8, 8, 16, 32, 32, 16, 8, 8] if open up 5 new containers.

Packing option 6840:  [16, 8, 8, 32, 8, 8, 16, 32, 32, 32]
>>Can pack using [16, 8, 8, 32, 8, 8, 16, 32, 32, 32] if open up 4 new containers.

Packing option 6985:  [16, 16, 32, 8, 8, 8, 8, 32, 32, 32]
>>Can pack using [16, 16, 32, 8, 8, 8, 8, 32, 32, 32] if open up 4 new containers.

Packing option 7001:  [16, 32, 8, 8, 8, 8, 16, 32, 32, 32]
>>Can pack using [16, 32, 8, 8, 8, 8, 16, 32, 32, 32] if open up 4 new containers.

Packing option 7073:  [16, 32, 16, 8, 8, 8, 8, 32, 32, 32]
>>Can pack using [16, 32, 16, 8, 8, 8, 8, 32, 32, 32] if open up 4 new containers.

Packing option 8050:  [16, 16, 32, 8, 8, 16, 32, 32, 32]
>>Can pack using [16, 16, 32, 8, 8, 16, 32, 32, 32] if open up 3 new containers.

Packing option 8117:  [16, 32, 16, 8, 8, 16, 32, 32, 32]
>>Can pack using [16, 32, 16, 8, 8, 16, 32, 32, 32] if open up 3 new containers.
Packing options tried: 8545
Packing options skipped: 0
Trying to place using best packing [16, 16, 32, 8, 8, 16, 32, 32, 32]
***Allocating phv185[15:12] for ipv4.version[3:0]
***Allocating phv185[11:8] for ipv4.ihl[3:0]
***Allocating phv185[7:0] for ipv4.diffserv[7:0]
***Allocating phv340[15:0] for ipv4.totalLen[15:0]
***Allocating phv267[31:16] for ipv4.identification[15:0]
***Allocating phv267[15:13] for ipv4.flags[2:0]
***Allocating phv267[12:0] for ipv4.fragOffset[12:0]
***Allocating phv106[7:0] for ipv4.ttl[7:0]
***Allocating phv104[7:0] for ipv4.protocol[7:0]
***Allocating phv341[15:0] for ipv4.hdrChecksum[15:0]
***Allocating phv36[31:0] for ipv4.srcAddr[31:0]
***Allocating phv38[31:0] for ipv4.dstAddr[31:0]
***Allocating phv276[31:0] for ipv4_option_32b.option_fields[31:0]
>> parse_ipv4_option_32b (ingress) took 27.81 seconds
Working on parse node parse_ipv4_no_options (17) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (12) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
All fields already allocated.
>> parse_ipv4_no_options (ingress) took 0.00 seconds
Working on parse node parse_ipv4_other (15) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (12) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
All fields already allocated.
>> parse_ipv4_other (ingress) took 0.00 seconds
Working on parse node start_e2e_mirrored (3) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (3) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
All fields already allocated.
>> start_e2e_mirrored (egress) took 0.00 seconds
Working on parse node start_i2e_mirrored (2) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (3) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
All fields already allocated.
>> start_i2e_mirrored (egress) took 0.00 seconds
Working on parse node parse_vlan (7) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (4) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
All fields already allocated.
>> parse_vlan (ingress) took 0.00 seconds
Working on parse node parse_vlan (7) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (4) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
All fields already allocated.
>> parse_vlan (egress) took 0.00 seconds
Working on parse node parse_gre (25) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (9) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  gre.C [0:0]
  gre.R [0:0]
  gre.K [0:0]
  gre.S [0:0]
  gre.s [0:0]
  gre.recurse [2:0]
  gre.flags [4:0]
  gre.ver [2:0]
  gre.proto [15:0]
-----------------------------------------------------------------------------------------
|     Name    | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------
|    gre.C    | 1  |   False   |     -      |  -   |     -     |    1     |     1      |
|    gre.R    | 1  |   False   |     -      |  -   |     -     |    1     |     1      |
|    gre.K    | 1  |   False   |     -      |  -   |     -     |    1     |     1      |
|    gre.S    | 1  |   False   |     -      |  -   |     -     |    1     |     1      |
|    gre.s    | 1  |   False   |     -      |  -   |     -     |    1     |     1      |
| gre.recurse | 3  |   False   |     -      |  -   |     -     |    1     |     1      |
|  gre.flags  | 5  |   False   |     -      |  -   |     -     |    1     |     1      |
|   gre.ver   | 3  |   False   |     -      |  -   |     -     |    1     |     1      |
|  gre.proto  | 16 |   False   | [(16, 16)] |  -   |    [32]   |    2     |     5      |
-----------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 5
  16-bit: 16
  32-bit: 4
Packing options: 6
Initial packing options: 6

Packing option 1:  [8, 8, 16]
>>Can pack using [8, 8, 16] if open up 0 new containers.
Packing options tried: 2
Packing options skipped: 0
Trying to place using best packing [8, 8, 16]
***Allocating phv86[7:7] for gre.C[0:0]
***Allocating phv86[6:6] for gre.R[0:0]
***Allocating phv86[5:5] for gre.K[0:0]
***Allocating phv86[4:4] for gre.S[0:0]
***Allocating phv86[3:3] for gre.s[0:0]
***Allocating phv86[2:0] for gre.recurse[2:0]
***Allocating phv114[7:3] for gre.flags[4:0]
***Allocating phv114[2:0] for gre.ver[2:0]
***Allocating phv196[15:0] for gre.proto[15:0]
>> parse_gre (egress) took 0.96 seconds
Working on parse node parse_geneve (35) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (8) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 64
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 64
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 64
Parse state 0 (64 bits)
  genv.ver [1:0]
  genv.optLen [5:0]
  genv.oam [0:0]
  genv.critical [0:0]
  genv.reserved [5:0]
  genv.protoType [15:0]
  genv.vni [23:0]
  genv.reserved2 [7:0]
----------------------------------------------------------------------------------------------------
|      Name      | BW | Tagalong? |        Req         | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------
|    genv.ver    | 2  |   False   |         -          |  -   |     -     |    1     |     1      |
|  genv.optLen   | 6  |   False   |         -          |  -   |     -     |    1     |     1      |
|    genv.oam    | 1  |   False   |         -          |  -   |     -     |    1     |     1      |
| genv.critical  | 1  |   False   |         -          |  -   |     -     |    1     |     1      |
| genv.reserved  | 6  |   False   |         -          |  -   |     -     |    1     |     1      |
| genv.protoType | 16 |   False   |         -          |  -   |     -     |    2     |     1      |
|    genv.vni    | 24 |   False   | [(8, 8), (16, 16)] |  -   |     -     |    3     |     4      |
| genv.reserved2 | 8  |   False   |         -          |  -   |     -     |    1     |     1      |
----------------------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 5
  16-bit: 16
  32-bit: 4
Packing options: 47
Initial packing options: 12

Packing option 0:  [32, 8, 16, 8]
>>Can pack using [32, 8, 16, 8] if open up 0 new containers.
Packing options tried: 1
Packing options skipped: 0
Trying to place using best packing [32, 8, 16, 8]
***Allocating phv18[31:30] for genv.ver[1:0]
***Allocating phv18[29:24] for genv.optLen[5:0]
***Allocating phv18[23:23] for genv.oam[0:0]
***Allocating phv18[22:22] for genv.critical[0:0]
***Allocating phv18[21:16] for genv.reserved[5:0]
***Allocating phv18[15:0] for genv.protoType[15:0]
***Allocating phv101[7:0] for genv.vni[23:16]
***Allocating phv196[15:0] for genv.vni[15:0]
***Allocating phv114[7:0] for genv.reserved2[7:0]
>> parse_geneve (egress) took 0.93 seconds
Working on parse node parse_nvgre (28) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (2) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  nvgre.tni [23:0]
  nvgre.flow_id [7:0]
---------------------------------------------------------------------------------------------------
|      Name     | BW | Tagalong? |        Req         | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------
|   nvgre.tni   | 24 |   False   | [(8, 8), (16, 16)] |  -   |     -     |    3     |     4      |
| nvgre.flow_id | 8  |   False   |      [(8, 8)]      |  -   |     -     |    1     |     4      |
---------------------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 5
  16-bit: 16
  32-bit: 4
Packing options: 6
Initial packing options: 6

Packing option 2:  [8, 16, 8]
>>Can pack using [8, 16, 8] if open up 1 new containers.
Packing options tried: 6
Packing options skipped: 0
Trying to place using best packing [8, 16, 8]
***Allocating phv101[7:0] for nvgre.tni[23:16]
***Allocating phv197[15:0] for nvgre.tni[15:0]
***Allocating phv83[7:0] for nvgre.flow_id[7:0]
>> parse_nvgre (egress) took 0.77 seconds
Working on parse node parse_tcp (23) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (10) / meta fields (3) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 160
Set metadata bits: 40
Gress: ingress
bits_will_need_to_parse = 200
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 200

 >>>> meta field l3_metadata.lkp_outer_l4_sport (ingress) is allocated?  True

 >>>> meta field l3_metadata.lkp_outer_l4_dport (ingress) is allocated?  True

 >>>> meta field l3_metadata.lkp_outer_tcp_flags (ingress) is allocated?  False
Parse state 0 (200 bits)
  tcp.srcPort [15:0]
  tcp.dstPort [15:0]
  tcp.seqNo [31:0]
  tcp.ackNo [31:0]
  tcp.dataOffset [3:0]
  tcp.res [3:0]
  tcp.flags [7:0]
  tcp.window [15:0]
  tcp.checksum [15:0]
  tcp.urgentPtr [15:0]
  l3_metadata.lkp_outer_l4_sport [15:0]
  l3_metadata.lkp_outer_l4_dport [15:0]
  l3_metadata.lkp_outer_tcp_flags [7:0]
-------------------------------------------------------------------------------------------------------------
|               Name              | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------------------
|           tcp.srcPort           | 16 |    True   |     -      |  -   |     -     |    2     |     1      |
|           tcp.dstPort           | 16 |    True   |     -      |  -   |     -     |    2     |     1      |
|            tcp.seqNo            | 32 |    True   |     -      |  -   |     -     |    4     |     1      |
|            tcp.ackNo            | 32 |    True   |     -      |  -   |     -     |    4     |     1      |
|          tcp.dataOffset         | 4  |    True   |     -      |  -   |     -     |    1     |     1      |
|             tcp.res             | 4  |    True   |     -      |  -   |     -     |    1     |     1      |
|            tcp.flags            | 8  |   False   |  [(8, 8)]  |  -   |     -     |    1     |     3      |
|            tcp.window           | 16 |    True   |     -      |  -   |     -     |    2     |     1      |
|           tcp.checksum          | 16 |    True   |     -      |  -   |     -     |    2     |     1      |
|          tcp.urgentPtr          | 16 |    True   |     -      |  -   |     -     |    2     |     1      |
|  l3_metadata.lkp_outer_l4_sport | 16 |   False   | [(16, 16)] |  -   |     -     |    2     |     2      |
|  l3_metadata.lkp_outer_l4_dport | 16 |   False   | [(16, 16)] |  -   |     -     |    2     |     2      |
| l3_metadata.lkp_outer_tcp_flags | 8  |   False   |  [(8, 8)]  |  -   |     -     |    1     |     3      |
-------------------------------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 14
  16-bit: 18
  32-bit: 7
Packing options: 4830
Initial packing options: 490

Packing option 94:  [8, 32, 32, 32, 8, 16, 32, 16, 16, 8]
Fields for container 16 at index 7 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_outer_l4_sport[15:0]
Fields for container 16 at index 8 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_outer_l4_dport[15:0]
>>Can pack using [8, 32, 32, 32, 8, 16, 32, 16, 16, 8] if open up 2 new containers.

Packing option 95:  [8, 32, 32, 32, 8, 32, 16, 16, 16, 8]
Fields for container 16 at index 7 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_outer_l4_sport[15:0]
Fields for container 16 at index 8 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_outer_l4_dport[15:0]
>>Can pack using [8, 32, 32, 32, 8, 32, 16, 16, 16, 8] if open up 2 new containers.

Packing option 274:  [32, 8, 32, 32, 8, 16, 32, 16, 16, 8]
Fields for container 16 at index 7 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_outer_l4_sport[15:0]
Fields for container 16 at index 8 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_outer_l4_dport[15:0]
>>Can pack using [32, 8, 32, 32, 8, 16, 32, 16, 16, 8] if open up 2 new containers.

Packing option 275:  [32, 8, 32, 32, 8, 32, 16, 16, 16, 8]
Fields for container 16 at index 7 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_outer_l4_sport[15:0]
Fields for container 16 at index 8 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_outer_l4_dport[15:0]
>>Can pack using [32, 8, 32, 32, 8, 32, 16, 16, 16, 8] if open up 2 new containers.

Packing option 374:  [32, 32, 8, 32, 8, 16, 32, 16, 16, 8]
Fields for container 16 at index 7 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_outer_l4_sport[15:0]
Fields for container 16 at index 8 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_outer_l4_dport[15:0]
>>Can pack using [32, 32, 8, 32, 8, 16, 32, 16, 16, 8] if open up 2 new containers.

Packing option 375:  [32, 32, 8, 32, 8, 32, 16, 16, 16, 8]
Fields for container 16 at index 7 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_outer_l4_sport[15:0]
Fields for container 16 at index 8 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_outer_l4_dport[15:0]
>>Can pack using [32, 32, 8, 32, 8, 32, 16, 16, 16, 8] if open up 2 new containers.

Packing option 422:  [32, 32, 32, 8, 8, 16, 32, 16, 16, 8]
Fields for container 16 at index 7 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_outer_l4_sport[15:0]
Fields for container 16 at index 8 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_outer_l4_dport[15:0]
>>Can pack using [32, 32, 32, 8, 8, 16, 32, 16, 16, 8] if open up 2 new containers.

Packing option 423:  [32, 32, 32, 8, 8, 32, 16, 16, 16, 8]
Fields for container 16 at index 7 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_outer_l4_sport[15:0]
Fields for container 16 at index 8 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_outer_l4_dport[15:0]
>>Can pack using [32, 32, 32, 8, 8, 32, 16, 16, 16, 8] if open up 2 new containers.
Packing options tried: 490
Packing options skipped: 0
Trying to place using best packing [8, 32, 32, 32, 8, 16, 32, 16, 16, 8]
***Allocating phv304[7:0] for tcp.srcPort[15:8]
***Allocating phv272[31:24] for tcp.srcPort[7:0]
***Allocating phv272[23:8] for tcp.dstPort[15:0]
***Allocating phv272[7:0] for tcp.seqNo[31:24]
***Allocating phv273[31:8] for tcp.seqNo[23:0]
***Allocating phv273[7:0] for tcp.ackNo[31:24]
***Allocating phv274[31:8] for tcp.ackNo[23:0]
***Allocating phv274[7:4] for tcp.dataOffset[3:0]
***Allocating phv274[3:0] for tcp.res[3:0]
***Allocating phv120[7:0] for tcp.flags[7:0]
***Allocating phv326[15:0] for tcp.window[15:0]
***Allocating phv275[31:16] for tcp.checksum[15:0]
***Allocating phv275[15:0] for tcp.urgentPtr[15:0]
Fields for container 16 at index 7 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_outer_l4_sport[15:0]
Fields for container 16 at index 8 already allocated.  No need to overlay or allocate new.
  l3_metadata.lkp_outer_l4_dport[15:0]
***Allocating phv122[7:0] for l3_metadata.lkp_outer_tcp_flags[7:0]
>> parse_tcp (ingress) took 4.26 seconds
Working on parse node parse_erspan_t3 (29) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (6) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 96
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 96
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 96
Parse state 0 (96 bits)
  erspan_t3_header.version [3:0]
  erspan_t3_header.vlan [11:0]
  erspan_t3_header.priority_span_id [15:0]
  erspan_t3_header.timestamp [31:0]
  erspan_t3_header.sgt [15:0]
  erspan_t3_header.ft_d_other [15:0]
---------------------------------------------------------------------------------------------------------------
|                Name               | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------------------
|      erspan_t3_header.version     | 4  |   False   |     -      |  -   |     -     |    1     |     1      |
|       erspan_t3_header.vlan       | 12 |   False   |     -      |  -   |     -     |    2     |     1      |
| erspan_t3_header.priority_span_id | 16 |   False   | [(16, 16)] |  -   |     -     |    2     |     2      |
|     erspan_t3_header.timestamp    | 32 |   False   | [(32, 32)] |  -   |     -     |    4     |     3      |
|        erspan_t3_header.sgt       | 16 |   False   |     -      |  -   |     -     |    2     |     1      |
|    erspan_t3_header.ft_d_other    | 16 |   False   |     -      |  -   |     -     |    2     |     1      |
---------------------------------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 5
  16-bit: 16
  32-bit: 4
Packing options: 292
Initial packing options: 95

Packing option 0:  [8, 8, 16, 32, 32]
--> Since packet field is not parsed and does not have overlapping liveness, can overlay erspan_t3_header.version <4 bits egress parsed W> and -pad-2- <3 bits egress meta tagalong>.
--> Since packet field is not parsed and does not have overlapping liveness, can overlay erspan_t3_header.version <4 bits egress parsed W> and -pad-3- <3 bits egress meta tagalong>.
--> Since packet field is not parsed and does not have overlapping liveness, can overlay erspan_t3_header.vlan <12 bits egress parsed W> and -pad-2- <3 bits egress meta tagalong>.
--> Since packet field is not parsed and does not have overlapping liveness, can overlay erspan_t3_header.vlan <12 bits egress parsed W> and -pad-3- <3 bits egress meta tagalong>.

MAU groups: 1
  Group 3 32 bits -- avail 5 -- ingress avail 4 and remain 5 and promised 1 and req 1 -- egress avail 5 and remain 3 and promised 2 and req 1 -- as if deparsed True -- container_to_use phv55
>>Can pack using [8, 8, 16, 32, 32] if open up 1 new containers.

Packing option 5:  [16, 16, 32, 32]
--> Since packet field is not parsed and does not have overlapping liveness, can overlay erspan_t3_header.version <4 bits egress parsed W> and -pad-0- <2 bits egress meta tagalong>.
--> Since packet field is not parsed and does not have overlapping liveness, can overlay erspan_t3_header.version <4 bits egress parsed W> and -pad-1- <2 bits egress meta tagalong>.
--> Since packet field is not parsed and does not have overlapping liveness, can overlay erspan_t3_header.version <4 bits egress parsed W> and -pad-5- <7 bits egress meta tagalong>.

MAU groups: 1
  Group 3 32 bits -- avail 5 -- ingress avail 4 and remain 5 and promised 1 and req 1 -- egress avail 5 and remain 3 and promised 2 and req 1 -- as if deparsed True -- container_to_use phv55
>>Can pack using [16, 16, 32, 32] if open up 1 new containers.

Packing option 8:  [8, 8, 16, 32, 16, 16]
--> Since packet field is not parsed and does not have overlapping liveness, can overlay erspan_t3_header.sgt <16 bits egress parsed W> and -pad-0- <2 bits egress meta tagalong>.
--> Since packet field is not parsed and does not have overlapping liveness, can overlay erspan_t3_header.sgt <16 bits egress parsed W> and -pad-1- <2 bits egress meta tagalong>.
--> Since packet field is not parsed and does not have overlapping liveness, can overlay erspan_t3_header.sgt <16 bits egress parsed W> and -pad-5- <7 bits egress meta tagalong>.
--> Since packet field is not parsed and does not have overlapping liveness, can overlay erspan_t3_header.ft_d_other <16 bits egress parsed W> and -pad-0- <2 bits egress meta tagalong>.
--> Since packet field is not parsed and does not have overlapping liveness, can overlay erspan_t3_header.ft_d_other <16 bits egress parsed W> and -pad-1- <2 bits egress meta tagalong>.
--> Since packet field is not parsed and does not have overlapping liveness, can overlay erspan_t3_header.ft_d_other <16 bits egress parsed W> and -pad-5- <7 bits egress meta tagalong>.
>>Can pack using [8, 8, 16, 32, 16, 16] if open up 0 new containers.
Packing options tried: 9
Packing options skipped: 0
Trying to place using best packing [8, 8, 16, 32, 16, 16]
***Allocating phv83[7:4] for erspan_t3_header.version[3:0]
***Allocating phv83[3:0] for erspan_t3_header.vlan[11:8]
***Allocating phv101[7:0] for erspan_t3_header.vlan[7:0]
***Allocating phv170[15:0] for erspan_t3_header.priority_span_id[15:0]
***Allocating phv18[31:0] for erspan_t3_header.timestamp[31:0]
***Allocating phv176[15:0] for erspan_t3_header.sgt[15:0]
***Allocating phv161[15:0] for erspan_t3_header.ft_d_other[15:0]
>> parse_erspan_t3 (egress) took 2.27 seconds
Working on parse node parse_mpls (10) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (4) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  mpls[0].label [19:0]
  mpls[0].exp [2:0]
  mpls[0].bos [0:0]
  mpls[0].ttl [7:0]
------------------------------------------------------------------------------------
|      Name     | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------
| mpls[0].label | 20 |   False   |  -  |  -   |     -     |    3     |     3      |
|  mpls[0].exp  | 3  |   False   |  -  |  -   |     -     |    1     |     3      |
|  mpls[0].bos  | 1  |   False   |  -  |  -   |     -     |    1     |     3      |
|  mpls[0].ttl  | 8  |   False   |  -  | [8]  |     -     |    1     |     3      |
------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 5
  16-bit: 16
  32-bit: 4
Packing options: 6
Initial packing options: 3

Packing option 0:  [8, 16, 8]

MAU groups: 1
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 13 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv208
>>Can pack using [8, 16, 8] if open up 1 new containers.

Packing option 1:  [16, 8, 8]

MAU groups: 1
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 13 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv208
>>Can pack using [16, 8, 8] if open up 1 new containers.

Packing option 2:  [8, 8, 8, 8]

MAU groups: 1
  Group 7 8 bits -- avail 6 -- ingress avail 3 and remain 0 and promised 0 and req 0 -- egress avail 6 and remain 0 and promised 6 and req 5 -- as if deparsed True -- container_to_use phv119
No more 8-bit containers available for egress.
Packing options tried: 3
Packing options skipped: 0
Trying to place using best packing [8, 16, 8]
***Allocating phv114[7:0] for mpls[0].label[19:12]

MAU groups: 1
  Group 13 16 bits -- avail 16 -- ingress avail 16 and remain 8 and promised 0 and req 0 -- egress avail 16 and remain 13 and promised 3 and req 3 -- as if deparsed True -- container_to_use phv208
***Allocating phv208[15:4] for mpls[0].label[11:0]
***Allocating phv208[3:1] for mpls[0].exp[2:0]
***Allocating phv208[0:0] for mpls[0].bos[0:0]
***Allocating phv89[7:0] for mpls[0].ttl[7:0]
>> parse_mpls (egress) took 1.81 seconds
Working on parse node parse_mpls (10) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (4) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  mpls[1].label [19:0]
  mpls[1].exp [2:0]
  mpls[1].bos [0:0]
  mpls[1].ttl [7:0]
---------------------------------------------------------------------------------------------------
|      Name     | BW | Tagalong? |        Req         | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------
| mpls[1].label | 20 |   False   | [(8, 8), (16, 12)] |  -   |     -     |    3     |     3      |
|  mpls[1].exp  | 3  |   False   |     [(16, 3)]      |  -   |     -     |    1     |     3      |
|  mpls[1].bos  | 1  |   False   |     [(16, 1)]      |  -   |     -     |    1     |     3      |
|  mpls[1].ttl  | 8  |   False   |      [(8, 8)]      |  -   |     -     |    1     |     3      |
---------------------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 3
  16-bit: 13
  32-bit: 4
Packing options: 6
Initial packing options: 6

Packing option 2:  [8, 16, 8]
>>Can pack using [8, 16, 8] if open up 2 new containers.
Packing options tried: 6
Packing options skipped: 0
Trying to place using best packing [8, 16, 8]
***Allocating phv116[7:0] for mpls[1].label[19:12]
***Allocating phv209[15:4] for mpls[1].label[11:0]
***Allocating phv209[3:1] for mpls[1].exp[2:0]
***Allocating phv209[0:0] for mpls[1].bos[0:0]
***Allocating phv91[7:0] for mpls[1].ttl[7:0]
>> parse_mpls (egress) took 0.87 seconds
Working on parse node parse_mpls (10) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (4) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  mpls[2].label [19:0]
  mpls[2].exp [2:0]
  mpls[2].bos [0:0]
  mpls[2].ttl [7:0]
---------------------------------------------------------------------------------------------------
|      Name     | BW | Tagalong? |        Req         | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------
| mpls[2].label | 20 |   False   | [(8, 8), (16, 12)] |  -   |     -     |    3     |     3      |
|  mpls[2].exp  | 3  |   False   |     [(16, 3)]      |  -   |     -     |    1     |     3      |
|  mpls[2].bos  | 1  |   False   |     [(16, 1)]      |  -   |     -     |    1     |     3      |
|  mpls[2].ttl  | 8  |   False   |      [(8, 8)]      |  -   |     -     |    1     |     3      |
---------------------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 3
  16-bit: 13
  32-bit: 4
Packing options: 6
Initial packing options: 6

Packing option 2:  [8, 16, 8]
>>Can pack using [8, 16, 8] if open up 2 new containers.
Packing options tried: 6
Packing options skipped: 0
Trying to place using best packing [8, 16, 8]
***Allocating phv118[7:0] for mpls[2].label[19:12]
***Allocating phv210[15:4] for mpls[2].label[11:0]
***Allocating phv210[3:1] for mpls[2].exp[2:0]
***Allocating phv210[0:0] for mpls[2].bos[0:0]
***Allocating phv83[7:0] for mpls[2].ttl[7:0]
>> parse_mpls (egress) took 0.44 seconds
Working on parse node parse_tcp (23) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (10) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 160
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 160
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 160
Parse state 0 (160 bits)
  tcp.srcPort [15:0]
  tcp.dstPort [15:0]
  tcp.seqNo [31:0]
  tcp.ackNo [31:0]
  tcp.dataOffset [3:0]
  tcp.res [3:0]
  tcp.flags [7:0]
  tcp.window [15:0]
  tcp.checksum [15:0]
  tcp.urgentPtr [15:0]
------------------------------------------------------------------------------------------------------
|      Name      | BW | Tagalong? |         Req          | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------------
|  tcp.srcPort   | 16 |   False   |   [(8, 8), (8, 8)]   |  -   |     -     |    2     |     2      |
|  tcp.dstPort   | 16 |   False   |      [(16, 16)]      |  -   |     -     |    2     |     2      |
|   tcp.seqNo    | 32 |   False   | [(16, 16), (16, 16)] |  -   |     -     |    4     |     2      |
|   tcp.ackNo    | 32 |   False   |      [(32, 32)]      |  -   |     -     |    4     |     2      |
| tcp.dataOffset | 4  |   False   |      [(32, 4)]       |  -   |     -     |    1     |     2      |
|    tcp.res     | 4  |   False   |      [(32, 4)]       |  -   |     -     |    1     |     2      |
|   tcp.flags    | 8  |   False   |      [(32, 8)]       |  -   |     -     |    1     |     2      |
|   tcp.window   | 16 |   False   |      [(32, 16)]      |  -   |     -     |    2     |     2      |
|  tcp.checksum  | 16 |   False   |      [(32, 16)]      |  -   |     -     |    2     |     2      |
| tcp.urgentPtr  | 16 |   False   |      [(32, 16)]      |  -   |     -     |    2     |     2      |
------------------------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 3
  16-bit: 13
  32-bit: 4
min_extracts[8] = 3
min_extracts[16] = 4
min_extracts[32] = 8
Packing options: 5196
Initial packing options: 5196

Packing option 0:  [8, 8, 16, 16, 16, 32, 32, 32]
>>Can pack using [8, 8, 16, 16, 16, 32, 32, 32] if open up 0 new containers.
Packing options tried: 1
Packing options skipped: 0
Trying to place using best packing [8, 8, 16, 16, 16, 32, 32, 32]
***Allocating phv90[7:0] for tcp.srcPort[15:8]
***Allocating phv92[7:0] for tcp.srcPort[7:0]
***Allocating phv176[15:0] for tcp.dstPort[15:0]
***Allocating phv179[15:0] for tcp.seqNo[31:16]
***Allocating phv195[15:0] for tcp.seqNo[15:0]
***Allocating phv49[31:0] for tcp.ackNo[31:0]
***Allocating phv51[31:28] for tcp.dataOffset[3:0]
***Allocating phv51[27:24] for tcp.res[3:0]
***Allocating phv51[23:16] for tcp.flags[7:0]
***Allocating phv51[15:0] for tcp.window[15:0]
***Allocating phv52[31:16] for tcp.checksum[15:0]
***Allocating phv52[15:0] for tcp.urgentPtr[15:0]
>> parse_tcp (egress) took 2.02 seconds
Working on parse node parse_inner_udp (39) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (4) / meta fields (2) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 64
Set metadata bits: 32
Gress: ingress
bits_will_need_to_parse = 64
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 64
Parse state 0 (64 bits)
  inner_udp.srcPort [15:0]
  inner_udp.dstPort [15:0]
  inner_udp.length_ [15:0]
  inner_udp.checksum [15:0]
-----------------------------------------------------------------------------------------
|        Name        | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------
| inner_udp.srcPort  | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
| inner_udp.dstPort  | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
| inner_udp.length_  | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
| inner_udp.checksum | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
-----------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 14
  16-bit: 10
  32-bit: 7
Packing options: 47
Initial packing options: 47

Packing option 0:  [8, 8, 16, 32]
>>Can pack using [8, 8, 16, 32] if open up 0 new containers.
Packing options tried: 1
Packing options skipped: 0
Trying to place using best packing [8, 8, 16, 32]
***Allocating phv298[7:0] for inner_udp.srcPort[15:8]
***Allocating phv299[7:0] for inner_udp.srcPort[7:0]
***Allocating phv330[15:0] for inner_udp.dstPort[15:0]
***Allocating phv272[31:16] for inner_udp.length_[15:0]
***Allocating phv272[15:0] for inner_udp.checksum[15:0]
>> parse_inner_udp (ingress) took 0.61 seconds
Working on parse node parse_icmp (21) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (2) / meta fields (1) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 16
Gress: ingress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  icmp.typeCode [15:0]
  icmp.hdrChecksum [15:0]
---------------------------------------------------------------------------------------
|       Name       | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------
|  icmp.typeCode   | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
| icmp.hdrChecksum | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
---------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 14
  16-bit: 10
  32-bit: 7
Packing options: 6
Initial packing options: 6

Packing option 0:  [32]
>>Can pack using [32] if open up 0 new containers.
Packing options tried: 1
Packing options skipped: 0
Trying to place using best packing [32]
***Allocating phv272[31:16] for icmp.typeCode[15:0]
***Allocating phv272[15:0] for icmp.hdrChecksum[15:0]
>> parse_icmp (ingress) took 0.29 seconds
Working on parse node parse_igmp (22) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (2) / meta fields (1) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 16
Gress: ingress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  igmp.typeCode [15:0]
  igmp.hdrChecksum [15:0]
---------------------------------------------------------------------------------------
|       Name       | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------
|  igmp.typeCode   | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
| igmp.hdrChecksum | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
---------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 14
  16-bit: 10
  32-bit: 7
Packing options: 6
Initial packing options: 6

Packing option 0:  [32]
>>Can pack using [32] if open up 0 new containers.
Packing options tried: 1
Packing options skipped: 0
Trying to place using best packing [32]
***Allocating phv272[31:16] for igmp.typeCode[15:0]
***Allocating phv272[15:0] for igmp.hdrChecksum[15:0]
>> parse_igmp (ingress) took 0.29 seconds
Working on parse node parse_inner_icmp (37) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (2) / meta fields (1) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 16
Gress: ingress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  inner_icmp.typeCode [15:0]
  inner_icmp.hdrChecksum [15:0]
---------------------------------------------------------------------------------------------
|          Name          | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------
|  inner_icmp.typeCode   | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
| inner_icmp.hdrChecksum | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
---------------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 14
  16-bit: 10
  32-bit: 7
Packing options: 6
Initial packing options: 6

Packing option 0:  [32]
>>Can pack using [32] if open up 0 new containers.
Packing options tried: 1
Packing options skipped: 0
Trying to place using best packing [32]
***Allocating phv272[31:16] for inner_icmp.typeCode[15:0]
***Allocating phv272[15:0] for inner_icmp.hdrChecksum[15:0]
>> parse_inner_icmp (ingress) took 0.29 seconds
Working on parse node parse_icmp (21) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (2) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  icmp.typeCode [15:0]
  icmp.hdrChecksum [15:0]
---------------------------------------------------------------------------------------
|       Name       | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------
|  icmp.typeCode   | 16 |   False   |  -  |  -   |     -     |    2     |     2      |
| icmp.hdrChecksum | 16 |   False   |  -  |  -   |     -     |    2     |     2      |
---------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 3
  16-bit: 13
  32-bit: 4
Packing options: 6
Initial packing options: 6

Packing option 0:  [32]
>>Can pack using [32] if open up 0 new containers.
Packing options tried: 1
Packing options skipped: 0
Trying to place using best packing [32]
***Allocating phv49[31:16] for icmp.typeCode[15:0]
***Allocating phv49[15:0] for icmp.hdrChecksum[15:0]
>> parse_icmp (egress) took 0.35 seconds
Working on parse node parse_inner_icmp (37) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (2) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  inner_icmp.typeCode [15:0]
  inner_icmp.hdrChecksum [15:0]
----------------------------------------------------------------------------------------------------
|          Name          | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------
|  inner_icmp.typeCode   | 16 |   False   | [(32, 16)] |  -   |     -     |    2     |     2      |
| inner_icmp.hdrChecksum | 16 |   False   | [(32, 16)] |  -   |     -     |    2     |     2      |
----------------------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 3
  16-bit: 13
  32-bit: 4
Packing options: 6
Initial packing options: 6

Packing option 0:  [32]
>>Can pack using [32] if open up 0 new containers.
Packing options tried: 1
Packing options skipped: 0
Trying to place using best packing [32]
***Allocating phv52[31:16] for inner_icmp.typeCode[15:0]
***Allocating phv52[15:0] for inner_icmp.hdrChecksum[15:0]
>> parse_inner_icmp (egress) took 0.26 seconds
Working on parse node parse_erspan_t3 (29) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (6) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 96
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 96
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 96
Parse state 0 (96 bits)
  erspan_t3_header.version [3:0]
  erspan_t3_header.vlan [11:0]
  erspan_t3_header.priority_span_id [15:0]
  erspan_t3_header.timestamp [31:0]
  erspan_t3_header.sgt [15:0]
  erspan_t3_header.ft_d_other [15:0]
--------------------------------------------------------------------------------------------------------
|                Name               | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------------------
|      erspan_t3_header.version     | 4  |    True   |  -  |  -   |     -     |    1     |     1      |
|       erspan_t3_header.vlan       | 12 |    True   |  -  |  -   |     -     |    2     |     1      |
| erspan_t3_header.priority_span_id | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
|     erspan_t3_header.timestamp    | 32 |    True   |  -  |  -   |     -     |    4     |     1      |
|        erspan_t3_header.sgt       | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
|    erspan_t3_header.ft_d_other    | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
--------------------------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 14
  16-bit: 10
  32-bit: 7
Packing options: 292
Initial packing options: 292

Packing option 0:  [8, 8, 16, 32, 32]
>>Can pack using [8, 8, 16, 32, 32] if open up 0 new containers.
Packing options tried: 1
Packing options skipped: 0
Trying to place using best packing [8, 8, 16, 32, 32]
***Allocating phv304[7:4] for erspan_t3_header.version[3:0]
***Allocating phv304[3:0] for erspan_t3_header.vlan[11:8]
***Allocating phv305[7:0] for erspan_t3_header.vlan[7:0]
***Allocating phv331[15:0] for erspan_t3_header.priority_span_id[15:0]
***Allocating phv274[31:0] for erspan_t3_header.timestamp[31:0]
***Allocating phv275[31:16] for erspan_t3_header.sgt[15:0]
***Allocating phv275[15:0] for erspan_t3_header.ft_d_other[15:0]
>> parse_erspan_t3 (ingress) took 0.69 seconds
Working on parse node parse_geneve (35) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (8) / meta fields (2) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 64
Set metadata bits: 29
Gress: ingress
bits_will_need_to_parse = 64
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 64
Parse state 0 (64 bits)
  genv.ver [1:0]
  genv.optLen [5:0]
  genv.oam [0:0]
  genv.critical [0:0]
  genv.reserved [5:0]
  genv.protoType [15:0]
  genv.vni [23:0]
  genv.reserved2 [7:0]
-------------------------------------------------------------------------------------
|      Name      | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------
|    genv.ver    | 2  |    True   |  -  |  -   |     -     |    1     |     1      |
|  genv.optLen   | 6  |    True   |  -  |  -   |     -     |    1     |     1      |
|    genv.oam    | 1  |    True   |  -  |  -   |     -     |    1     |     1      |
| genv.critical  | 1  |    True   |  -  |  -   |     -     |    1     |     1      |
| genv.reserved  | 6  |    True   |  -  |  -   |     -     |    1     |     1      |
| genv.protoType | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
|    genv.vni    | 24 |    True   |  -  |  -   |     -     |    3     |     1      |
| genv.reserved2 | 8  |    True   |  -  |  -   |     -     |    1     |     1      |
-------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 14
  16-bit: 10
  32-bit: 7
Packing options: 47
Initial packing options: 47

Packing option 0:  [8, 8, 16, 32]
>>Can pack using [8, 8, 16, 32] if open up 0 new containers.
Packing options tried: 1
Packing options skipped: 0
Trying to place using best packing [8, 8, 16, 32]
***Allocating phv304[7:6] for genv.ver[1:0]
***Allocating phv304[5:0] for genv.optLen[5:0]
***Allocating phv305[7:7] for genv.oam[0:0]
***Allocating phv305[6:6] for genv.critical[0:0]
***Allocating phv305[5:0] for genv.reserved[5:0]
***Allocating phv333[15:0] for genv.protoType[15:0]
***Allocating phv275[31:8] for genv.vni[23:0]
***Allocating phv275[7:0] for genv.reserved2[7:0]
>> parse_geneve (ingress) took 1.11 seconds
Working on parse node parse_nvgre (28) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (2) / meta fields (2) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 29
Gress: ingress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  nvgre.tni [23:0]
  nvgre.flow_id [7:0]
------------------------------------------------------------------------------------
|      Name     | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------
|   nvgre.tni   | 24 |    True   |  -  |  -   |     -     |    3     |     1      |
| nvgre.flow_id | 8  |    True   |  -  |  -   |     -     |    1     |     1      |
------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 14
  16-bit: 10
  32-bit: 7
Packing options: 6
Initial packing options: 6

Packing option 0:  [32]
>>Can pack using [32] if open up 0 new containers.
Packing options tried: 1
Packing options skipped: 0
Trying to place using best packing [32]
***Allocating phv274[31:8] for nvgre.tni[23:0]
***Allocating phv274[7:0] for nvgre.flow_id[7:0]
>> parse_nvgre (ingress) took 0.31 seconds
Working on parse node parse_mpls (10) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (4) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  mpls[0].label [19:0]
  mpls[0].exp [2:0]
  mpls[0].bos [0:0]
  mpls[0].ttl [7:0]
------------------------------------------------------------------------------------
|      Name     | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------
| mpls[0].label | 20 |   False   |  -  |  -   |     -     |    3     |     1      |
|  mpls[0].exp  | 3  |    True   |  -  |  -   |     -     |    1     |     1      |
|  mpls[0].bos  | 1  |    True   |  -  |  -   |     -     |    1     |     1      |
|  mpls[0].ttl  | 8  |    True   |  -  |  -   |     -     |    1     |     1      |
------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 14
  16-bit: 10
  32-bit: 7
Packing options: 6
Initial packing options: 6

Packing option 0:  [32]
>>Can pack using [32] if open up 0 new containers.
Packing options tried: 1
Packing options skipped: 0
Trying to place using best packing [32]
***Allocating phv36[31:12] for mpls[0].label[19:0]
***Allocating phv36[11:9] for mpls[0].exp[2:0]
***Allocating phv36[8:8] for mpls[0].bos[0:0]
***Allocating phv36[7:0] for mpls[0].ttl[7:0]
>> parse_mpls (ingress) took 0.36 seconds
Working on parse node parse_mpls (10) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (4) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  mpls[1].label [19:0]
  mpls[1].exp [2:0]
  mpls[1].bos [0:0]
  mpls[1].ttl [7:0]
------------------------------------------------------------------------------------
|      Name     | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------
| mpls[1].label | 20 |    True   |  -  |  -   |     -     |    3     |     1      |
|  mpls[1].exp  | 3  |    True   |  -  |  -   |     -     |    1     |     1      |
|  mpls[1].bos  | 1  |    True   |  -  |  -   |     -     |    1     |     1      |
|  mpls[1].ttl  | 8  |    True   |  -  |  -   |     -     |    1     |     1      |
------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 14
  16-bit: 10
  32-bit: 7
Packing options: 6
Initial packing options: 6

Packing option 0:  [32]
>>Can pack using [32] if open up 0 new containers.
Packing options tried: 1
Packing options skipped: 0
Trying to place using best packing [32]
***Allocating phv274[31:12] for mpls[1].label[19:0]
***Allocating phv274[11:9] for mpls[1].exp[2:0]
***Allocating phv274[8:8] for mpls[1].bos[0:0]
***Allocating phv274[7:0] for mpls[1].ttl[7:0]
>> parse_mpls (ingress) took 0.32 seconds
Working on parse node parse_mpls (10) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (4) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  mpls[2].label [19:0]
  mpls[2].exp [2:0]
  mpls[2].bos [0:0]
  mpls[2].ttl [7:0]
------------------------------------------------------------------------------------
|      Name     | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------
| mpls[2].label | 20 |    True   |  -  |  -   |     -     |    3     |     1      |
|  mpls[2].exp  | 3  |    True   |  -  |  -   |     -     |    1     |     1      |
|  mpls[2].bos  | 1  |    True   |  -  |  -   |     -     |    1     |     1      |
|  mpls[2].ttl  | 8  |    True   |  -  |  -   |     -     |    1     |     1      |
------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 14
  16-bit: 10
  32-bit: 7
Packing options: 6
Initial packing options: 6

Packing option 0:  [32]
>>Can pack using [32] if open up 0 new containers.
Packing options tried: 1
Packing options skipped: 0
Trying to place using best packing [32]
***Allocating phv275[31:12] for mpls[2].label[19:0]
***Allocating phv275[11:9] for mpls[2].exp[2:0]
***Allocating phv275[8:8] for mpls[2].bos[0:0]
***Allocating phv275[7:0] for mpls[2].ttl[7:0]
>> parse_mpls (ingress) took 0.32 seconds
Working on parse node parse_gre (25) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (9) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 0
Gress: ingress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  gre.C [0:0]
  gre.R [0:0]
  gre.K [0:0]
  gre.S [0:0]
  gre.s [0:0]
  gre.recurse [2:0]
  gre.flags [4:0]
  gre.ver [2:0]
  gre.proto [15:0]
----------------------------------------------------------------------------------
|     Name    | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------
|    gre.C    | 1  |    True   |  -  |  -   |     -     |    1     |     1      |
|    gre.R    | 1  |    True   |  -  |  -   |     -     |    1     |     1      |
|    gre.K    | 1  |    True   |  -  |  -   |     -     |    1     |     1      |
|    gre.S    | 1  |    True   |  -  |  -   |     -     |    1     |     1      |
|    gre.s    | 1  |    True   |  -  |  -   |     -     |    1     |     1      |
| gre.recurse | 3  |    True   |  -  |  -   |     -     |    1     |     1      |
|  gre.flags  | 5  |    True   |  -  |  -   |     -     |    1     |     1      |
|   gre.ver   | 3  |    True   |  -  |  -   |     -     |    1     |     1      |
|  gre.proto  | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
----------------------------------------------------------------------------------

MAU containers available:
  8-bit: 14
  16-bit: 10
  32-bit: 7
Packing options: 6
Initial packing options: 6

Packing option 0:  [32]
>>Can pack using [32] if open up 1 new containers.

Packing option 1:  [8, 8, 16]
>>Can pack using [8, 8, 16] if open up 0 new containers.
Packing options tried: 2
Packing options skipped: 0
Trying to place using best packing [8, 8, 16]
***Allocating phv300[7:7] for gre.C[0:0]
***Allocating phv300[6:6] for gre.R[0:0]
***Allocating phv300[5:5] for gre.K[0:0]
***Allocating phv300[4:4] for gre.S[0:0]
***Allocating phv300[3:3] for gre.s[0:0]
***Allocating phv300[2:0] for gre.recurse[2:0]
***Allocating phv301[7:3] for gre.flags[4:0]
***Allocating phv301[2:0] for gre.ver[2:0]
***Allocating phv333[15:0] for gre.proto[15:0]
>> parse_gre (ingress) took 0.77 seconds
Working on parse node parse_igmp (22) (egress)

-------------------------------------------
Overlaying parsed header: pkt fields (2) / meta fields (0) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 32
Set metadata bits: 0
Gress: egress
bits_will_need_to_parse = 32
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 32
Parse state 0 (32 bits)
  igmp.typeCode [15:0]
  igmp.hdrChecksum [15:0]
---------------------------------------------------------------------------------------
|       Name       | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------
|  igmp.typeCode   | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
| igmp.hdrChecksum | 16 |    True   |  -  |  -   |     -     |    2     |     1      |
---------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 3
  16-bit: 13
  32-bit: 4
Packing options: 6
Initial packing options: 6

Packing option 0:  [32]
>>Can pack using [32] if open up 0 new containers.
Packing options tried: 1
Packing options skipped: 0
Trying to place using best packing [32]
***Allocating phv24[31:16] for igmp.typeCode[15:0]
***Allocating phv24[15:0] for igmp.hdrChecksum[15:0]
>> parse_igmp (egress) took 0.42 seconds
Working on parse node parse_ipv4_in_ip (18) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (0) / meta fields (1) using extraction bandwidth 224
-------------------------------------------
All fields already allocated.
>> parse_ipv4_in_ip (ingress) took 0.00 seconds
Working on parse node parse_ipv6_in_ip (19) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (0) / meta fields (1) using extraction bandwidth 224
-------------------------------------------
All fields already allocated.
>> parse_ipv6_in_ip (ingress) took 0.00 seconds
Working on parse node parse_mpls_inner_ipv4 (12) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (0) / meta fields (1) using extraction bandwidth 224
-------------------------------------------
All fields already allocated.
>> parse_mpls_inner_ipv4 (ingress) took 0.00 seconds
Working on parse node parse_mpls_inner_ipv6 (13) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (0) / meta fields (1) using extraction bandwidth 224
-------------------------------------------
All fields already allocated.
>> parse_mpls_inner_ipv6 (ingress) took 0.00 seconds
Working on parse node parse_eompls (33) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (0) / meta fields (1) using extraction bandwidth 224
-------------------------------------------
All fields already allocated.
>> parse_eompls (ingress) took 0.00 seconds
Working on parse node parse_gre_ipv4 (26) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (0) / meta fields (1) using extraction bandwidth 224
-------------------------------------------
All fields already allocated.
>> parse_gre_ipv4 (ingress) took 0.00 seconds
Working on parse node parse_gre_ipv6 (27) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (0) / meta fields (1) using extraction bandwidth 224
-------------------------------------------
All fields already allocated.
>> parse_gre_ipv6 (ingress) took 0.00 seconds
Working on parse node parse_arp_rarp_req (30) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (0) / meta fields (1) using extraction bandwidth 224
-------------------------------------------
Extracted bits: 0
Set metadata bits: 2
Gress: ingress
bits_will_need_to_parse = 8
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8

 >>>> meta field l2_metadata.arp_opcode (ingress) is allocated?  False
Parse state 0 (8 bits)
  -pad-8- [5:0]
  l2_metadata.arp_opcode [1:0]
---------------------------------------------------------------------------------------------
|          Name          | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------
|        -pad-8-         | 6  |    True   |  -  |  -   |     -     |   None   |     1      |
| l2_metadata.arp_opcode | 2  |   False   |  -  |  -   |     -     |    1     |     1      |
---------------------------------------------------------------------------------------------

MAU containers available:
  8-bit: 14
  16-bit: 10
  32-bit: 7
Packing options: 1
Initial packing options: 1

Packing option 0:  [8]

MAU groups: 2
  Group 7 8 bits -- avail 4 -- ingress avail 3 and remain 2 and promised 1 and req 0 -- egress avail 4 and remain 2 and promised 1 and req 0 -- as if deparsed False -- container_to_use phv125
  Group 4 8 bits -- avail 11 -- ingress avail 11 and remain 10 and promised 1 and req 0 -- egress avail 8 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv69
>>Can pack using [8] if open up 1 new containers.
Packing options tried: 1
Packing options skipped: 0
Trying to place using best packing [8]

MAU groups: 2
  Group 7 8 bits -- avail 4 -- ingress avail 3 and remain 2 and promised 1 and req 0 -- egress avail 4 and remain 2 and promised 1 and req 0 -- as if deparsed False -- container_to_use phv125
  Group 4 8 bits -- avail 11 -- ingress avail 11 and remain 10 and promised 1 and req 0 -- egress avail 8 and remain 8 and promised 0 and req 0 -- as if deparsed False -- container_to_use phv69
***Allocating phv125[7:2] for -pad-8-[5:0]
***Allocating phv125[1:0] for l2_metadata.arp_opcode[1:0]
>> parse_arp_rarp_req (ingress) took 0.86 seconds
Working on parse node parse_arp_rarp_res (31) (ingress)

-------------------------------------------
Overlaying parsed header: pkt fields (0) / meta fields (1) using extraction bandwidth 224
-------------------------------------------
All fields already allocated.
>> parse_arp_rarp_res (ingress) took 0.00 seconds

After allocating remaining parse nodes:
Allocation state: Final Allocation
------------------------------------------------------------------------------
|       PHV Group        | Containers Used |   Bits Used   | Bits Available |
| (container bit widths) |     (% used)    |    (% used)   |                |
------------------------------------------------------------------------------
|         0 (32)         |   12 (75.00%)   |  384 (75.00%) |      512       |
|         1 (32)         |   16 (100.00%)  | 512 (100.00%) |      512       |
|         2 (32)         |   16 (100.00%)  | 512 (100.00%) |      512       |
|         3 (32)         |    8 (50.00%)   |  256 (50.00%) |      512       |
|    Total for 32 bit    |   52 (81.25%)   | 1664 (81.25%) |      2048      |
|                        |                 |               |                |
|         4 (8)          |    5 (31.25%)   |  40 (31.25%)  |      128       |
|         5 (8)          |   16 (100.00%)  | 128 (100.00%) |      128       |
|         6 (8)          |   14 (87.50%)   |  112 (87.50%) |      128       |
|         7 (8)          |   13 (81.25%)   |  104 (81.25%) |      128       |
|    Total for 8 bit     |   48 (75.00%)   |  384 (75.00%) |      512       |
|                        |                 |               |                |
|         8 (16)         |    9 (56.25%)   |  144 (56.25%) |      256       |
|         9 (16)         |   12 (75.00%)   |  192 (75.00%) |      256       |
|        10 (16)         |   14 (87.50%)   |  224 (87.50%) |      256       |
|        11 (16)         |   14 (87.50%)   |  224 (87.50%) |      256       |
|        12 (16)         |   13 (81.25%)   |  208 (81.25%) |      256       |
|        13 (16)         |    3 (18.75%)   |  48 (18.75%)  |      256       |
|    Total for 16 bit    |   65 (67.71%)   | 1040 (67.71%) |      1536      |
|                        |                 |               |                |
|       14 (32) T        |   14 (87.50%)   |  448 (87.50%) |      512       |
|       15 (32) T        |    5 (31.25%)   |  160 (31.25%) |      512       |
|    Total for 32 bit    |   19 (59.38%)   |  608 (59.38%) |      1024      |
|                        |                 |               |                |
|        16 (8) T        |   15 (93.75%)   |  120 (93.75%) |      128       |
|        17 (8) T        |    8 (50.00%)   |  64 (50.00%)  |      128       |
|    Total for 8 bit     |   23 (71.88%)   |  184 (71.88%) |      256       |
|                        |                 |               |                |
|       18 (16) T        |   15 (93.75%)   |  240 (93.75%) |      256       |
|       19 (16) T        |    6 (37.50%)   |  96 (37.50%)  |      256       |
|       20 (16) T        |    0 (0.00%)    |   0 (0.00%)   |      256       |
|    Total for 16 bit    |   21 (43.75%)   |  336 (43.75%) |      768       |
|                        |                 |               |                |
|       MAU total        |   165 (73.66%)  | 3088 (75.39%) |      4096      |
|     Tagalong total     |   63 (56.25%)   | 1128 (55.08%) |      2048      |
|     Overall total      |   228 (67.86%)  | 4216 (68.62%) |      6144      |
------------------------------------------------------------------------------



Difference in allocation between critical parse path and overlaying headers:
Allocation state: Diff
----------------------------------------------------------------------------
|       PHV Group        | Containers Used |  Bits Used  | Bits Available |
| (container bit widths) |     (% used)    |   (% used)  |                |
----------------------------------------------------------------------------
|         0 (32)         |    0 (0.00%)    |  0 (0.00%)  |      512       |
|         1 (32)         |    0 (0.00%)    |  0 (0.00%)  |      512       |
|         2 (32)         |    0 (0.00%)    |  0 (0.00%)  |      512       |
|         3 (32)         |    0 (0.00%)    |  0 (0.00%)  |      512       |
|    Total for 32 bit    |    0 (0.00%)    |  0 (0.00%)  |      2048      |
|                        |                 |             |                |
|         4 (8)          |    0 (0.00%)    |  0 (0.00%)  |      128       |
|         5 (8)          |    0 (0.00%)    |  0 (0.00%)  |      128       |
|         6 (8)          |    0 (0.00%)    |  0 (0.00%)  |      128       |
|         7 (8)          |    5 (31.25%)   | 40 (31.25%) |      128       |
|    Total for 8 bit     |    5 (7.81%)    |  40 (7.81%) |      512       |
|                        |                 |             |                |
|         8 (16)         |    0 (0.00%)    |  0 (0.00%)  |      256       |
|         9 (16)         |    0 (0.00%)    |  0 (0.00%)  |      256       |
|        10 (16)         |    2 (12.50%)   | 32 (12.50%) |      256       |
|        11 (16)         |    0 (0.00%)    |  0 (0.00%)  |      256       |
|        12 (16)         |    1 (6.25%)    |  16 (6.25%) |      256       |
|        13 (16)         |    3 (18.75%)   | 48 (18.75%) |      256       |
|    Total for 16 bit    |    6 (6.25%)    |  96 (6.25%) |      1536      |
|                        |                 |             |                |
|       14 (32) T        |    1 (6.25%)    |  32 (6.25%) |      512       |
|       15 (32) T        |    1 (6.25%)    |  32 (6.25%) |      512       |
|    Total for 32 bit    |    2 (6.25%)    |  64 (6.25%) |      1024      |
|                        |                 |             |                |
|        16 (8) T        |    0 (0.00%)    |  0 (0.00%)  |      128       |
|        17 (8) T        |    0 (0.00%)    |  0 (0.00%)  |      128       |
|    Total for 8 bit     |    0 (0.00%)    |  0 (0.00%)  |      256       |
|                        |                 |             |                |
|       18 (16) T        |    0 (0.00%)    |  0 (0.00%)  |      256       |
|       19 (16) T        |    2 (12.50%)   | 32 (12.50%) |      256       |
|       20 (16) T        |    0 (0.00%)    |  0 (0.00%)  |      256       |
|    Total for 16 bit    |    2 (4.17%)    |  32 (4.17%) |      768       |
|                        |                 |             |                |
|       MAU total        |    11 (4.91%)   | 136 (3.32%) |      4096      |
|     Tagalong total     |    4 (3.57%)    |  96 (4.69%) |      2048      |
|     Overall total      |    15 (4.46%)   | 232 (3.78%) |      6144      |
----------------------------------------------------------------------------

>>Event 'pa_meta1' at time 1573972239.13
   Took 92.06 seconds

-----------------------------------------------
  Allocating metadata (pass 1)
-----------------------------------------------
Allocation Step
Removing unnecessary metadata -pad-7- (ingress).
Creating 4 processes
  Task queue has 114 elements
Process 0 started.
Process 1 started.
Process 2 started.
Process 3 started.
Process 0 examined 31 fields
Process 3 examined 28 fields
Process 1 examined 32 fields
Process 2 examined 23 fields
Total metadata field instances to allocate: 114  / 781 bits (543 ingress bits and 238 egress bits)
>> Metadata analysis took 132.99 seconds
Promised metadata field instances to allocate: 27 / 365 bits (194 ingress bits and 171 egress bits)
     0: egress_metadata.mac_da (egress) (highly=0, mau_group_size=3, max_overlay=10, max_share=46, max_split=48, bit_width=48, initial_usage_read=2, earliest_use=3, latest_use=4)
     1: tunnel_metadata.vnid (egress) (highly=0, mau_group_size=4, max_overlay=8, max_share=44, max_split=24, bit_width=24, initial_usage_read=2, earliest_use=3, latest_use=6)
     2: l2_metadata.lkp_mac_type (ingress) (highly=0, mau_group_size=5, max_overlay=4, max_share=72, max_split=16, bit_width=16, initial_usage_read=1, earliest_use=0, latest_use=9)
     3: l2_metadata.l2_nexthop (ingress) (highly=0, mau_group_size=5, max_overlay=7, max_share=92, max_split=16, bit_width=16, initial_usage_read=2, earliest_use=3, latest_use=7)
     4: l3_metadata.fib_nexthop (ingress) (highly=0, mau_group_size=5, max_overlay=7, max_share=73, max_split=16, bit_width=16, initial_usage_read=4, earliest_use=2, latest_use=7)
     5: acl_metadata.acl_nexthop (ingress) (highly=0, mau_group_size=5, max_overlay=7, max_share=92, max_split=16, bit_width=16, initial_usage_read=4, earliest_use=3, latest_use=7)
     6: acl_metadata.racl_nexthop (ingress) (highly=0, mau_group_size=5, max_overlay=7, max_share=110, max_split=16, bit_width=16, initial_usage_read=3, earliest_use=5, latest_use=7)
     7: hash_metadata.hash1 (ingress) (highly=1, mau_group_size=5, max_overlay=9, max_share=0, max_split=1, bit_width=16, initial_usage_read=3, earliest_use=6, latest_use=8)
     8: hash_metadata.hash2 (ingress) (highly=1, mau_group_size=5, max_overlay=7, max_share=0, max_split=1, bit_width=16, initial_usage_read=1, earliest_use=7, latest_use=9)
     9: l3_metadata.l3_mtu_check (egress) (highly=0, mau_group_size=9, max_overlay=4, max_share=30, max_split=1, bit_width=16, initial_usage_read=2, earliest_use=2, latest_use=8)
    10: egress_metadata.payload_length (egress) (highly=1, mau_group_size=9, max_overlay=8, max_share=0, max_split=1, bit_width=16, initial_usage_read=1, earliest_use=5, latest_use=6)
    11: l2_metadata.l2_src_move (ingress) (highly=1, mau_group_size=3, max_overlay=7, max_share=0, max_split=14, bit_width=14, initial_usage_read=3, earliest_use=3, latest_use=9)
    12: l3_metadata.same_bd_check (ingress) (highly=1, mau_group_size=2, max_overlay=7, max_share=0, max_split=14, bit_width=14, initial_usage_read=3, earliest_use=7, latest_use=9)
    13: tunnel_metadata.vtep_ifindex (ingress) (highly=0, mau_group_size=3, max_overlay=15, max_share=100, max_split=14, bit_width=14, initial_usage_read=3, earliest_use=1, latest_use=2)
    14: egress_metadata.outer_bd (egress) (highly=0, mau_group_size=7, max_overlay=0, max_share=44, max_split=14, bit_width=14, initial_usage_read=4, earliest_use=0, latest_use=7)
    15: egress_metadata.bd (egress) (highly=0, mau_group_size=7, max_overlay=3, max_share=37, max_split=14, bit_width=14, initial_usage_read=5, earliest_use=0, latest_use=5)
    16: ingress_metadata.outer_bd (egress) (highly=0, mau_group_size=7, max_overlay=0, max_share=50, max_split=14, bit_width=14, initial_usage_read=1, earliest_use=0, latest_use=0)
    17: egress_metadata.same_bd_check (egress) (highly=1, mau_group_size=7, max_overlay=10, max_share=0, max_split=14, bit_width=14, initial_usage_read=2, earliest_use=0, latest_use=1)
    18: ig_intr_md_for_tm.level1_mcast_hash (ingress) (highly=0, mau_group_size=5, max_overlay=7, max_share=109, max_split=1, bit_width=13, initial_usage_read=1, earliest_use=7, latest_use=12)
    19: ig_intr_md_for_tm.level2_mcast_hash (ingress) (highly=0, mau_group_size=5, max_overlay=7, max_share=45, max_split=1, bit_width=13, initial_usage_read=1, earliest_use=7, latest_use=12)
    20: ig_intr_md_for_tm.ucast_egress_port (ingress) (highly=0, mau_group_size=2, max_overlay=1, max_share=0, max_split=1, bit_width=9, initial_usage_read=1, earliest_use=0, latest_use=12)
    21: tunnel_metadata.inner_ip_proto (egress) (highly=0, mau_group_size=5, max_overlay=8, max_share=46, max_split=8, bit_width=8, initial_usage_read=1, earliest_use=5, latest_use=6)
    22: l3_metadata.lkp_ip_version (ingress) (highly=0, mau_group_size=5, max_overlay=12, max_share=73, max_split=4, bit_width=4, initial_usage_read=4, earliest_use=0, latest_use=3)
    23: tunnel_metadata.tunnel_if_check (ingress) (highly=1, mau_group_size=2, max_overlay=38, max_share=0, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=8, latest_use=9)
    24: eg_intr_md_for_oport.capture_tstamp_on_tx (egress) (highly=1, mau_group_size=2, max_overlay=19, max_share=25, max_split=1, bit_width=1, initial_usage_read=1, earliest_use=7, latest_use=12)
    25: egress_metadata.routed (egress) (highly=0, mau_group_size=3, max_overlay=4, max_share=37, max_split=1, bit_width=1, initial_usage_read=4, earliest_use=0, latest_use=4)
    26: l3_metadata.outer_routed (egress) (highly=0, mau_group_size=3, max_overlay=0, max_share=50, max_split=1, bit_width=1, initial_usage_read=1, earliest_use=0, latest_use=0)

--------------
Working on:
egress_metadata.mac_da <48 bits egress meta R W>
bits_will_need_to_parse = 48
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 48
extracted_bits = 48 while meta_fi.bit_width = 48
Parse state 0 (48 bits)
  egress_metadata.mac_da [47:0]
--------------------------------------------------------------------------------------------------------------------
|          Name          | BW | Tagalong? |            Req             | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------------------------------
| egress_metadata.mac_da | 48 |   False   | [(8, 8), (8, 8), (32, 32)] |  -   |     -     |   None   |     3      |
--------------------------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(8, 8), (8, 8), (32, 32)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [8, 8, 32]
  (3) msb_offset = 8
  case 3: No particular alignment needed
  (1) msb_offset = 8
  case 3: No particular alignment needed
  (1) msb_offset = 32
>>Can pack using [8, 8, 32] if open up 3 new containers.

Attempting to share...

>>Choose overlay option
  (3) msb_offset = 8
***Allocating phv102[7:0] for egress_metadata.mac_da[47:40]
  case 3: No particular alignment needed
  (1) msb_offset = 8
***Allocating phv119[7:0] for egress_metadata.mac_da[39:32]
  case 3: No particular alignment needed
  (1) msb_offset = 32
***Allocating phv55[31:0] for egress_metadata.mac_da[31:0]

--------------
Working on:
tunnel_metadata.vnid <24 bits egress meta R W>
bits_will_need_to_parse = 24
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 24
extracted_bits = 24 while meta_fi.bit_width = 24
Parse state 0 (24 bits)
  tunnel_metadata.vnid [23:0]
----------------------------------------------------------------------------------------------------------
|         Name         | BW | Tagalong? |        Req         | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------------
| tunnel_metadata.vnid | 24 |   False   | [(8, 8), (16, 16)] |  -   |     -     |   None   |     4      |
----------------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(8, 8), (16, 16)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [8, 16]
  (3) msb_offset = 8
  case 3: No particular alignment needed
  (1) msb_offset = 16
>>Can pack using [8, 16] if open up 2 new containers.

Attempting to share...

  [8, 16]
  (3) msb_offset = 8
  (3) msb_offset = 16
>>Can pack using [8, 16] if open up 2 new containers.

>>Choose overlay option
  (3) msb_offset = 8
***Allocating phv103[7:0] for tunnel_metadata.vnid[23:16]
  case 3: No particular alignment needed
  (1) msb_offset = 16
***Allocating phv205[15:0] for tunnel_metadata.vnid[15:0]

--------------
Working on:
l2_metadata.lkp_mac_type <16 bits ingress meta R W>
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 16 while meta_fi.bit_width = 16
Parse state 0 (16 bits)
  l2_metadata.lkp_mac_type [15:0]
------------------------------------------------------------------------------------------------------
|           Name           | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------------
| l2_metadata.lkp_mac_type | 16 |   False   | [(16, 16)] |  -   |    [32]   |   None   |     5      |
------------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(16, 16)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  (3) msb_offset = 16
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
  (3) msb_offset = 16
>>Can pack using [16] if open up 1 new containers.

>>Choose overlay option
  (3) msb_offset = 16
***Allocating phv183[15:0] for l2_metadata.lkp_mac_type[15:0]

--------------
Working on:
l2_metadata.l2_nexthop <16 bits ingress meta R W>
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 16 while meta_fi.bit_width = 16
Parse state 0 (16 bits)
  l2_metadata.l2_nexthop [15:0]
----------------------------------------------------------------------------------------------------
|          Name          | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------
| l2_metadata.l2_nexthop | 16 |   False   | [(32, 16)] |  -   |     -     |   None   |     5      |
----------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(32, 16)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [32]
  (2a) msb_offset = 16
>> HEY!:  Adjusted msb_offset!
>>Can pack using [32] if open up 1 new containers.

Attempting to share...

  [32]
  (2a) msb_offset = 16
>> HEY!:  Adjusted msb_offset!
>>Can pack using [32] if open up 1 new containers.

>>Choose overlay option
  (2a) msb_offset = 16
>> HEY!:  Adjusted msb_offset!
***Allocating phv13[15:0] for l2_metadata.l2_nexthop[15:0]

--------------
Working on:
l3_metadata.fib_nexthop <16 bits ingress meta R W>
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 16 while meta_fi.bit_width = 16
Parse state 0 (16 bits)
  l3_metadata.fib_nexthop [15:0]
-----------------------------------------------------------------------------------------------------
|           Name          | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------
| l3_metadata.fib_nexthop | 16 |   False   | [(32, 16)] |  -   |     -     |   None   |     5      |
-----------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(32, 16)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [32]
  (2a) msb_offset = 16
>> HEY!:  Adjusted msb_offset!
>>Can pack using [32] if open up 1 new containers.

Attempting to share...

  [32]
  (2a) msb_offset = 16
>> HEY!:  Adjusted msb_offset!
>>Can pack using [32] if open up 1 new containers.

>>Choose overlay option
  (2a) msb_offset = 16
>> HEY!:  Adjusted msb_offset!
***Allocating phv12[15:0] for l3_metadata.fib_nexthop[15:0]

--------------
Working on:
acl_metadata.acl_nexthop <16 bits ingress meta R W>
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 16 while meta_fi.bit_width = 16
Parse state 0 (16 bits)
  acl_metadata.acl_nexthop [15:0]
------------------------------------------------------------------------------------------------------
|           Name           | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------------
| acl_metadata.acl_nexthop | 16 |   False   | [(32, 16)] |  -   |     -     |   None   |     5      |
------------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(32, 16)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [32]
  (2a) msb_offset = 16
>> HEY!:  Adjusted msb_offset!
>>Can pack using [32] if open up 1 new containers.

Attempting to share...

  [32]
  (2a) msb_offset = 16
>> HEY!:  Adjusted msb_offset!
>>Can pack using [32] if open up 1 new containers.

>>Choose overlay option
  (2a) msb_offset = 16
>> HEY!:  Adjusted msb_offset!
***Allocating phv14[15:0] for acl_metadata.acl_nexthop[15:0]

--------------
Working on:
acl_metadata.racl_nexthop <16 bits ingress meta R W>
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 16 while meta_fi.bit_width = 16
Parse state 0 (16 bits)
  acl_metadata.racl_nexthop [15:0]
-------------------------------------------------------------------------------------------------------
|            Name           | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------------
| acl_metadata.racl_nexthop | 16 |   False   | [(32, 16)] |  -   |     -     |   None   |     5      |
-------------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(32, 16)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [32]
  (2a) msb_offset = 16
>> HEY!:  Adjusted msb_offset!
>>Can pack using [32] if open up 1 new containers.

Attempting to share...

  [32]
>>Can pack using [32] if open up 0 new containers.

>>Choose share option
***Allocating phv13[31:16] for acl_metadata.racl_nexthop[15:0]

--------------
Working on:
hash_metadata.hash1 <16 bits ingress meta R W>
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 16 while meta_fi.bit_width = 16
Parse state 0 (16 bits)
  hash_metadata.hash1 [15:0]
-------------------------------------------------------------------------------------------------
|         Name        | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------
| hash_metadata.hash1 | 16 |   False   | [(16, 16)] | [16] |    [8]    |    1     |     5      |
-------------------------------------------------------------------------------------------------

max_split = 1, adj = False
required_packing = [(16, 16)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 16
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
  (3) msb_offset = 16
>>Can pack using [16] if open up 1 new containers.

>>Choose overlay option
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 16
***Allocating phv142[15:0] for hash_metadata.hash1[15:0]

--------------
Working on:
hash_metadata.hash2 <16 bits ingress meta R W>
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 16 while meta_fi.bit_width = 16
Parse state 0 (16 bits)
  hash_metadata.hash2 [15:0]
-------------------------------------------------------------------------------------------------
|         Name        | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------
| hash_metadata.hash2 | 16 |   False   | [(16, 16)] | [16] |    [8]    |    1     |     5      |
-------------------------------------------------------------------------------------------------

max_split = 1, adj = False
required_packing = [(16, 16)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 16
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
>>Can pack using [16] if open up 1 new containers.

>>Choose overlay option
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 16
***Allocating phv143[15:0] for hash_metadata.hash2[15:0]

--------------
Working on:
l3_metadata.l3_mtu_check <16 bits egress meta R W>
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 16 while meta_fi.bit_width = 16
Parse state 0 (16 bits)
  l3_metadata.l3_mtu_check [15:0]
------------------------------------------------------------------------------------------------------
|           Name           | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------------
| l3_metadata.l3_mtu_check | 16 |   False   | [(16, 16)] | [16] |    [8]    |    1     |     9      |
------------------------------------------------------------------------------------------------------

max_split = 1, adj = False
required_packing = [(16, 16)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 16
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
>>Can pack using [16] if open up 1 new containers.

>>Choose overlay option
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 16
***Allocating phv173[15:0] for l3_metadata.l3_mtu_check[15:0]

--------------
Working on:
egress_metadata.payload_length <16 bits egress meta R W>
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 16 while meta_fi.bit_width = 16
Parse state 0 (16 bits)
  egress_metadata.payload_length [15:0]
------------------------------------------------------------------------------------------------------------
|              Name              | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------------------
| egress_metadata.payload_length | 16 |   False   | [(16, 16)] | [16] |    [8]    |    1     |     9      |
------------------------------------------------------------------------------------------------------------

max_split = 1, adj = False
required_packing = [(16, 16)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 16
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
>>Can pack using [16] if open up 1 new containers.

>>Choose overlay option
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 16
***Allocating phv174[15:0] for egress_metadata.payload_length[15:0]

--------------
Working on:
l2_metadata.l2_src_move <14 bits ingress meta R W>
bits_will_need_to_parse = 14
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 14 while meta_fi.bit_width = 14
Parse state 0 (14 bits)
  l2_metadata.l2_src_move [13:0]
-----------------------------------------------------------------------------------------------------
|           Name          | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------
| l2_metadata.l2_src_move | 14 |   False   | [(16, 14)] | [16] |     -     |   None   |     3      |
-----------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(16, 14)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
>>Can pack using [16] if open up 1 new containers.

>>Choose overlay option
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
***Allocating phv139[13:0] for l2_metadata.l2_src_move[13:0]

--------------
Working on:
l3_metadata.same_bd_check <14 bits ingress meta R W>
bits_will_need_to_parse = 14
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 14 while meta_fi.bit_width = 14
Parse state 0 (14 bits)
  l3_metadata.same_bd_check [13:0]
-------------------------------------------------------------------------------------------------------
|            Name           | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------------
| l3_metadata.same_bd_check | 14 |   False   | [(16, 14)] | [16] |     -     |   None   |     2      |
-------------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(16, 14)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
>>Can pack using [16] if open up 1 new containers.

>>Choose overlay option
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
***Allocating phv138[13:0] for l3_metadata.same_bd_check[13:0]

--------------
Working on:
tunnel_metadata.vtep_ifindex <14 bits ingress meta R W>
bits_will_need_to_parse = 14
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 14 while meta_fi.bit_width = 14
Parse state 0 (14 bits)
  tunnel_metadata.vtep_ifindex [13:0]
----------------------------------------------------------------------------------------------------------
|             Name             | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------------
| tunnel_metadata.vtep_ifindex | 14 |   False   | [(16, 14)] |  -   |     -     |   None   |     3      |
----------------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(16, 14)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  (2a) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
  (2a) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
>>Can pack using [16] if open up 1 new containers.

>>Choose overlay option
  (2a) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
***Allocating phv140[13:0] for tunnel_metadata.vtep_ifindex[13:0]

--------------
Working on:
egress_metadata.outer_bd <14 bits egress meta R W>
bits_will_need_to_parse = 14
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 14 while meta_fi.bit_width = 14
Parse state 0 (14 bits)
  egress_metadata.outer_bd [13:0]
------------------------------------------------------------------------------------------------------
|           Name           | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------------
| egress_metadata.outer_bd | 14 |   False   | [(16, 14)] |  -   |     -     |   None   |     7      |
------------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(16, 14)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  (2a) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
  (2a) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
>>Can pack using [16] if open up 1 new containers.

>>Choose overlay option
  (2a) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
***Allocating phv158[13:0] for egress_metadata.outer_bd[13:0]

--------------
Working on:
egress_metadata.bd <14 bits egress meta R W>
bits_will_need_to_parse = 14
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 14 while meta_fi.bit_width = 14
Parse state 0 (14 bits)
  egress_metadata.bd [13:0]
------------------------------------------------------------------------------------------------
|        Name        | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------
| egress_metadata.bd | 14 |   False   | [(16, 14)] |  -   |     -     |   None   |     7      |
------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(16, 14)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  (2a) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
  (2a) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
>>Can pack using [16] if open up 1 new containers.

>>Choose overlay option
  (2a) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
***Allocating phv157[13:0] for egress_metadata.bd[13:0]

--------------
Working on:
ingress_metadata.outer_bd <14 bits egress meta R>
bits_will_need_to_parse = 14
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 14 while meta_fi.bit_width = 14
Parse state 0 (14 bits)
  ingress_metadata.outer_bd [13:0]
-------------------------------------------------------------------------------------------------------
|            Name           | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------------
| ingress_metadata.outer_bd | 14 |   False   | [(16, 14)] | [16] |     -     |   None   |     7      |
-------------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(16, 14)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
>>Can pack using [16] if open up 1 new containers.

>>Choose overlay option
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
***Allocating phv154[13:0] for ingress_metadata.outer_bd[13:0]

--------------
Working on:
egress_metadata.same_bd_check <14 bits egress meta R W>
bits_will_need_to_parse = 14
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 14 while meta_fi.bit_width = 14
Parse state 0 (14 bits)
  egress_metadata.same_bd_check [13:0]
-----------------------------------------------------------------------------------------------------------
|              Name             | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------------
| egress_metadata.same_bd_check | 14 |   False   | [(16, 14)] | [16] |     -     |   None   |     7      |
-----------------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(16, 14)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
>>Can pack using [16] if open up 1 new containers.

>>Choose overlay option
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
***Allocating phv156[13:0] for egress_metadata.same_bd_check[13:0]

--------------
Working on:
ig_intr_md_for_tm.level1_mcast_hash <13 bits ingress imeta W>
bits_will_need_to_parse = 13
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 13 while meta_fi.bit_width = 13
Parse state 0 (13 bits)
  ig_intr_md_for_tm.level1_mcast_hash [12:0]
-----------------------------------------------------------------------------------------------------------------
|                 Name                | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------------------
| ig_intr_md_for_tm.level1_mcast_hash | 13 |   False   | [(16, 13)] |  -   |    [8]    |    1     |     5      |
-----------------------------------------------------------------------------------------------------------------

max_split = 1, adj = False
required_packing = [(16, 13)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 13
>> HEY!:  Adjusted msb_offset!
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
  (2a) msb_offset = 13
>> HEY!:  Adjusted msb_offset!
>>Can pack using [16] if open up 1 new containers.

>>Choose overlay option
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 13
>> HEY!:  Adjusted msb_offset!
***Allocating phv137[12:0] for ig_intr_md_for_tm.level1_mcast_hash[12:0]

--------------
Working on:
ig_intr_md_for_tm.level2_mcast_hash <13 bits ingress imeta W>
bits_will_need_to_parse = 13
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 13 while meta_fi.bit_width = 13
Parse state 0 (13 bits)
  ig_intr_md_for_tm.level2_mcast_hash [12:0]
-----------------------------------------------------------------------------------------------------------------
|                 Name                | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------------------
| ig_intr_md_for_tm.level2_mcast_hash | 13 |   False   | [(16, 13)] | [16] |    [8]    |    1     |     5      |
-----------------------------------------------------------------------------------------------------------------

max_split = 1, adj = False
required_packing = [(16, 13)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 13
>> HEY!:  Adjusted msb_offset!
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
>>Can pack using [16] if open up 1 new containers.

>>Choose overlay option
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 13
>> HEY!:  Adjusted msb_offset!
***Allocating phv136[12:0] for ig_intr_md_for_tm.level2_mcast_hash[12:0]

--------------
Working on:
ig_intr_md_for_tm.ucast_egress_port <9 bits ingress imeta W>
bits_will_need_to_parse = 9
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 9 while meta_fi.bit_width = 9
Parse state 0 (9 bits)
  ig_intr_md_for_tm.ucast_egress_port [8:0]
----------------------------------------------------------------------------------------------------------------
|                 Name                | BW | Tagalong? |    Req    | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------------------
| ig_intr_md_for_tm.ucast_egress_port | 9  |   False   | [(32, 9)] |  -   |    [8]    |    1     |     2      |
----------------------------------------------------------------------------------------------------------------

max_split = 1, adj = False
required_packing = [(32, 9)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [32]
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 9
>> HEY!:  Adjusted msb_offset!
>>Can pack using [32] if open up 1 new containers.

Attempting to share...

  [32]
  (2a) msb_offset = 9
>> HEY!:  Adjusted msb_offset!
>>Can pack using [32] if open up 1 new containers.

>>Choose overlay option
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 9
>> HEY!:  Adjusted msb_offset!
***Allocating phv57[8:0] for ig_intr_md_for_tm.ucast_egress_port[8:0]

--------------
Working on:
tunnel_metadata.inner_ip_proto <8 bits egress meta R W>
bits_will_need_to_parse = 8
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
extracted_bits = 8 while meta_fi.bit_width = 8
Parse state 0 (8 bits)
  tunnel_metadata.inner_ip_proto [7:0]
----------------------------------------------------------------------------------------------------------
|              Name              | BW | Tagalong? |   Req    | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------------
| tunnel_metadata.inner_ip_proto | 8  |   False   | [(8, 8)] |  -   |     -     |   None   |     5      |
----------------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(8, 8)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [8]
  (3) msb_offset = 8
>>Can pack using [8] if open up 0 new containers.
  (3) msb_offset = 8
***Allocating phv95[7:0] for tunnel_metadata.inner_ip_proto[7:0]

--------------
Working on:
l3_metadata.lkp_ip_version <4 bits ingress meta R W>
bits_will_need_to_parse = 4
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
extracted_bits = 4 while meta_fi.bit_width = 4
Parse state 0 (4 bits)
  l3_metadata.lkp_ip_version [3:0]
-------------------------------------------------------------------------------------------------------
|            Name            | BW | Tagalong? |    Req    | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------------
| l3_metadata.lkp_ip_version | 4  |   False   | [(16, 4)] |  -   |     -     |   None   |     5      |
-------------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(16, 4)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  (2a) msb_offset = 16
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
  (2a) msb_offset = 16
>>Can pack using [16] if open up 1 new containers.

>>Choose overlay option
  (2a) msb_offset = 16
***Allocating phv182[15:12] for l3_metadata.lkp_ip_version[3:0]

--------------
Working on:
tunnel_metadata.tunnel_if_check <1 bits ingress meta R W>
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
extracted_bits = 1 while meta_fi.bit_width = 1
Parse state 0 (1 bits)
  tunnel_metadata.tunnel_if_check [0:0]
------------------------------------------------------------------------------------------------------------
|               Name              | BW | Tagalong? |    Req    | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------------------
| tunnel_metadata.tunnel_if_check | 1  |   False   | [(16, 1)] | [8]  |     -     |   None   |     2      |
------------------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(16, 1)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  case 2: looking at allowed start bits [15]
    final start_bit = 15
  (1) msb_offset = 16
>>Can pack using [16] if open up 0 new containers.
  case 2: looking at allowed start bits [15]
    final start_bit = 15
  (1) msb_offset = 16
***Allocating phv140[15:15] for tunnel_metadata.tunnel_if_check[0:0]

--------------
Working on:
eg_intr_md_for_oport.capture_tstamp_on_tx <1 bits egress imeta W>
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
extracted_bits = 1 while meta_fi.bit_width = 1
Parse state 0 (1 bits)
  eg_intr_md_for_oport.capture_tstamp_on_tx [0:0]
----------------------------------------------------------------------------------------------------------------------
|                    Name                   | BW | Tagalong? |    Req    | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------------------------
| eg_intr_md_for_oport.capture_tstamp_on_tx | 1  |   False   | [(16, 1)] | [8]  |     -     |    1     |     2      |
----------------------------------------------------------------------------------------------------------------------

max_split = 1, adj = False
required_packing = [(16, 1)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  case 2: looking at allowed start bits [0, 1, 2, 3, 4, 5, 6, 7]
    final start_bit = 7
  (1) msb_offset = 8
>> HEY!:  Adjusted msb_offset!
>>Can pack using [16] if open up 0 new containers.
  case 2: looking at allowed start bits [0, 1, 2, 3, 4, 5, 6, 7]
    final start_bit = 7
  (1) msb_offset = 8
>> HEY!:  Adjusted msb_offset!
***Allocating phv151[7:7] for eg_intr_md_for_oport.capture_tstamp_on_tx[0:0]

--------------
Working on:
egress_metadata.routed <1 bits egress meta R W>
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
extracted_bits = 1 while meta_fi.bit_width = 1
Parse state 0 (1 bits)
  egress_metadata.routed [0:0]
---------------------------------------------------------------------------------------------------
|          Name          | BW | Tagalong? |    Req    | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------
| egress_metadata.routed | 1  |   False   | [(16, 1)] |  -   |     -     |   None   |     3      |
---------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(16, 1)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  (2a) msb_offset = 16
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
>>Can pack using [16] if open up 0 new containers.

>>Choose share option
***Allocating phv154[14:14] for egress_metadata.routed[0:0]

--------------
Working on:
l3_metadata.outer_routed <1 bits egress meta R>
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
extracted_bits = 1 while meta_fi.bit_width = 1
Parse state 0 (1 bits)
  l3_metadata.outer_routed [0:0]
-----------------------------------------------------------------------------------------------------
|           Name           | BW | Tagalong? |    Req    | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------
| l3_metadata.outer_routed | 1  |   False   | [(16, 1)] |  -   |     -     |   None   |     3      |
-----------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(16, 1)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  (2a) msb_offset = 16
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
>>Can pack using [16] if open up 0 new containers.

>>Choose share option
***Allocating phv158[14:14] for l3_metadata.outer_routed[0:0]
Allocation state after promised meta allocated:
Allocation state: Final Allocation
------------------------------------------------------------------------------
|       PHV Group        | Containers Used |   Bits Used   | Bits Available |
| (container bit widths) |     (% used)    |    (% used)   |                |
------------------------------------------------------------------------------
|         0 (32)         |   15 (93.75%)   |  448 (87.50%) |      512       |
|         1 (32)         |   16 (100.00%)  | 512 (100.00%) |      512       |
|         2 (32)         |   16 (100.00%)  | 512 (100.00%) |      512       |
|         3 (32)         |   10 (62.50%)   |  297 (58.01%) |      512       |
|    Total for 32 bit    |   57 (89.06%)   | 1769 (86.38%) |      2048      |
|                        |                 |               |                |
|         4 (8)          |    5 (31.25%)   |  40 (31.25%)  |      128       |
|         5 (8)          |   16 (100.00%)  | 128 (100.00%) |      128       |
|         6 (8)          |   16 (100.00%)  | 128 (100.00%) |      128       |
|         7 (8)          |   14 (87.50%)   |  112 (87.50%) |      128       |
|    Total for 8 bit     |   51 (79.69%)   |  408 (79.69%) |      512       |
|                        |                 |               |                |
|         8 (16)         |   16 (100.00%)  |  245 (95.70%) |      256       |
|         9 (16)         |   16 (100.00%)  |  250 (97.66%) |      256       |
|        10 (16)         |   16 (100.00%)  | 256 (100.00%) |      256       |
|        11 (16)         |   16 (100.00%)  |  244 (95.31%) |      256       |
|        12 (16)         |   14 (87.50%)   |  224 (87.50%) |      256       |
|        13 (16)         |    3 (18.75%)   |  48 (18.75%)  |      256       |
|    Total for 16 bit    |   81 (84.38%)   | 1267 (82.49%) |      1536      |
|                        |                 |               |                |
|       14 (32) T        |   14 (87.50%)   |  448 (87.50%) |      512       |
|       15 (32) T        |    5 (31.25%)   |  160 (31.25%) |      512       |
|    Total for 32 bit    |   19 (59.38%)   |  608 (59.38%) |      1024      |
|                        |                 |               |                |
|        16 (8) T        |   15 (93.75%)   |  120 (93.75%) |      128       |
|        17 (8) T        |    8 (50.00%)   |  64 (50.00%)  |      128       |
|    Total for 8 bit     |   23 (71.88%)   |  184 (71.88%) |      256       |
|                        |                 |               |                |
|       18 (16) T        |   15 (93.75%)   |  240 (93.75%) |      256       |
|       19 (16) T        |    6 (37.50%)   |  96 (37.50%)  |      256       |
|       20 (16) T        |    0 (0.00%)    |   0 (0.00%)   |      256       |
|    Total for 16 bit    |   21 (43.75%)   |  336 (43.75%) |      768       |
|                        |                 |               |                |
|       MAU total        |   189 (84.38%)  | 3444 (84.08%) |      4096      |
|     Tagalong total     |   63 (56.25%)   | 1128 (55.08%) |      2048      |
|     Overall total      |   252 (75.00%)  | 4572 (74.41%) |      6144      |
------------------------------------------------------------------------------

Allocation state difference after promised meta allocated:
Allocation state: Diff
-----------------------------------------------------------------------------
|       PHV Group        | Containers Used |  Bits Used   | Bits Available |
| (container bit widths) |     (% used)    |   (% used)   |                |
-----------------------------------------------------------------------------
|         0 (32)         |    3 (18.75%)   | 64 (12.50%)  |      512       |
|         1 (32)         |    0 (0.00%)    |  0 (0.00%)   |      512       |
|         2 (32)         |    0 (0.00%)    |  0 (0.00%)   |      512       |
|         3 (32)         |    2 (12.50%)   |  41 (8.01%)  |      512       |
|    Total for 32 bit    |    5 (7.81%)    | 105 (5.13%)  |      2048      |
|                        |                 |              |                |
|         4 (8)          |    0 (0.00%)    |  0 (0.00%)   |      128       |
|         5 (8)          |    0 (0.00%)    |  0 (0.00%)   |      128       |
|         6 (8)          |    2 (12.50%)   | 16 (12.50%)  |      128       |
|         7 (8)          |    1 (6.25%)    |  8 (6.25%)   |      128       |
|    Total for 8 bit     |    3 (4.69%)    |  24 (4.69%)  |      512       |
|                        |                 |              |                |
|         8 (16)         |    7 (43.75%)   | 101 (39.45%) |      256       |
|         9 (16)         |    4 (25.00%)   | 58 (22.66%)  |      256       |
|        10 (16)         |    2 (12.50%)   | 32 (12.50%)  |      256       |
|        11 (16)         |    2 (12.50%)   |  20 (7.81%)  |      256       |
|        12 (16)         |    1 (6.25%)    |  16 (6.25%)  |      256       |
|        13 (16)         |    0 (0.00%)    |  0 (0.00%)   |      256       |
|    Total for 16 bit    |   16 (16.67%)   | 227 (14.78%) |      1536      |
|                        |                 |              |                |
|       14 (32) T        |    0 (0.00%)    |  0 (0.00%)   |      512       |
|       15 (32) T        |    0 (0.00%)    |  0 (0.00%)   |      512       |
|    Total for 32 bit    |    0 (0.00%)    |  0 (0.00%)   |      1024      |
|                        |                 |              |                |
|        16 (8) T        |    0 (0.00%)    |  0 (0.00%)   |      128       |
|        17 (8) T        |    0 (0.00%)    |  0 (0.00%)   |      128       |
|    Total for 8 bit     |    0 (0.00%)    |  0 (0.00%)   |      256       |
|                        |                 |              |                |
|       18 (16) T        |    0 (0.00%)    |  0 (0.00%)   |      256       |
|       19 (16) T        |    0 (0.00%)    |  0 (0.00%)   |      256       |
|       20 (16) T        |    0 (0.00%)    |  0 (0.00%)   |      256       |
|    Total for 16 bit    |    0 (0.00%)    |  0 (0.00%)   |      768       |
|                        |                 |              |                |
|       MAU total        |   24 (10.71%)   | 356 (8.69%)  |      4096      |
|     Tagalong total     |    0 (0.00%)    |  0 (0.00%)   |      2048      |
|     Overall total      |    24 (7.14%)   | 356 (5.79%)  |      6144      |
-----------------------------------------------------------------------------

>> 'Promised' metadata allocation took 13.90 seconds
Sorted metadata field instances to allocate: 10 / 97 bits (92 ingress bits and 5 egress bits)
     0: l2_metadata.same_if_check (ingress) (highly=1, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=0, best_share_pack=0, max_split=14, bit_width=14, initial_usage_read=1, earliest_use=0, latest_use=9)
     1: acl_metadata.port_lag_label (ingress) (highly=1, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=0, best_share_pack=0, max_split=1, bit_width=16, initial_usage_read=1, earliest_use=0, latest_use=9)
     2: multicast_metadata.multicast_route_mc_index (ingress) (highly=1, mau_group_size=3, max_overlay=0, best_overlay_pack=0, max_share=0, best_share_pack=0, max_split=1, bit_width=16, initial_usage_read=3, earliest_use=5, latest_use=7)
     3: multicast_metadata.multicast_bridge_mc_index (ingress) (highly=1, mau_group_size=3, max_overlay=0, best_overlay_pack=0, max_share=0, best_share_pack=0, max_split=1, bit_width=16, initial_usage_read=3, earliest_use=4, latest_use=7)
     4: l2_metadata.bd_stats_idx (ingress) (highly=1, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=0, best_share_pack=0, max_split=1, bit_width=14, initial_usage_read=2, earliest_use=0, latest_use=7)
     5: multicast_metadata.mcast_rpf_group (ingress) (highly=1, mau_group_size=2, max_overlay=0, best_overlay_pack=0, max_share=0, best_share_pack=0, max_split=14, bit_width=14, initial_usage_read=3, earliest_use=5, latest_use=7)
     6: eg_intr_md_for_oport.drop_ctl (egress) (highly=1, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=0, best_share_pack=0, max_split=1, bit_width=3, initial_usage_read=2, earliest_use=8, latest_use=12)
     7: egress_metadata.port_type (egress) (highly=1, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=0, best_share_pack=0, max_split=1, bit_width=2, initial_usage_read=1, earliest_use=2, latest_use=7)
     8: ig_intr_md_for_tm.disable_ucast_cutthru (ingress) (highly=1, mau_group_size=2, max_overlay=0, best_overlay_pack=0, max_share=0, best_share_pack=0, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=8, latest_use=12)
     9: l2_metadata.non_ip_packet (ingress) (highly=1, mau_group_size=2, max_overlay=0, best_overlay_pack=0, max_share=0, best_share_pack=0, max_split=1, bit_width=1, initial_usage_read=4, earliest_use=2, latest_use=8)
>> Free metadata analysis took 0.00 seconds

---------------------------------------
Working on:
l2_metadata.same_if_check <14 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 0 (0 bits)
bits_will_need_to_parse = 14
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (14 bits)
  l2_metadata.same_if_check [13:0]
-------------------------------------------------------------------------------------------------------
|            Name           | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------------
| l2_metadata.same_if_check | 14 |   False   | [(16, 14)] |  -   |     -     |   None   |     1      |
-------------------------------------------------------------------------------------------------------

  req packing: [[(16, 14)]]
  disallowed packing: [None]
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 2 and promised 1 -- ingress promised 1 and remain 1 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv206 -- fails False
  Group 13 16 bits -- avail 13 and promised 1 -- ingress promised 1 and remain 7 and req 0 -- egress promised 0 and remain 12 and req 0 -- as if deparsed False -- container_to_use phv216 -- fails False
Metadata instance: l2_metadata.same_if_check <14 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv206[13:0] for l2_metadata.same_if_check[13:0]

---------------------------------------
Working on:
acl_metadata.port_lag_label <16 bits ingress meta R W>
max_split = 1, adj = False
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 0 (0 bits)
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (16 bits)
  acl_metadata.port_lag_label [15:0]
--------------------------------------------------------------------------------------------------
|             Name            | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------------
| acl_metadata.port_lag_label | 16 |   False   |  -  | [16] |    [8]    |    1     |     1      |
--------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [[8]]
  Group 0 32 bits -- avail 1 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv15 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 6 and promised 1 -- ingress promised 1 and remain 5 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 1 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv207 -- fails False
  Group 13 16 bits -- avail 13 and promised 1 -- ingress promised 1 and remain 7 and req 0 -- egress promised 0 and remain 12 and req 0 -- as if deparsed False -- container_to_use phv216 -- fails False
Metadata instance: acl_metadata.port_lag_label <16 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv207[15:0] for acl_metadata.port_lag_label[15:0]

---------------------------------------
Working on:
multicast_metadata.multicast_route_mc_index <16 bits ingress meta R W>
max_split = 1, adj = False
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 0 (0 bits)
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (16 bits)
  multicast_metadata.multicast_route_mc_index [15:0]
------------------------------------------------------------------------------------------------------------------
|                     Name                    | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------------------------
| multicast_metadata.multicast_route_mc_index | 16 |   False   |  -  | [16] |    [8]    |    1     |     3      |
------------------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [[8]]
  Group 0 32 bits -- avail 1 and promised 2 -- ingress promised 3 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 3 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 6 and promised 3 -- ingress promised 3 and remain 3 and req 1 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv59 -- fails False
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 3 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 3 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 3 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 3 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 13 and promised 3 -- ingress promised 3 and remain 5 and req 1 -- egress promised 0 and remain 10 and req 0 -- as if deparsed False -- container_to_use phv217 -- fails False
Metadata instance: multicast_metadata.multicast_route_mc_index <16 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv217[15:0] for multicast_metadata.multicast_route_mc_index[15:0]

---------------------------------------
Working on:
multicast_metadata.multicast_bridge_mc_index <16 bits ingress meta R W>
max_split = 1, adj = False
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 0 (0 bits)
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (16 bits)
  multicast_metadata.multicast_bridge_mc_index [15:0]
--------------------------------------------------------------------------------------------------------------------------
|                     Name                     | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------------------------------------
| multicast_metadata.multicast_bridge_mc_index | 16 |   False   | [(16, 16)] | [16] |    [8]    |    1     |     3      |
--------------------------------------------------------------------------------------------------------------------------

  req packing: [[(16, 16)]]
  disallowed packing: [[8]]
  Group 13 16 bits -- avail 12 and promised 2 -- ingress promised 2 and remain 5 and req 1 -- egress promised 0 and remain 10 and req 0 -- as if deparsed False -- container_to_use phv218 -- fails False
Metadata instance: multicast_metadata.multicast_bridge_mc_index <16 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv218[15:0] for multicast_metadata.multicast_bridge_mc_index[15:0]

---------------------------------------
Working on:
l2_metadata.bd_stats_idx <14 bits ingress meta R W>
max_split = 1, adj = False
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 0 (0 bits)
bits_will_need_to_parse = 14
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (14 bits)
  l2_metadata.bd_stats_idx [13:0]
-----------------------------------------------------------------------------------------------
|           Name           | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------
| l2_metadata.bd_stats_idx | 14 |   False   |  -  | [16] |    [8]    |    1     |     1      |
-----------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [[8]]
  Group 0 32 bits -- avail 1 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv15 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 6 and promised 1 -- ingress promised 1 and remain 5 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 11 and promised 2 -- ingress promised 2 and remain 4 and req 1 -- egress promised 0 and remain 9 and req 0 -- as if deparsed False -- container_to_use phv219 -- fails False
Metadata instance: l2_metadata.bd_stats_idx <14 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv219[13:0] for l2_metadata.bd_stats_idx[13:0]

---------------------------------------
Working on:
multicast_metadata.mcast_rpf_group <14 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 0 (0 bits)
bits_will_need_to_parse = 14
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (14 bits)
  multicast_metadata.mcast_rpf_group [13:0]
---------------------------------------------------------------------------------------------------------
|                Name                | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------------
| multicast_metadata.mcast_rpf_group | 14 |   False   |  -  | [16] |     -     |   None   |     2      |
---------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 1 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv15 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 2 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 6 and promised 2 -- ingress promised 2 and remain 4 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv59 -- fails False
  Group 4 8 bits -- avail 11 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 2 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 2 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 2 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 2 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 10 and promised 3 -- ingress promised 3 and remain 2 and req 1 -- egress promised 0 and remain 7 and req 0 -- as if deparsed False -- container_to_use phv221 -- fails False
Metadata instance: multicast_metadata.mcast_rpf_group <14 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv221[13:0] for multicast_metadata.mcast_rpf_group[13:0]

---------------------------------------
Working on:
eg_intr_md_for_oport.drop_ctl <3 bits egress imeta W>
max_split = 1, adj = False
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 0 (0 bits)
bits_will_need_to_parse = 3
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (3 bits)
  eg_intr_md_for_oport.drop_ctl [2:0]
----------------------------------------------------------------------------------------------------
|              Name             | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------
| eg_intr_md_for_oport.drop_ctl | 3  |   False   |  -  | [8]  |     -     |    1     |     1      |
----------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 1 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 1 -- as if deparsed True -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 1 -- as if deparsed True -- container_to_use None -- fails True
  Group 3 32 bits -- avail 6 and promised 0 -- ingress promised 0 and remain 6 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed True -- container_to_use phv55 -- fails False
  Group 4 8 bits -- avail 11 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv82 -- fails False
  Group 6 8 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv102 -- fails False
  Group 7 8 bits -- avail 2 and promised 0 -- ingress promised 0 and remain 2 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed True -- container_to_use phv117 -- fails False
  Group 9 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv157 -- fails False
  Group 10 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv172 -- fails False
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 1 -- as if deparsed True -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 1 -- as if deparsed True -- container_to_use None -- fails True
  Group 13 16 bits -- avail 9 and promised 3 -- ingress promised 2 and remain 2 and req 1 -- egress promised 1 and remain 6 and req 1 -- as if deparsed True -- container_to_use phv211 -- fails False
Metadata instance: eg_intr_md_for_oport.drop_ctl <3 bits egress imeta W>
>>req_alignment = None
>>allowed_container_start_bits = [0, 1, 2, 3, 4, 5]
>>req_container = None
  case 2: looking at allowed start bits [0, 1, 2, 3, 4, 5]
    final start_bit = 5
  (1) msb_offset = 8
***Allocating phv82[7:5] for eg_intr_md_for_oport.drop_ctl[2:0]

---------------------------------------
Working on:
egress_metadata.port_type <2 bits egress meta R W>
max_split = 1, adj = False
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 0 (0 bits)
bits_will_need_to_parse = 2
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (2 bits)
  egress_metadata.port_type [1:0]
------------------------------------------------------------------------------------------------
|            Name           | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------
| egress_metadata.port_type | 2  |   False   |  -  | [8]  |     -     |    1     |     1      |
------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 1 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 6 and promised 1 -- ingress promised 0 and remain 5 and req 0 -- egress promised 1 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv60 -- fails False
  Group 4 8 bits -- avail 11 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 2 and promised 1 -- ingress promised 0 and remain 1 and req 0 -- egress promised 1 and remain 1 and req 0 -- as if deparsed False -- container_to_use phv126 -- fails False
  Group 8 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 9 and promised 3 -- ingress promised 2 and remain 2 and req 1 -- egress promised 1 and remain 6 and req 0 -- as if deparsed False -- container_to_use phv211 -- fails False
Metadata instance: egress_metadata.port_type <2 bits egress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv126[1:0] for egress_metadata.port_type[1:0]

---------------------------------------
Working on:
ig_intr_md_for_tm.disable_ucast_cutthru <1 bits ingress imeta W>
max_split = 1, adj = False
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 0 (0 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  ig_intr_md_for_tm.disable_ucast_cutthru [0:0]
--------------------------------------------------------------------------------------------------------------
|                   Name                  | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------------------------
| ig_intr_md_for_tm.disable_ucast_cutthru | 1  |   False   |  -  | [8]  |     -     |    1     |     2      |
--------------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 1 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv7 -- fails False
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv37 -- fails True
  Group 3 32 bits -- avail 6 and promised 2 -- ingress promised 2 and remain 4 and req 1 -- egress promised 0 and remain 4 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised 2 -- ingress promised 2 and remain 9 and req 1 -- egress promised 0 and remain 8 and req 0 -- as if deparsed True -- container_to_use phv69 -- fails False
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv105 -- fails True
  Group 7 8 bits -- avail 1 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv121 -- fails False
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 2 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 2 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv201 -- fails True
  Group 13 16 bits -- avail 9 and promised 3 -- ingress promised 3 and remain 1 and req 2 -- egress promised 0 and remain 6 and req 0 -- as if deparsed True -- container_to_use phv220 -- fails False
Metadata instance: ig_intr_md_for_tm.disable_ucast_cutthru <1 bits ingress imeta W>
>>req_alignment = None
>>allowed_container_start_bits = [0, 1, 2, 3, 4, 5, 6, 7]
>>req_container = None
  case 2: looking at allowed start bits [0, 1, 2, 3, 4, 5, 6, 7]
    final start_bit = 7
  (1) msb_offset = 8
***Allocating phv121[7:7] for ig_intr_md_for_tm.disable_ucast_cutthru[0:0]

---------------------------------------
Working on:
l2_metadata.non_ip_packet <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 0 (0 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  l2_metadata.non_ip_packet [0:0]
-----------------------------------------------------------------------------------------------------
|            Name           | BW | Tagalong? |   Req    | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------
| l2_metadata.non_ip_packet | 1  |   False   | [(8, 1)] | [8]  |     -     |   None   |     2      |
-----------------------------------------------------------------------------------------------------

  req packing: [[(8, 1)]]
  disallowed packing: [None]
  Group 7 8 bits -- avail 1 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv127 -- fails False
Metadata instance: l2_metadata.non_ip_packet <1 bits ingress meta R W>
>>req_alignment = 7
>>allowed_container_start_bits = [7]
>>req_container = None
  case 2: looking at allowed start bits [7]
    final start_bit = 7
  (1) msb_offset = 8
***Allocating phv127[7:7] for l2_metadata.non_ip_packet[0:0]
>> Free metadata allocation took 1.79 seconds
>>Event 'pa_pov' at time 1573972387.85
   Took 148.73 seconds

-----------------------------------------------
  Allocating POV
-----------------------------------------------
Allocation Step
Allocation state: Final Allocation
------------------------------------------------------------------------------
|       PHV Group        | Containers Used |   Bits Used   | Bits Available |
| (container bit widths) |     (% used)    |    (% used)   |                |
------------------------------------------------------------------------------
|         0 (32)         |   15 (93.75%)   |  448 (87.50%) |      512       |
|         1 (32)         |   16 (100.00%)  | 512 (100.00%) |      512       |
|         2 (32)         |   16 (100.00%)  | 512 (100.00%) |      512       |
|         3 (32)         |   10 (62.50%)   |  297 (58.01%) |      512       |
|    Total for 32 bit    |   57 (89.06%)   | 1769 (86.38%) |      2048      |
|                        |                 |               |                |
|         4 (8)          |    5 (31.25%)   |  40 (31.25%)  |      128       |
|         5 (8)          |   16 (100.00%)  | 128 (100.00%) |      128       |
|         6 (8)          |   16 (100.00%)  | 128 (100.00%) |      128       |
|         7 (8)          |   16 (100.00%)  |  115 (89.84%) |      128       |
|    Total for 8 bit     |   53 (82.81%)   |  411 (80.27%) |      512       |
|                        |                 |               |                |
|         8 (16)         |   16 (100.00%)  |  245 (95.70%) |      256       |
|         9 (16)         |   16 (100.00%)  |  250 (97.66%) |      256       |
|        10 (16)         |   16 (100.00%)  | 256 (100.00%) |      256       |
|        11 (16)         |   16 (100.00%)  |  244 (95.31%) |      256       |
|        12 (16)         |   16 (100.00%)  |  254 (99.22%) |      256       |
|        13 (16)         |    7 (43.75%)   |  108 (42.19%) |      256       |
|    Total for 16 bit    |   87 (90.62%)   | 1357 (88.35%) |      1536      |
|                        |                 |               |                |
|       14 (32) T        |   14 (87.50%)   |  448 (87.50%) |      512       |
|       15 (32) T        |    5 (31.25%)   |  160 (31.25%) |      512       |
|    Total for 32 bit    |   19 (59.38%)   |  608 (59.38%) |      1024      |
|                        |                 |               |                |
|        16 (8) T        |   15 (93.75%)   |  120 (93.75%) |      128       |
|        17 (8) T        |    8 (50.00%)   |  64 (50.00%)  |      128       |
|    Total for 8 bit     |   23 (71.88%)   |  184 (71.88%) |      256       |
|                        |                 |               |                |
|       18 (16) T        |   15 (93.75%)   |  240 (93.75%) |      256       |
|       19 (16) T        |    6 (37.50%)   |  96 (37.50%)  |      256       |
|       20 (16) T        |    0 (0.00%)    |   0 (0.00%)   |      256       |
|    Total for 16 bit    |   21 (43.75%)   |  336 (43.75%) |      768       |
|                        |                 |               |                |
|       MAU total        |   197 (87.95%)  | 3537 (86.35%) |      4096      |
|     Tagalong total     |   63 (56.25%)   | 1128 (55.08%) |      2048      |
|     Overall total      |   260 (77.38%)  | 4665 (75.93%) |      6144      |
------------------------------------------------------------------------------

  base_name = vlan_tag_ (ingress)
max_push = 0, max_pop = 0
  base_name = mpls (ingress)
max_push = 0, max_pop = 0
  base_name = vlan_tag_ (egress)
max_push = 0, max_pop = 0
  base_name = mpls (egress)
max_push = 3, max_pop = 0
Call to _allocate_pov_helper for:
  --validity_check--one0--mpls (egress)
  --validity_check--one1--mpls (egress)
  --validity_check--one2--mpls (egress)
  --validity_check--mpls[0] (egress)
  --validity_check--mpls[1] (egress)
  --validity_check--mpls[2] (egress)
  Best pack group: (16)

Looking for container to share POV bit in from already allocated containers for POV.
Container availability (not used yet for POV): total 22 / partial 11

Looking for container to share POV bit in from already allocated containers that have not been used for POV.
>>Choose container phv60, starting at container bit 0, which results in 26 bits still available (unused = 32 and could fit = 17).
  >> Decided to allocate new container
Required container phv60
***Allocating phv60[0:0] for --validity_check--mpls[2][0:0]
***Allocating phv60[0:0] for --validity_check--mpls[1][0:0]
***Allocating phv60[0:0] for --validity_check--mpls[0][0:0]
***Allocating phv60[0:0] for --validity_check--one2--mpls[0:0]
***Allocating phv60[0:0] for --validity_check--one1--mpls[0:0]
***Allocating phv60[0:0] for --validity_check--one0--mpls[0:0]
***Allocating phv60[6:6] for --validity_check--vlan_tag_[0][0:0]
***Allocating phv60[7:7] for --validity_check--ethernet[0:0]
***Allocating phv60[8:8] for --validity_check--llc_header[0:0]
***Allocating phv60[9:9] for --validity_check--snap_header[0:0]
***Allocating phv60[10:10] for --validity_check--ipv4[0:0]
***Allocating phv60[11:11] for --validity_check--ipv4_option_32b[0:0]
***Allocating phv60[12:12] for --validity_check--ipv6[0:0]
***Allocating phv60[13:13] for --validity_check--igmp[0:0]
***Allocating phv60[14:14] for --validity_check--gre[0:0]
***Allocating phv60[15:15] for --validity_check--nvgre[0:0]
***Allocating phv60[16:16] for --validity_check--erspan_t3_header[0:0]
Sorted POV field instances to allocate (with best pack): 58
    0: --validity_check--ethernet (ingress)  -- max pov share 30 / best pack 29
    1: --validity_check--llc_header (ingress)  -- max pov share 30 / best pack 29
    2: --validity_check--snap_header (ingress)  -- max pov share 30 / best pack 29
    3: --validity_check--ipv4 (ingress)  -- max pov share 30 / best pack 29
    4: --validity_check--ipv4_option_32b (ingress)  -- max pov share 30 / best pack 29
    5: --validity_check--ipv6 (ingress)  -- max pov share 30 / best pack 29
    6: --validity_check--icmp (ingress)  -- max pov share 30 / best pack 29
    7: --validity_check--igmp (ingress)  -- max pov share 30 / best pack 29
    8: --validity_check--tcp (ingress)  -- max pov share 30 / best pack 29
    9: --validity_check--udp (ingress)  -- max pov share 30 / best pack 29
   10: --validity_check--gre (ingress)  -- max pov share 30 / best pack 29
   11: --validity_check--nvgre (ingress)  -- max pov share 30 / best pack 29
   12: --validity_check--inner_ethernet (ingress)  -- max pov share 30 / best pack 29
   13: --validity_check--inner_ipv4 (ingress)  -- max pov share 30 / best pack 29
   14: --validity_check--inner_ipv6 (ingress)  -- max pov share 30 / best pack 29
   15: --validity_check--erspan_t3_header (ingress)  -- max pov share 30 / best pack 29
   16: --validity_check--vxlan (ingress)  -- max pov share 30 / best pack 29
   17: --validity_check--genv (ingress)  -- max pov share 30 / best pack 29
   18: --validity_check--inner_icmp (ingress)  -- max pov share 30 / best pack 29
   19: --validity_check--inner_tcp (ingress)  -- max pov share 30 / best pack 29
   20: --validity_check--inner_udp (ingress)  -- max pov share 30 / best pack 29
   21: --validity_check--fabric_header (ingress)  -- max pov share 30 / best pack 29
   22: --validity_check--fabric_header_cpu (ingress)  -- max pov share 30 / best pack 29
   23: --validity_check--fabric_payload_header (ingress)  -- max pov share 30 / best pack 29
   24: --validity_check--fabric_header_timestamp (ingress)  -- max pov share 30 / best pack 29
   25: --validity_check--metadata_bridge (ingress)  -- max pov share 30 / best pack 29
   26: --validity_check--vlan_tag_[0] (ingress)  -- max pov share 30 / best pack 29
   27: --validity_check--vlan_tag_[1] (ingress)  -- max pov share 30 / best pack 29
   28: --validity_check--mpls[0] (ingress)  -- max pov share 30 / best pack 29
   29: --validity_check--mpls[1] (ingress)  -- max pov share 30 / best pack 29
   30: --validity_check--mpls[2] (ingress)  -- max pov share 30 / best pack 29
   31: --validity_check--fabric_header (egress)  -- max pov share 19 / best pack 18
   32: --validity_check--fabric_header_cpu (egress)  -- max pov share 19 / best pack 18
   33: --validity_check--fabric_payload_header (egress)  -- max pov share 19 / best pack 18
   34: --validity_check--ethernet (egress)  -- max pov share 20 / best pack 15
   35: --validity_check--ipv4 (egress)  -- max pov share 20 / best pack 15
   36: --validity_check--ipv6 (egress)  -- max pov share 20 / best pack 15
   37: --validity_check--gre (egress)  -- max pov share 20 / best pack 15
   38: --validity_check--nvgre (egress)  -- max pov share 20 / best pack 15
   39: --validity_check--erspan_t3_header (egress)  -- max pov share 20 / best pack 15
   40: --validity_check--vxlan (egress)  -- max pov share 20 / best pack 15
   41: --validity_check--genv (egress)  -- max pov share 20 / best pack 15
   42: --validity_check--inner_ipv4 (egress)  -- max pov share 20 / best pack 15
   43: --validity_check--inner_ipv6 (egress)  -- max pov share 20 / best pack 15
   44: --validity_check--inner_ethernet (egress)  -- max pov share 20 / best pack 15
   45: --validity_check--vlan_tag_[0] (egress)  -- max pov share 16 / best pack 15
   46: --validity_check--llc_header (egress)  -- max pov share 26 / best pack 10
   47: --validity_check--snap_header (egress)  -- max pov share 26 / best pack 10
   48: --validity_check--ipv4_option_32b (egress)  -- max pov share 26 / best pack 10
   49: --validity_check--icmp (egress)  -- max pov share 11 / best pack 10
   50: --validity_check--igmp (egress)  -- max pov share 26 / best pack 10
   51: --validity_check--tcp (egress)  -- max pov share 11 / best pack 10
   52: --validity_check--udp (egress)  -- max pov share 11 / best pack 10
   53: --validity_check--inner_icmp (egress)  -- max pov share 11 / best pack 10
   54: --validity_check--inner_tcp (egress)  -- max pov share 11 / best pack 10
   55: --validity_check--inner_udp (egress)  -- max pov share 11 / best pack 10
   56: --validity_check--fabric_header_timestamp (egress)  -- max pov share 25 / best pack 10
   57: --validity_check--vlan_tag_[1] (egress)  -- max pov share 26 / best pack 10

Working on
--validity_check--ethernet <1 bits ingress parsed pov R>
Call to _allocate_pov_helper for:
  --validity_check--ethernet (ingress)
  Best pack group: (30)

Looking for container to share POV bit in from already allocated containers for POV.
Container availability (not used yet for POV): total 21 / partial 13

Looking for container to share POV bit in from already allocated containers that have not been used for POV.
>>Choose container phv15, starting at container bit 0, which results in 31 bits still available (unused = 32 and could fit = 31).
  >> Decided to allocate new container
Required container phv15
***Allocating phv15[0:0] for --validity_check--ethernet[0:0]
***Allocating phv15[1:1] for --validity_check--llc_header[0:0]
***Allocating phv15[2:2] for --validity_check--snap_header[0:0]
***Allocating phv15[3:3] for --validity_check--ipv4[0:0]
***Allocating phv15[4:4] for --validity_check--ipv4_option_32b[0:0]
***Allocating phv15[5:5] for --validity_check--ipv6[0:0]
***Allocating phv15[6:6] for --validity_check--icmp[0:0]
***Allocating phv15[7:7] for --validity_check--igmp[0:0]
***Allocating phv15[8:8] for --validity_check--tcp[0:0]
***Allocating phv15[9:9] for --validity_check--udp[0:0]
***Allocating phv15[10:10] for --validity_check--gre[0:0]
***Allocating phv15[11:11] for --validity_check--nvgre[0:0]
***Allocating phv15[12:12] for --validity_check--inner_ethernet[0:0]
***Allocating phv15[13:13] for --validity_check--inner_ipv4[0:0]
***Allocating phv15[14:14] for --validity_check--inner_ipv6[0:0]
***Allocating phv15[15:15] for --validity_check--erspan_t3_header[0:0]
***Allocating phv15[16:16] for --validity_check--vxlan[0:0]
***Allocating phv15[17:17] for --validity_check--genv[0:0]
***Allocating phv15[18:18] for --validity_check--inner_icmp[0:0]
***Allocating phv15[19:19] for --validity_check--inner_tcp[0:0]
***Allocating phv15[20:20] for --validity_check--inner_udp[0:0]
***Allocating phv15[21:21] for --validity_check--fabric_header[0:0]
***Allocating phv15[22:22] for --validity_check--fabric_header_cpu[0:0]
***Allocating phv15[23:23] for --validity_check--fabric_payload_header[0:0]
***Allocating phv15[24:24] for --validity_check--fabric_header_timestamp[0:0]
***Allocating phv15[25:25] for --validity_check--metadata_bridge[0:0]
***Allocating phv15[26:26] for --validity_check--vlan_tag_[0][0:0]
***Allocating phv15[27:27] for --validity_check--vlan_tag_[1][0:0]
***Allocating phv15[28:28] for --validity_check--mpls[0][0:0]
***Allocating phv15[29:29] for --validity_check--mpls[1][0:0]
***Allocating phv15[30:30] for --validity_check--mpls[2][0:0]

Working on
--validity_check--llc_header <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--snap_header <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--ipv4 <1 bits ingress parsed pov R>
  Already allocated.

Working on
--validity_check--ipv4_option_32b <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--ipv6 <1 bits ingress parsed pov R>
  Already allocated.

Working on
--validity_check--icmp <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--igmp <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--tcp <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--udp <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--gre <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--nvgre <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--inner_ethernet <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--inner_ipv4 <1 bits ingress parsed pov R>
  Already allocated.

Working on
--validity_check--inner_ipv6 <1 bits ingress parsed pov R>
  Already allocated.

Working on
--validity_check--erspan_t3_header <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--vxlan <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--genv <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--inner_icmp <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--inner_tcp <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--inner_udp <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--fabric_header <1 bits ingress parsed pov W>
  Already allocated.

Working on
--validity_check--fabric_header_cpu <1 bits ingress parsed pov R W>
  Already allocated.

Working on
--validity_check--fabric_payload_header <1 bits ingress parsed pov W>
  Already allocated.

Working on
--validity_check--fabric_header_timestamp <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--metadata_bridge <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--vlan_tag_[0] <1 bits ingress parsed pov R>
  Already allocated.

Working on
--validity_check--vlan_tag_[1] <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--mpls[0] <1 bits ingress parsed pov R>
  Already allocated.

Working on
--validity_check--mpls[1] <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--mpls[2] <1 bits ingress parsed pov>
  Already allocated.

Working on
--validity_check--fabric_header <1 bits egress parsed pov W>
Call to _allocate_pov_helper for:
  --validity_check--fabric_header (egress)
  Best pack group: (9)

Looking for container to share POV bit in from already allocated containers for POV.
Container availability (not used yet for POV): total 20 / partial 10

Looking for container to share POV bit in from already allocated containers that have not been used for POV.
>>Choose container phv61, starting at container bit 0, which results in 31 bits still available (unused = 32 and could fit = 10).
  >> Decided to allocate new container
Required container phv61
***Allocating phv61[0:0] for --validity_check--fabric_header[0:0]
***Allocating phv61[1:1] for --validity_check--fabric_header_timestamp[0:0]
***Allocating phv61[2:2] for --validity_check--vlan_tag_[1][0:0]
***Allocating phv61[3:3] for --validity_check--fabric_header_cpu[0:0]
***Allocating phv61[4:4] for --validity_check--fabric_payload_header[0:0]
***Allocating phv61[5:5] for --validity_check--vxlan[0:0]
***Allocating phv61[6:6] for --validity_check--genv[0:0]
***Allocating phv61[7:7] for --validity_check--inner_ipv4[0:0]
***Allocating phv61[8:8] for --validity_check--inner_ipv6[0:0]
***Allocating phv61[9:9] for --validity_check--inner_ethernet[0:0]

Working on
--validity_check--fabric_header_cpu <1 bits egress parsed pov W>
  Already allocated.

Working on
--validity_check--fabric_payload_header <1 bits egress parsed pov W>
  Already allocated.

Working on
--validity_check--ethernet <1 bits egress parsed pov R W>
  Already allocated.

Working on
--validity_check--ipv4 <1 bits egress parsed pov R W>
  Already allocated.

Working on
--validity_check--ipv6 <1 bits egress parsed pov R W>
  Already allocated.

Working on
--validity_check--gre <1 bits egress parsed pov W>
  Already allocated.

Working on
--validity_check--nvgre <1 bits egress parsed pov W>
  Already allocated.

Working on
--validity_check--erspan_t3_header <1 bits egress parsed pov W>
  Already allocated.

Working on
--validity_check--vxlan <1 bits egress parsed pov W>
  Already allocated.

Working on
--validity_check--genv <1 bits egress parsed pov W>
  Already allocated.

Working on
--validity_check--inner_ipv4 <1 bits egress parsed pov R W>
  Already allocated.

Working on
--validity_check--inner_ipv6 <1 bits egress parsed pov R W>
  Already allocated.

Working on
--validity_check--inner_ethernet <1 bits egress parsed pov R W>
  Already allocated.

Working on
--validity_check--vlan_tag_[0] <1 bits egress parsed pov R W>
  Already allocated.

Working on
--validity_check--llc_header <1 bits egress parsed pov>
  Already allocated.

Working on
--validity_check--snap_header <1 bits egress parsed pov>
  Already allocated.

Working on
--validity_check--ipv4_option_32b <1 bits egress parsed pov>
  Already allocated.

Working on
--validity_check--icmp <1 bits egress parsed pov R W>
Call to _allocate_pov_helper for:
  --validity_check--icmp (egress)
  Best pack group: (5)

Looking for container to share POV bit in from already allocated containers for POV.
Container availability (not used yet for POV): total 19 / partial 11

Looking for container to share POV bit in from already allocated containers that have not been used for POV.
>>Choose container phv211, starting at container bit 0, which results in 15 bits still available (unused = 16 and could fit = 6).
  >> Decided to allocate new container
Required container phv211
***Allocating phv211[0:0] for --validity_check--icmp[0:0]
***Allocating phv211[1:1] for --validity_check--tcp[0:0]
***Allocating phv211[2:2] for --validity_check--udp[0:0]
***Allocating phv211[3:3] for --validity_check--inner_icmp[0:0]
***Allocating phv211[4:4] for --validity_check--inner_tcp[0:0]
***Allocating phv211[5:5] for --validity_check--inner_udp[0:0]

Working on
--validity_check--igmp <1 bits egress parsed pov>
  Already allocated.

Working on
--validity_check--tcp <1 bits egress parsed pov R W>
  Already allocated.

Working on
--validity_check--udp <1 bits egress parsed pov R W>
  Already allocated.

Working on
--validity_check--inner_icmp <1 bits egress parsed pov R W>
  Already allocated.

Working on
--validity_check--inner_tcp <1 bits egress parsed pov R W>
  Already allocated.

Working on
--validity_check--inner_udp <1 bits egress parsed pov R W>
  Already allocated.

Working on
--validity_check--fabric_header_timestamp <1 bits egress parsed pov W>
  Already allocated.

Working on
--validity_check--vlan_tag_[1] <1 bits egress parsed pov>
  Already allocated.

POV container occupancy:
 ingress
    phv15 (32 bits)
  >> 32 total bits
 egress
    phv60 (32 bits)
    phv61 (32 bits)
    phv211 (16 bits)
  >> 80 total bits
>>Event 'pa_meta2' at time 1573972406.24
   Took 18.38 seconds

-----------------------------------------------
  Allocating metadata (pass 2)
-----------------------------------------------
Allocation Step
Creating 4 processes
  Task queue has 77 elements
Process 0 started.
Process 1 started.
Process 2 started.
Process 3 started.
Process 0 examined 20 fields
Process 1 examined 19 fields
Process 2 examined 19 fields
Process 3 examined 19 fields
Total metadata field instances to allocate: 77  / 319 bits (257 ingress bits and 62 egress bits)
>> Metadata analysis took 1.65 seconds
Promised metadata field instances to allocate: 2 / 30 bits (30 ingress bits and 0 egress bits)
     0: ig_intr_md_for_tm.mcast_grp_b (ingress) (highly=0, mau_group_size=3, max_overlay=1, max_share=0, max_split=1, bit_width=16, initial_usage_read=4, earliest_use=3, latest_use=12)
     1: multicast_metadata.bd_mrpf_group (ingress) (highly=0, mau_group_size=2, max_overlay=12, max_share=90, max_split=14, bit_width=14, initial_usage_read=2, earliest_use=0, latest_use=5)

--------------
Working on:
ig_intr_md_for_tm.mcast_grp_b <16 bits ingress imeta W>
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 16 while meta_fi.bit_width = 16
Parse state 0 (16 bits)
  ig_intr_md_for_tm.mcast_grp_b [15:0]
-----------------------------------------------------------------------------------------------------------
|              Name             | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------------
| ig_intr_md_for_tm.mcast_grp_b | 16 |   False   | [(16, 16)] |  -   |    [8]    |    1     |     3      |
-----------------------------------------------------------------------------------------------------------

max_split = 1, adj = False
required_packing = [(16, 16)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 16
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
  (3) msb_offset = 16
>>Can pack using [16] if open up 1 new containers.

>>Choose overlay option
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 16
***Allocating phv216[15:0] for ig_intr_md_for_tm.mcast_grp_b[15:0]

--------------
Working on:
multicast_metadata.bd_mrpf_group <14 bits ingress meta R W>
bits_will_need_to_parse = 14
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
extracted_bits = 14 while meta_fi.bit_width = 14
Parse state 0 (14 bits)
  multicast_metadata.bd_mrpf_group [13:0]
--------------------------------------------------------------------------------------------------------------
|               Name               | BW | Tagalong? |    Req     | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------------------------
| multicast_metadata.bd_mrpf_group | 14 |   False   | [(16, 14)] | [16] |     -     |   None   |     2      |
--------------------------------------------------------------------------------------------------------------

max_split = None, adj = None
required_packing = [(16, 14)]
Packing options: 1
Valid packing options: 1

Attempting to overlay...
  [16]
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
>>Can pack using [16] if open up 1 new containers.

Attempting to share...

  [16]
>>Can pack using [16] if open up 1 new containers.

>>Choose overlay option
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 14
>> HEY!:  Adjusted msb_offset!
***Allocating phv220[13:0] for multicast_metadata.bd_mrpf_group[13:0]
Allocation state after promised meta allocated:
Allocation state: Final Allocation
------------------------------------------------------------------------------
|       PHV Group        | Containers Used |   Bits Used   | Bits Available |
| (container bit widths) |     (% used)    |    (% used)   |                |
------------------------------------------------------------------------------
|         0 (32)         |   16 (100.00%)  |  479 (93.55%) |      512       |
|         1 (32)         |   16 (100.00%)  | 512 (100.00%) |      512       |
|         2 (32)         |   16 (100.00%)  | 512 (100.00%) |      512       |
|         3 (32)         |   12 (75.00%)   |  324 (63.28%) |      512       |
|    Total for 32 bit    |   60 (93.75%)   | 1827 (89.21%) |      2048      |
|                        |                 |               |                |
|         4 (8)          |    5 (31.25%)   |  40 (31.25%)  |      128       |
|         5 (8)          |   16 (100.00%)  | 128 (100.00%) |      128       |
|         6 (8)          |   16 (100.00%)  | 128 (100.00%) |      128       |
|         7 (8)          |   16 (100.00%)  |  115 (89.84%) |      128       |
|    Total for 8 bit     |   53 (82.81%)   |  411 (80.27%) |      512       |
|                        |                 |               |                |
|         8 (16)         |   16 (100.00%)  |  245 (95.70%) |      256       |
|         9 (16)         |   16 (100.00%)  |  250 (97.66%) |      256       |
|        10 (16)         |   16 (100.00%)  | 256 (100.00%) |      256       |
|        11 (16)         |   16 (100.00%)  |  244 (95.31%) |      256       |
|        12 (16)         |   16 (100.00%)  |  254 (99.22%) |      256       |
|        13 (16)         |   10 (62.50%)   |  144 (56.25%) |      256       |
|    Total for 16 bit    |   90 (93.75%)   | 1393 (90.69%) |      1536      |
|                        |                 |               |                |
|       14 (32) T        |   14 (87.50%)   |  448 (87.50%) |      512       |
|       15 (32) T        |    5 (31.25%)   |  160 (31.25%) |      512       |
|    Total for 32 bit    |   19 (59.38%)   |  608 (59.38%) |      1024      |
|                        |                 |               |                |
|        16 (8) T        |   15 (93.75%)   |  120 (93.75%) |      128       |
|        17 (8) T        |    8 (50.00%)   |  64 (50.00%)  |      128       |
|    Total for 8 bit     |   23 (71.88%)   |  184 (71.88%) |      256       |
|                        |                 |               |                |
|       18 (16) T        |   15 (93.75%)   |  240 (93.75%) |      256       |
|       19 (16) T        |    6 (37.50%)   |  96 (37.50%)  |      256       |
|       20 (16) T        |    0 (0.00%)    |   0 (0.00%)   |      256       |
|    Total for 16 bit    |   21 (43.75%)   |  336 (43.75%) |      768       |
|                        |                 |               |                |
|       MAU total        |   203 (90.62%)  | 3631 (88.65%) |      4096      |
|     Tagalong total     |   63 (56.25%)   | 1128 (55.08%) |      2048      |
|     Overall total      |   266 (79.17%)  | 4759 (77.46%) |      6144      |
------------------------------------------------------------------------------

Allocation state difference after promised meta allocated:
Allocation state: Diff
----------------------------------------------------------------------------
|       PHV Group        | Containers Used |  Bits Used  | Bits Available |
| (container bit widths) |     (% used)    |   (% used)  |                |
----------------------------------------------------------------------------
|         0 (32)         |    0 (0.00%)    |  0 (0.00%)  |      512       |
|         1 (32)         |    0 (0.00%)    |  0 (0.00%)  |      512       |
|         2 (32)         |    0 (0.00%)    |  0 (0.00%)  |      512       |
|         3 (32)         |    0 (0.00%)    |  0 (0.00%)  |      512       |
|    Total for 32 bit    |    0 (0.00%)    |  0 (0.00%)  |      2048      |
|                        |                 |             |                |
|         4 (8)          |    0 (0.00%)    |  0 (0.00%)  |      128       |
|         5 (8)          |    0 (0.00%)    |  0 (0.00%)  |      128       |
|         6 (8)          |    0 (0.00%)    |  0 (0.00%)  |      128       |
|         7 (8)          |    0 (0.00%)    |  0 (0.00%)  |      128       |
|    Total for 8 bit     |    0 (0.00%)    |  0 (0.00%)  |      512       |
|                        |                 |             |                |
|         8 (16)         |    0 (0.00%)    |  0 (0.00%)  |      256       |
|         9 (16)         |    0 (0.00%)    |  0 (0.00%)  |      256       |
|        10 (16)         |    0 (0.00%)    |  0 (0.00%)  |      256       |
|        11 (16)         |    0 (0.00%)    |  0 (0.00%)  |      256       |
|        12 (16)         |    0 (0.00%)    |  0 (0.00%)  |      256       |
|        13 (16)         |    2 (12.50%)   | 30 (11.72%) |      256       |
|    Total for 16 bit    |    2 (2.08%)    |  30 (1.95%) |      1536      |
|                        |                 |             |                |
|       14 (32) T        |    0 (0.00%)    |  0 (0.00%)  |      512       |
|       15 (32) T        |    0 (0.00%)    |  0 (0.00%)  |      512       |
|    Total for 32 bit    |    0 (0.00%)    |  0 (0.00%)  |      1024      |
|                        |                 |             |                |
|        16 (8) T        |    0 (0.00%)    |  0 (0.00%)  |      128       |
|        17 (8) T        |    0 (0.00%)    |  0 (0.00%)  |      128       |
|    Total for 8 bit     |    0 (0.00%)    |  0 (0.00%)  |      256       |
|                        |                 |             |                |
|       18 (16) T        |    0 (0.00%)    |  0 (0.00%)  |      256       |
|       19 (16) T        |    0 (0.00%)    |  0 (0.00%)  |      256       |
|       20 (16) T        |    0 (0.00%)    |  0 (0.00%)  |      256       |
|    Total for 16 bit    |    0 (0.00%)    |  0 (0.00%)  |      768       |
|                        |                 |             |                |
|       MAU total        |    2 (0.89%)    |  30 (0.73%) |      4096      |
|     Tagalong total     |    0 (0.00%)    |  0 (0.00%)  |      2048      |
|     Overall total      |    2 (0.60%)    |  30 (0.49%) |      6144      |
----------------------------------------------------------------------------

>> 'Promised' metadata allocation took 0.19 seconds
Sorted metadata field instances to allocate: 75 / 289 bits (227 ingress bits and 62 egress bits)
     0: l2_metadata.l2_nexthop_type (ingress) (highly=0, mau_group_size=5, max_overlay=0, best_overlay_pack=0, max_share=43, best_share_pack=21, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=3, latest_use=7)
     1: acl_metadata.acl_nexthop_type (ingress) (highly=0, mau_group_size=5, max_overlay=0, best_overlay_pack=0, max_share=43, best_share_pack=21, max_split=1, bit_width=1, initial_usage_read=4, earliest_use=3, latest_use=7)
     2: acl_metadata.racl_nexthop_type (ingress) (highly=0, mau_group_size=5, max_overlay=0, best_overlay_pack=0, max_share=58, best_share_pack=16, max_split=1, bit_width=1, initial_usage_read=3, earliest_use=5, latest_use=7)
     3: nexthop_metadata.nexthop_type (ingress) (highly=0, mau_group_size=5, max_overlay=0, best_overlay_pack=0, max_share=63, best_share_pack=16, max_split=1, bit_width=1, initial_usage_read=1, earliest_use=7, latest_use=8)
     4: l3_metadata.fib_nexthop_type (ingress) (highly=0, mau_group_size=5, max_overlay=0, best_overlay_pack=0, max_share=37, best_share_pack=15, max_split=1, bit_width=1, initial_usage_read=4, earliest_use=2, latest_use=7)
     5: l3_metadata.urpf_mode (ingress) (highly=0, mau_group_size=3, max_overlay=8, best_overlay_pack=4, max_share=37, best_share_pack=15, max_split=2, bit_width=2, initial_usage_read=3, earliest_use=5, latest_use=6)
     6: ipv4_metadata.ipv4_urpf_mode (ingress) (highly=0, mau_group_size=3, max_overlay=4, best_overlay_pack=0, max_share=55, best_share_pack=26, max_split=2, bit_width=2, initial_usage_read=2, earliest_use=0, latest_use=5)
     7: ipv6_metadata.ipv6_urpf_mode (ingress) (highly=0, mau_group_size=3, max_overlay=4, best_overlay_pack=0, max_share=55, best_share_pack=26, max_split=2, bit_width=2, initial_usage_read=2, earliest_use=0, latest_use=5)
     8: l3_metadata.urpf_bd_group (ingress) (highly=0, mau_group_size=1, max_overlay=8, best_overlay_pack=4, max_share=37, best_share_pack=15, max_split=14, bit_width=14, initial_usage_read=3, earliest_use=5, latest_use=6)
     9: l3_metadata.urpf_hit (ingress) (highly=0, mau_group_size=1, max_overlay=8, best_overlay_pack=4, max_share=37, best_share_pack=15, max_split=1, bit_width=1, initial_usage_read=3, earliest_use=5, latest_use=6)
    10: l2_metadata.stp_group (ingress) (highly=0, mau_group_size=1, max_overlay=7, best_overlay_pack=3, max_share=55, best_share_pack=26, max_split=10, bit_width=10, initial_usage_read=2, earliest_use=0, latest_use=1)
    11: tunnel_metadata.src_vtep_hit (ingress) (highly=0, mau_group_size=1, max_overlay=7, best_overlay_pack=3, max_share=59, best_share_pack=17, max_split=1, bit_width=1, initial_usage_read=3, earliest_use=1, latest_use=2)
    12: tunnel_metadata.tunnel_term_type (ingress) (highly=0, mau_group_size=1, max_overlay=7, best_overlay_pack=3, max_share=60, best_share_pack=17, max_split=1, bit_width=1, initial_usage_read=3, earliest_use=1, latest_use=2)
    13: tunnel_metadata.tunnel_lookup (ingress) (highly=0, mau_group_size=1, max_overlay=7, best_overlay_pack=3, max_share=40, best_share_pack=12, max_split=1, bit_width=1, initial_usage_read=4, earliest_use=0, latest_use=2)
    14: acl_metadata.bd_label (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=55, best_share_pack=26, max_split=16, bit_width=16, initial_usage_read=2, earliest_use=0, latest_use=9)
    15: l3_metadata.rmac_group (ingress) (highly=0, mau_group_size=1, max_overlay=4, best_overlay_pack=0, max_share=55, best_share_pack=26, max_split=10, bit_width=10, initial_usage_read=2, earliest_use=0, latest_use=4)
    16: l2_metadata.learning_enabled (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=55, best_share_pack=26, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=0, latest_use=9)
    17: l2_metadata.port_vlan_mapping_miss (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=55, best_share_pack=26, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=0, latest_use=9)
    18: ipv4_metadata.ipv4_unicast_enabled (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=55, best_share_pack=26, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=0, latest_use=9)
    19: ipv6_metadata.ipv6_unicast_enabled (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=55, best_share_pack=26, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=0, latest_use=9)
    20: multicast_metadata.ipv4_multicast_enabled (ingress) (highly=0, mau_group_size=1, max_overlay=4, best_overlay_pack=0, max_share=55, best_share_pack=26, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=0, latest_use=5)
    21: multicast_metadata.ipv6_multicast_enabled (ingress) (highly=0, mau_group_size=1, max_overlay=4, best_overlay_pack=0, max_share=55, best_share_pack=26, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=0, latest_use=5)
    22: multicast_metadata.igmp_snooping_enabled (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=55, best_share_pack=26, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=0, latest_use=7)
    23: multicast_metadata.mld_snooping_enabled (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=55, best_share_pack=26, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=0, latest_use=7)
    24: multicast_metadata.mcast_bridge_hit (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=47, best_share_pack=24, max_split=1, bit_width=1, initial_usage_read=3, earliest_use=4, latest_use=7)
    25: acl_metadata.acl_stats_index (ingress) (highly=0, mau_group_size=1, max_overlay=3, best_overlay_pack=0, max_share=43, best_share_pack=21, max_split=12, bit_width=12, initial_usage_read=4, earliest_use=3, latest_use=7)
    26: ig_intr_md_for_mb.ingress_mirror_id (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=43, best_share_pack=21, max_split=1, bit_width=10, initial_usage_read=4, earliest_use=-1, latest_use=12)
    27: ig_intr_md_for_tm.drop_ctl (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=43, best_share_pack=21, max_split=1, bit_width=3, initial_usage_read=3, earliest_use=3, latest_use=12)
    28: l2_metadata.l2_redirect (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=43, best_share_pack=21, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=3, latest_use=7)
    29: l2_metadata.l2_dst_miss (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=43, best_share_pack=21, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=3, latest_use=9)
    30: acl_metadata.acl_deny (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=43, best_share_pack=21, max_split=1, bit_width=1, initial_usage_read=4, earliest_use=3, latest_use=9)
    31: acl_metadata.acl_redirect (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=43, best_share_pack=21, max_split=1, bit_width=1, initial_usage_read=4, earliest_use=3, latest_use=7)
    32: multicast_metadata.mcast_copy_to_cpu (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=44, best_share_pack=21, max_split=1, bit_width=1, initial_usage_read=5, earliest_use=4, latest_use=9)
    33: ig_intr_md_for_tm.level1_exclusion_id (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=64, best_share_pack=20, max_split=1, bit_width=16, initial_usage_read=2, earliest_use=2, latest_use=12)
    34: ingress_metadata.egress_port_lag_index (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=42, best_share_pack=20, max_split=10, bit_width=10, initial_usage_read=4, earliest_use=3, latest_use=9)
    35: ingress_metadata.drop_reason (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=21, best_share_pack=17, max_split=8, bit_width=8, initial_usage_read=1, earliest_use=0, latest_use=9)
    36: acl_metadata.ingress_src_port_range_id (ingress) (highly=0, mau_group_size=1, max_overlay=4, best_overlay_pack=0, max_share=39, best_share_pack=17, max_split=8, bit_width=8, initial_usage_read=1, earliest_use=3, latest_use=5)
    37: acl_metadata.ingress_dst_port_range_id (ingress) (highly=0, mau_group_size=1, max_overlay=4, best_overlay_pack=0, max_share=39, best_share_pack=17, max_split=8, bit_width=8, initial_usage_read=1, earliest_use=3, latest_use=5)
    38: l2_metadata.stp_state (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=60, best_share_pack=17, max_split=3, bit_width=3, initial_usage_read=3, earliest_use=1, latest_use=9)
    39: ingress_metadata.drop_flag (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=21, best_share_pack=17, max_split=1, bit_width=1, initial_usage_read=1, earliest_use=0, latest_use=9)
    40: l2_metadata.l2_src_miss (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=39, best_share_pack=17, max_split=1, bit_width=1, initial_usage_read=3, earliest_use=3, latest_use=9)
    41: l3_metadata.rmac_hit (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=39, best_share_pack=17, max_split=1, bit_width=1, initial_usage_read=6, earliest_use=1, latest_use=9)
    42: ipv6_metadata.ipv6_src_is_link_local (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=42, best_share_pack=17, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=3, latest_use=9)
    43: acl_metadata.racl_stats_index (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=58, best_share_pack=16, max_split=12, bit_width=12, initial_usage_read=3, earliest_use=5, latest_use=9)
    44: ig_intr_md_for_tm.qid (ingress) (highly=0, mau_group_size=1, max_overlay=15, best_overlay_pack=0, max_share=65, best_share_pack=16, max_split=1, bit_width=5, initial_usage_read=2, earliest_use=9, latest_use=12)
    45: ig_intr_md_for_tm.ingress_cos (ingress) (highly=0, mau_group_size=1, max_overlay=15, best_overlay_pack=0, max_share=65, best_share_pack=16, max_split=1, bit_width=3, initial_usage_read=2, earliest_use=9, latest_use=12)
    46: l3_metadata.lkp_ip_type (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=45, best_share_pack=16, max_split=2, bit_width=2, initial_usage_read=6, earliest_use=0, latest_use=7)
    47: ig_intr_md_for_tm.copy_to_cpu (ingress) (highly=0, mau_group_size=1, max_overlay=15, best_overlay_pack=0, max_share=65, best_share_pack=16, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=9, latest_use=12)
    48: l3_metadata.lkp_ip_llmc (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=45, best_share_pack=16, max_split=1, bit_width=1, initial_usage_read=3, earliest_use=0, latest_use=7)
    49: l3_metadata.lkp_ip_mc (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=45, best_share_pack=16, max_split=1, bit_width=1, initial_usage_read=3, earliest_use=0, latest_use=7)
    50: l3_metadata.l3_copy (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=65, best_share_pack=16, max_split=1, bit_width=1, initial_usage_read=1, earliest_use=9, latest_use=9)
    51: acl_metadata.racl_deny (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=58, best_share_pack=16, max_split=1, bit_width=1, initial_usage_read=3, earliest_use=5, latest_use=9)
    52: acl_metadata.racl_redirect (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=58, best_share_pack=16, max_split=1, bit_width=1, initial_usage_read=3, earliest_use=5, latest_use=7)
    53: multicast_metadata.mcast_rpf_fail (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=64, best_share_pack=16, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=7, latest_use=9)
    54: multicast_metadata.flood_to_mrouters (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=64, best_share_pack=16, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=7, latest_use=9)
    55: nexthop_metadata.nexthop_glean (ingress) (highly=0, mau_group_size=1, max_overlay=14, best_overlay_pack=0, max_share=65, best_share_pack=16, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=8, latest_use=9)
    56: ig_intr_md_for_tm.level2_exclusion_id (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=45, best_share_pack=15, max_split=1, bit_width=9, initial_usage_read=1, earliest_use=0, latest_use=12)
    57: l2_metadata.port_learning_enabled (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=45, best_share_pack=15, max_split=1, bit_width=1, initial_usage_read=1, earliest_use=0, latest_use=9)
    58: l3_metadata.fib_hit (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=37, best_share_pack=15, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=2, latest_use=7)
    59: l3_metadata.fib_hit_myip (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=37, best_share_pack=15, max_split=1, bit_width=1, initial_usage_read=3, earliest_use=5, latest_use=9)
    60: meter_metadata.storm_control_color (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=17, best_share_pack=14, max_split=1, bit_width=2, initial_usage_read=2, earliest_use=2, latest_use=9)
    61: multicast_metadata.mcast_mode (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=62, best_share_pack=14, max_split=2, bit_width=2, initial_usage_read=3, earliest_use=5, latest_use=7)
    62: multicast_metadata.mcast_route_hit (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=62, best_share_pack=14, max_split=1, bit_width=1, initial_usage_read=4, earliest_use=5, latest_use=9)
    63: multicast_metadata.mcast_route_s_g_hit (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=62, best_share_pack=14, max_split=1, bit_width=1, initial_usage_read=3, earliest_use=5, latest_use=9)
    64: ig_intr_md_for_tm.rid (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=40, best_share_pack=12, max_split=1, bit_width=16, initial_usage_read=1, earliest_use=0, latest_use=12)
    65: l3_metadata.urpf_check_fail (ingress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=34, best_share_pack=12, max_split=1, bit_width=1, initial_usage_read=4, earliest_use=5, latest_use=9)
    66: egress_metadata.smac_idx (egress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=7, best_share_pack=6, max_split=9, bit_width=9, initial_usage_read=1, earliest_use=4, latest_use=5)
    67: l3_metadata.mtu_index (egress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=7, best_share_pack=6, max_split=8, bit_width=8, initial_usage_read=2, earliest_use=4, latest_use=7)
    68: tunnel_metadata.tunnel_index (egress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=8, best_share_pack=5, max_split=14, bit_width=14, initial_usage_read=3, earliest_use=0, latest_use=7)
    69: tunnel_metadata.tunnel_dmac_index (egress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=8, best_share_pack=5, max_split=12, bit_width=12, initial_usage_read=3, earliest_use=0, latest_use=7)
    70: tunnel_metadata.tunnel_smac_index (egress) (highly=0, mau_group_size=1, max_overlay=1, best_overlay_pack=0, max_share=6, best_share_pack=5, max_split=8, bit_width=8, initial_usage_read=1, earliest_use=6, latest_use=7)
    71: tunnel_metadata.egress_tunnel_type (egress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=8, best_share_pack=5, max_split=5, bit_width=5, initial_usage_read=3, earliest_use=0, latest_use=6)
    72: tunnel_metadata.egress_header_count (egress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=8, best_share_pack=5, max_split=4, bit_width=4, initial_usage_read=3, earliest_use=0, latest_use=6)
    73: multicast_metadata.inner_replica (egress) (highly=0, mau_group_size=1, max_overlay=1, best_overlay_pack=0, max_share=8, best_share_pack=5, max_split=1, bit_width=1, initial_usage_read=2, earliest_use=0, latest_use=3)
    74: multicast_metadata.replica (egress) (highly=0, mau_group_size=1, max_overlay=0, best_overlay_pack=0, max_share=8, best_share_pack=5, max_split=1, bit_width=1, initial_usage_read=4, earliest_use=0, latest_use=6)
>> Free metadata analysis took 0.83 seconds

---------------------------------------
Working on:
l2_metadata.l2_nexthop_type <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 43 (148 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  l2_metadata.l2_nexthop_type [0:0]
--------------------------------------------------------------------------------------------------
|             Name            | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------------
| l2_metadata.l2_nexthop_type | 1  |   False   |  -  |  -   |     -     |   None   |     5      |
--------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 2 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 3 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 3 -- ingress promised 3 and remain 1 and req 0 -- egress promised 0 and remain 1 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised 3 -- ingress promised 3 and remain 8 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv69 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 3 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 3 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 2 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv137 -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 3 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 2 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv182 -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 3 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 6 and promised 2 -- ingress promised 2 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv220 -- fails False
Metadata instance: l2_metadata.l2_nexthop_type <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv220[14:14] for l2_metadata.l2_nexthop_type[0:0]

---------------------------------------
Working on:
acl_metadata.acl_nexthop_type <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 43 (157 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  acl_metadata.acl_nexthop_type [0:0]
----------------------------------------------------------------------------------------------------------
|              Name             | BW | Tagalong? |    Req    | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------------
| acl_metadata.acl_nexthop_type | 1  |   False   | [(16, 1)] |  -   |     -     |   None   |     5      |
----------------------------------------------------------------------------------------------------------

  req packing: [[(16, 1)]]
  disallowed packing: [None]
  Group 13 16 bits -- avail 6 and promised 2 -- ingress promised 2 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv222 -- fails False
Metadata instance: acl_metadata.acl_nexthop_type <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv222[0:0] for acl_metadata.acl_nexthop_type[0:0]

---------------------------------------
Working on:
acl_metadata.racl_nexthop_type <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 56 (203 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  acl_metadata.racl_nexthop_type [0:0]
-----------------------------------------------------------------------------------------------------------
|              Name              | BW | Tagalong? |    Req    | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------------
| acl_metadata.racl_nexthop_type | 1  |   False   | [(16, 1)] |  -   |     -     |   None   |     5      |
-----------------------------------------------------------------------------------------------------------

  req packing: [[(16, 1)]]
  disallowed packing: [None]
  Group 13 16 bits -- avail 5 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv220 -- fails False
Metadata instance: acl_metadata.racl_nexthop_type <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv220[15:15] for acl_metadata.racl_nexthop_type[0:0]

---------------------------------------
Working on:
nexthop_metadata.nexthop_type <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 60 (212 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  nexthop_metadata.nexthop_type [0:0]
----------------------------------------------------------------------------------------------------------
|              Name             | BW | Tagalong? |    Req    | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------------
| nexthop_metadata.nexthop_type | 1  |   False   | [(16, 1)] |  -   |     -     |   None   |     5      |
----------------------------------------------------------------------------------------------------------

  req packing: [[(16, 1)]]
  disallowed packing: [None]
  Group 13 16 bits -- avail 5 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv222 -- fails False
Metadata instance: nexthop_metadata.nexthop_type <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv222[1:1] for nexthop_metadata.nexthop_type[0:0]

---------------------------------------
Working on:
l3_metadata.fib_nexthop_type <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 36 (119 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  l3_metadata.fib_nexthop_type [0:0]
---------------------------------------------------------------------------------------------------------
|             Name             | BW | Tagalong? |    Req    | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------------
| l3_metadata.fib_nexthop_type | 1  |   False   | [(16, 1)] |  -   |     -     |   None   |     5      |
---------------------------------------------------------------------------------------------------------

  req packing: [[(16, 1)]]
  disallowed packing: [None]
  Group 13 16 bits -- avail 5 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv223 -- fails False
Metadata instance: l3_metadata.fib_nexthop_type <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv223[0:0] for l3_metadata.fib_nexthop_type[0:0]

---------------------------------------
Working on:
l3_metadata.urpf_mode <2 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 8 (23 bits)
   max_share = 36 (119 bits)
bits_will_need_to_parse = 2
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (2 bits)
  l3_metadata.urpf_mode [1:0]
--------------------------------------------------------------------------------------------
|          Name         | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------
| l3_metadata.urpf_mode | 2  |   False   |  -  |  -   |     -     |   None   |     3      |
--------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised 1 -- ingress promised 1 and remain 10 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv69 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv124 -- fails True
  Group 8 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv140 -- fails False
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv175 -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv182 -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv202 -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: l3_metadata.urpf_mode <2 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
  (2b) msb_offset = 16
***Allocating phv140[15:14] for l3_metadata.urpf_mode[1:0]

---------------------------------------
Working on:
ipv4_metadata.ipv4_urpf_mode <2 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 4 (10 bits)
   max_share = 49 (176 bits)
bits_will_need_to_parse = 2
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (2 bits)
  ipv4_metadata.ipv4_urpf_mode [1:0]
---------------------------------------------------------------------------------------------------------
|             Name             | BW | Tagalong? |    Req    | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------------
| ipv4_metadata.ipv4_urpf_mode | 2  |   False   | [(16, 2)] |  -   |     -     |   None   |     3      |
---------------------------------------------------------------------------------------------------------

  req packing: [[(16, 2)]]
  disallowed packing: [None]
  Group 8 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv137 -- fails False
Metadata instance: ipv4_metadata.ipv4_urpf_mode <2 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv137[14:13] for ipv4_metadata.ipv4_urpf_mode[1:0]

---------------------------------------
Working on:
ipv6_metadata.ipv6_urpf_mode <2 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 4 (10 bits)
   max_share = 48 (174 bits)
bits_will_need_to_parse = 2
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (2 bits)
  ipv6_metadata.ipv6_urpf_mode [1:0]
---------------------------------------------------------------------------------------------------------
|             Name             | BW | Tagalong? |    Req    | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------------
| ipv6_metadata.ipv6_urpf_mode | 2  |   False   | [(16, 2)] |  -   |     -     |   None   |     3      |
---------------------------------------------------------------------------------------------------------

  req packing: [[(16, 2)]]
  disallowed packing: [None]
  Group 8 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv136 -- fails False
Metadata instance: ipv6_metadata.ipv6_urpf_mode <2 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv136[14:13] for ipv6_metadata.ipv6_urpf_mode[1:0]

---------------------------------------
Working on:
l3_metadata.urpf_bd_group <14 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 8 (23 bits)
   max_share = 34 (115 bits)
bits_will_need_to_parse = 14
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (14 bits)
  l3_metadata.urpf_bd_group [13:0]
------------------------------------------------------------------------------------------------
|            Name           | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------
| l3_metadata.urpf_bd_group | 14 |   False   |  -  |  -   |     -     |   None   |     1      |
------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv175 -- fails False
  Group 11 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv182 -- fails False
  Group 12 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv202 -- fails False
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: l3_metadata.urpf_bd_group <14 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
  (2b) msb_offset = 16
***Allocating phv175[15:2] for l3_metadata.urpf_bd_group[13:0]

---------------------------------------
Working on:
l3_metadata.urpf_hit <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 8 (23 bits)
   max_share = 34 (115 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  l3_metadata.urpf_hit [0:0]
-------------------------------------------------------------------------------------------
|         Name         | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------
| l3_metadata.urpf_hit | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
-------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised 1 -- ingress promised 1 and remain 10 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv69 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv124 -- fails False
  Group 8 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv137 -- fails False
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv182 -- fails False
  Group 12 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv202 -- fails False
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: l3_metadata.urpf_hit <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
  (2b) msb_offset = 8
***Allocating phv124[7:7] for l3_metadata.urpf_hit[0:0]

---------------------------------------
Working on:
l2_metadata.stp_group <10 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 4 (10 bits)
   max_share = 45 (149 bits)
bits_will_need_to_parse = 10
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (10 bits)
  l2_metadata.stp_group [9:0]
--------------------------------------------------------------------------------------------
|          Name         | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------
| l2_metadata.stp_group | 10 |   False   |  -  |  -   |     -     |   None   |     1      |
--------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv12 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv222 -- fails False
Metadata instance: l2_metadata.stp_group <10 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv222[11:2] for l2_metadata.stp_group[9:0]

---------------------------------------
Working on:
tunnel_metadata.src_vtep_hit <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 4 (10 bits)
   max_share = 48 (166 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  tunnel_metadata.src_vtep_hit [0:0]
---------------------------------------------------------------------------------------------------
|             Name             | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------
| tunnel_metadata.src_vtep_hit | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
---------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv12 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised 1 -- ingress promised 1 and remain 10 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv69 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv137 -- fails False
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv182 -- fails False
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv222 -- fails False
Metadata instance: tunnel_metadata.src_vtep_hit <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv222[12:12] for tunnel_metadata.src_vtep_hit[0:0]

---------------------------------------
Working on:
tunnel_metadata.tunnel_term_type <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 4 (10 bits)
   max_share = 49 (167 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  tunnel_metadata.tunnel_term_type [0:0]
-------------------------------------------------------------------------------------------------------
|               Name               | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------------
| tunnel_metadata.tunnel_term_type | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
-------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv12 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised 1 -- ingress promised 1 and remain 10 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv69 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv137 -- fails False
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv182 -- fails False
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv223 -- fails False
Metadata instance: tunnel_metadata.tunnel_term_type <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv223[1:1] for tunnel_metadata.tunnel_term_type[0:0]

---------------------------------------
Working on:
tunnel_metadata.tunnel_lookup <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 4 (10 bits)
   max_share = 31 (109 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  tunnel_metadata.tunnel_lookup [0:0]
----------------------------------------------------------------------------------------------------
|              Name             | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------
| tunnel_metadata.tunnel_lookup | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
----------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv12 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised 1 -- ingress promised 1 and remain 10 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv69 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv223 -- fails False
Metadata instance: tunnel_metadata.tunnel_lookup <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv223[2:2] for tunnel_metadata.tunnel_lookup[0:0]

---------------------------------------
Working on:
acl_metadata.bd_label <16 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 42 (131 bits)
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (16 bits)
  acl_metadata.bd_label [15:0]
--------------------------------------------------------------------------------------------
|          Name         | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------
| acl_metadata.bd_label | 16 |   False   |  -  |  -   |     -     |   None   |     1      |
--------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv12 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: acl_metadata.bd_label <16 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv12[31:16] for acl_metadata.bd_label[15:0]

---------------------------------------
Working on:
l3_metadata.rmac_group <10 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 4 (10 bits)
   max_share = 41 (121 bits)
bits_will_need_to_parse = 10
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (10 bits)
  l3_metadata.rmac_group [9:0]
---------------------------------------------------------------------------------------------
|          Name          | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------
| l3_metadata.rmac_group | 10 |   False   |  -  |  -   |     -     |   None   |     1      |
---------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv142 -- fails False
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: l3_metadata.rmac_group <10 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
  (2b) msb_offset = 16
***Allocating phv142[15:6] for l3_metadata.rmac_group[9:0]

---------------------------------------
Working on:
l2_metadata.learning_enabled <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 40 (120 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  l2_metadata.learning_enabled [0:0]
---------------------------------------------------------------------------------------------------
|             Name             | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------
| l2_metadata.learning_enabled | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
---------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised 1 -- ingress promised 1 and remain 10 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv69 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv137 -- fails False
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv222 -- fails False
Metadata instance: l2_metadata.learning_enabled <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv222[13:13] for l2_metadata.learning_enabled[0:0]

---------------------------------------
Working on:
l2_metadata.port_vlan_mapping_miss <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 39 (119 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  l2_metadata.port_vlan_mapping_miss [0:0]
---------------------------------------------------------------------------------------------------------
|                Name                | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------------
| l2_metadata.port_vlan_mapping_miss | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
---------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised 1 -- ingress promised 1 and remain 10 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv69 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv137 -- fails False
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv222 -- fails False
Metadata instance: l2_metadata.port_vlan_mapping_miss <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv222[14:14] for l2_metadata.port_vlan_mapping_miss[0:0]

---------------------------------------
Working on:
ipv4_metadata.ipv4_unicast_enabled <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 38 (118 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  ipv4_metadata.ipv4_unicast_enabled [0:0]
---------------------------------------------------------------------------------------------------------
|                Name                | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------------
| ipv4_metadata.ipv4_unicast_enabled | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
---------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised 1 -- ingress promised 1 and remain 10 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv69 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv137 -- fails False
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv222 -- fails False
Metadata instance: ipv4_metadata.ipv4_unicast_enabled <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv222[15:15] for ipv4_metadata.ipv4_unicast_enabled[0:0]

---------------------------------------
Working on:
ipv6_metadata.ipv6_unicast_enabled <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 37 (117 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  ipv6_metadata.ipv6_unicast_enabled [0:0]
---------------------------------------------------------------------------------------------------------
|                Name                | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------------
| ipv6_metadata.ipv6_unicast_enabled | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
---------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised 1 -- ingress promised 1 and remain 10 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv69 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv137 -- fails False
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: ipv6_metadata.ipv6_unicast_enabled <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv137[15:15] for ipv6_metadata.ipv6_unicast_enabled[0:0]

---------------------------------------
Working on:
multicast_metadata.ipv4_multicast_enabled <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 4 (10 bits)
   max_share = 36 (116 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  multicast_metadata.ipv4_multicast_enabled [0:0]
----------------------------------------------------------------------------------------------------------------
|                    Name                   | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------------------
| multicast_metadata.ipv4_multicast_enabled | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
----------------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised 1 -- ingress promised 1 and remain 10 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv69 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv136 -- fails False
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: multicast_metadata.ipv4_multicast_enabled <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv136[15:15] for multicast_metadata.ipv4_multicast_enabled[0:0]

---------------------------------------
Working on:
multicast_metadata.ipv6_multicast_enabled <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 4 (10 bits)
   max_share = 35 (115 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  multicast_metadata.ipv6_multicast_enabled [0:0]
----------------------------------------------------------------------------------------------------------------
|                    Name                   | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------------------
| multicast_metadata.ipv6_multicast_enabled | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
----------------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised 1 -- ingress promised 1 and remain 10 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv69 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: multicast_metadata.ipv6_multicast_enabled <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv14[16:16] for multicast_metadata.ipv6_multicast_enabled[0:0]

---------------------------------------
Working on:
multicast_metadata.igmp_snooping_enabled <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 34 (114 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  multicast_metadata.igmp_snooping_enabled [0:0]
---------------------------------------------------------------------------------------------------------------
|                   Name                   | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------------------
| multicast_metadata.igmp_snooping_enabled | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
---------------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised 1 -- ingress promised 1 and remain 10 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv69 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: multicast_metadata.igmp_snooping_enabled <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv14[17:17] for multicast_metadata.igmp_snooping_enabled[0:0]

---------------------------------------
Working on:
multicast_metadata.mld_snooping_enabled <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 33 (113 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  multicast_metadata.mld_snooping_enabled [0:0]
--------------------------------------------------------------------------------------------------------------
|                   Name                  | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------------------------
| multicast_metadata.mld_snooping_enabled | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
--------------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised 1 -- ingress promised 1 and remain 10 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv69 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: multicast_metadata.mld_snooping_enabled <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv14[18:18] for multicast_metadata.mld_snooping_enabled[0:0]

---------------------------------------
Working on:
multicast_metadata.mcast_bridge_hit <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 25 (84 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  multicast_metadata.mcast_bridge_hit [0:0]
----------------------------------------------------------------------------------------------------------
|                 Name                | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------------
| multicast_metadata.mcast_bridge_hit | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
----------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised 1 -- ingress promised 1 and remain 10 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv69 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv182 -- fails False
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv223 -- fails False
Metadata instance: multicast_metadata.mcast_bridge_hit <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv223[3:3] for multicast_metadata.mcast_bridge_hit[0:0]

---------------------------------------
Working on:
acl_metadata.acl_stats_index <12 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 3 (9 bits)
   max_share = 24 (92 bits)
bits_will_need_to_parse = 12
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (12 bits)
  acl_metadata.acl_stats_index [11:0]
---------------------------------------------------------------------------------------------------
|             Name             | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------
| acl_metadata.acl_stats_index | 12 |   False   |  -  |  -   |     -     |   None   |     1      |
---------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 11 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv182 -- fails False
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: acl_metadata.acl_stats_index <12 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv182[11:0] for acl_metadata.acl_stats_index[11:0]

---------------------------------------
Working on:
ig_intr_md_for_mb.ingress_mirror_id <10 bits ingress imeta W>
max_split = 1, adj = False
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 23 (82 bits)
bits_will_need_to_parse = 10
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (10 bits)
  ig_intr_md_for_mb.ingress_mirror_id [9:0]
----------------------------------------------------------------------------------------------------------
|                 Name                | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------------
| ig_intr_md_for_mb.ingress_mirror_id | 10 |   False   |  -  |  -   |    [8]    |    1     |     1      |
----------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [[8]]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 3 32 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 1 -- egress promised 0 and remain 2 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 4 and req 0 -- as if deparsed True -- container_to_use None -- fails True
Metadata instance: ig_intr_md_for_mb.ingress_mirror_id <10 bits ingress imeta W>
>>req_alignment = None
>>allowed_container_start_bits = [0]
>>req_container = None
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 10
>> HEY!:  Adjusted msb_offset!
***Allocating phv58[9:0] for ig_intr_md_for_mb.ingress_mirror_id[9:0]

---------------------------------------
Working on:
ig_intr_md_for_tm.drop_ctl <3 bits ingress imeta W>
max_split = 1, adj = False
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 24 (92 bits)
bits_will_need_to_parse = 3
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (3 bits)
  ig_intr_md_for_tm.drop_ctl [2:0]
-------------------------------------------------------------------------------------------------
|            Name            | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------
| ig_intr_md_for_tm.drop_ctl | 3  |   False   |  -  |  -   |     -     |    1     |     1      |
-------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 3 32 bits -- avail 3 and promised 1 -- ingress promised 1 and remain 2 and req 1 -- egress promised 0 and remain 2 and req 0 -- as if deparsed True -- container_to_use phv59 -- fails False
  Group 4 8 bits -- avail 11 and promised 1 -- ingress promised 1 and remain 10 and req 1 -- egress promised 0 and remain 8 and req 0 -- as if deparsed True -- container_to_use phv69 -- fails False
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 4 and req 0 -- as if deparsed True -- container_to_use None -- fails True
Metadata instance: ig_intr_md_for_tm.drop_ctl <3 bits ingress imeta W>
>>req_alignment = None
>>allowed_container_start_bits = [0, 1, 2, 3, 4, 5]
>>req_container = None
  case 2: looking at allowed start bits [0, 1, 2, 3, 4, 5]
    final start_bit = 5
  (1) msb_offset = 8
***Allocating phv69[7:5] for ig_intr_md_for_tm.drop_ctl[2:0]

---------------------------------------
Working on:
l2_metadata.l2_redirect <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 23 (91 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  l2_metadata.l2_redirect [0:0]
----------------------------------------------------------------------------------------------
|           Name          | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------
| l2_metadata.l2_redirect | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
----------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 3 and promised 1 -- ingress promised 1 and remain 2 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv59 -- fails False
  Group 4 8 bits -- avail 10 and promised 0 -- ingress promised 0 and remain 10 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed True -- container_to_use phv69 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: l2_metadata.l2_redirect <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv69[0:0] for l2_metadata.l2_redirect[0:0]

---------------------------------------
Working on:
l2_metadata.l2_dst_miss <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 22 (90 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  l2_metadata.l2_dst_miss [0:0]
----------------------------------------------------------------------------------------------
|           Name          | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------
| l2_metadata.l2_dst_miss | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
----------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 3 and promised 1 -- ingress promised 1 and remain 2 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv59 -- fails False
  Group 4 8 bits -- avail 10 and promised 0 -- ingress promised 0 and remain 10 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed True -- container_to_use phv69 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: l2_metadata.l2_dst_miss <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv69[1:1] for l2_metadata.l2_dst_miss[0:0]

---------------------------------------
Working on:
acl_metadata.acl_deny <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 22 (81 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  acl_metadata.acl_deny [0:0]
--------------------------------------------------------------------------------------------
|          Name         | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------
| acl_metadata.acl_deny | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
--------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 3 and promised 0 -- ingress promised 0 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 10 and promised 1 -- ingress promised 1 and remain 9 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv70 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: acl_metadata.acl_deny <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv14[19:19] for acl_metadata.acl_deny[0:0]

---------------------------------------
Working on:
acl_metadata.acl_redirect <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 21 (80 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  acl_metadata.acl_redirect [0:0]
------------------------------------------------------------------------------------------------
|            Name           | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------
| acl_metadata.acl_redirect | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 3 and promised 0 -- ingress promised 0 and remain 3 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 10 and promised 1 -- ingress promised 1 and remain 9 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv70 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: acl_metadata.acl_redirect <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv14[20:20] for acl_metadata.acl_redirect[0:0]

---------------------------------------
Working on:
multicast_metadata.mcast_copy_to_cpu <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 22 (80 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  multicast_metadata.mcast_copy_to_cpu [0:0]
-----------------------------------------------------------------------------------------------------------
|                 Name                 | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------------
| multicast_metadata.mcast_copy_to_cpu | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
-----------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 3 and promised 1 -- ingress promised 1 and remain 2 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv59 -- fails False
  Group 4 8 bits -- avail 10 and promised 1 -- ingress promised 1 and remain 9 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv70 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: multicast_metadata.mcast_copy_to_cpu <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv70[0:0] for multicast_metadata.mcast_copy_to_cpu[0:0]

---------------------------------------
Working on:
ig_intr_md_for_tm.level1_exclusion_id <16 bits ingress imeta W>
max_split = 1, adj = False
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 31 (105 bits)
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (16 bits)
  ig_intr_md_for_tm.level1_exclusion_id [15:0]
------------------------------------------------------------------------------------------------------------
|                  Name                 | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------------------
| ig_intr_md_for_tm.level1_exclusion_id | 16 |   False   |  -  |  -   |    [8]    |    1     |     1      |
------------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [[8]]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 3 32 bits -- avail 3 and promised 1 -- ingress promised 1 and remain 2 and req 1 -- egress promised 0 and remain 2 and req 0 -- as if deparsed True -- container_to_use phv59 -- fails False
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 4 and req 0 -- as if deparsed True -- container_to_use None -- fails True
Metadata instance: ig_intr_md_for_tm.level1_exclusion_id <16 bits ingress imeta W>
>>req_alignment = None
>>allowed_container_start_bits = [0]
>>req_container = None
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 16
>> HEY!:  Adjusted msb_offset!
***Allocating phv59[15:0] for ig_intr_md_for_tm.level1_exclusion_id[15:0]

---------------------------------------
Working on:
ingress_metadata.egress_port_lag_index <10 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 20 (64 bits)
bits_will_need_to_parse = 10
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (10 bits)
  ingress_metadata.egress_port_lag_index [9:0]
-------------------------------------------------------------------------------------------------------------
|                  Name                  | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------------------
| ingress_metadata.egress_port_lag_index | 10 |   False   |  -  |  -   |     -     |   None   |     1      |
-------------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 0 -- ingress promised 0 and remain 2 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed True -- container_to_use phv59 -- fails False
  Group 4 8 bits -- avail 9 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: ingress_metadata.egress_port_lag_index <10 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv59[25:16] for ingress_metadata.egress_port_lag_index[9:0]

---------------------------------------
Working on:
ingress_metadata.drop_reason <8 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 16 (36 bits)
bits_will_need_to_parse = 8
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (8 bits)
  ingress_metadata.drop_reason [7:0]
---------------------------------------------------------------------------------------------------
|             Name             | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------
| ingress_metadata.drop_reason | 8  |   False   |  -  |  -   |     -     |   None   |     1      |
---------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 1 -- ingress promised 1 and remain 1 and req 0 -- egress promised 0 and remain 1 and req 0 -- as if deparsed False -- container_to_use phv62 -- fails False
  Group 4 8 bits -- avail 9 and promised 1 -- ingress promised 1 and remain 8 and req 0 -- egress promised 0 and remain 8 and req 0 -- as if deparsed False -- container_to_use phv71 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: ingress_metadata.drop_reason <8 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv71[7:0] for ingress_metadata.drop_reason[7:0]

---------------------------------------
Working on:
acl_metadata.ingress_src_port_range_id <8 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 4 (10 bits)
   max_share = 20 (64 bits)
bits_will_need_to_parse = 8
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (8 bits)
  acl_metadata.ingress_src_port_range_id [7:0]
-------------------------------------------------------------------------------------------------------------
|                  Name                  | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------------------
| acl_metadata.ingress_src_port_range_id | 8  |   False   |  -  |  -   |     -     |   None   |     1      |
-------------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 1 -- ingress promised 1 and remain 1 and req 0 -- egress promised 0 and remain 1 and req 0 -- as if deparsed False -- container_to_use phv62 -- fails False
  Group 4 8 bits -- avail 8 and promised 1 -- ingress promised 1 and remain 7 and req 0 -- egress promised 0 and remain 7 and req 0 -- as if deparsed False -- container_to_use phv72 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: acl_metadata.ingress_src_port_range_id <8 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv72[7:0] for acl_metadata.ingress_src_port_range_id[7:0]

---------------------------------------
Working on:
acl_metadata.ingress_dst_port_range_id <8 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 4 (10 bits)
   max_share = 20 (64 bits)
bits_will_need_to_parse = 8
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (8 bits)
  acl_metadata.ingress_dst_port_range_id [7:0]
-------------------------------------------------------------------------------------------------------------
|                  Name                  | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------------------
| acl_metadata.ingress_dst_port_range_id | 8  |   False   |  -  |  -   |     -     |   None   |     1      |
-------------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 1 -- ingress promised 1 and remain 1 and req 0 -- egress promised 0 and remain 1 and req 0 -- as if deparsed False -- container_to_use phv62 -- fails False
  Group 4 8 bits -- avail 7 and promised 1 -- ingress promised 1 and remain 6 and req 0 -- egress promised 0 and remain 6 and req 0 -- as if deparsed False -- container_to_use phv73 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: acl_metadata.ingress_dst_port_range_id <8 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv73[7:0] for acl_metadata.ingress_dst_port_range_id[7:0]

---------------------------------------
Working on:
l2_metadata.stp_state <3 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 25 (67 bits)
bits_will_need_to_parse = 3
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (3 bits)
  l2_metadata.stp_state [2:0]
--------------------------------------------------------------------------------------------
|          Name         | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------
| l2_metadata.stp_state | 3  |   False   |  -  |  -   |     -     |   None   |     1      |
--------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 0 -- ingress promised 0 and remain 2 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 6 and promised 0 -- ingress promised 0 and remain 6 and req 0 -- egress promised 0 and remain 6 and req 0 -- as if deparsed True -- container_to_use phv69 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: l2_metadata.stp_state <3 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv69[4:2] for l2_metadata.stp_state[2:0]

---------------------------------------
Working on:
ingress_metadata.drop_flag <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 15 (33 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  ingress_metadata.drop_flag [0:0]
-------------------------------------------------------------------------------------------------
|            Name            | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------
| ingress_metadata.drop_flag | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
-------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 1 -- ingress promised 1 and remain 1 and req 0 -- egress promised 0 and remain 1 and req 0 -- as if deparsed False -- container_to_use phv62 -- fails False
  Group 4 8 bits -- avail 6 and promised 1 -- ingress promised 1 and remain 5 and req 0 -- egress promised 0 and remain 5 and req 0 -- as if deparsed False -- container_to_use phv74 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: ingress_metadata.drop_flag <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv74[0:0] for ingress_metadata.drop_flag[0:0]

---------------------------------------
Working on:
l2_metadata.l2_src_miss <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 19 (61 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  l2_metadata.l2_src_miss [0:0]
----------------------------------------------------------------------------------------------
|           Name          | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------
| l2_metadata.l2_src_miss | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
----------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 1 -- ingress promised 1 and remain 1 and req 0 -- egress promised 0 and remain 1 and req 0 -- as if deparsed False -- container_to_use phv62 -- fails False
  Group 4 8 bits -- avail 5 and promised 1 -- ingress promised 1 and remain 4 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv75 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: l2_metadata.l2_src_miss <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv75[0:0] for l2_metadata.l2_src_miss[0:0]

---------------------------------------
Working on:
l3_metadata.rmac_hit <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 19 (46 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  l3_metadata.rmac_hit [0:0]
-------------------------------------------------------------------------------------------
|         Name         | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------
| l3_metadata.rmac_hit | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
-------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 1 -- ingress promised 1 and remain 1 and req 0 -- egress promised 0 and remain 1 and req 0 -- as if deparsed False -- container_to_use phv62 -- fails False
  Group 4 8 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 4 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv70 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: l3_metadata.rmac_hit <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv70[1:1] for l3_metadata.rmac_hit[0:0]

---------------------------------------
Working on:
ipv6_metadata.ipv6_src_is_link_local <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 19 (61 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  ipv6_metadata.ipv6_src_is_link_local [0:0]
-----------------------------------------------------------------------------------------------------------
|                 Name                 | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------------
| ipv6_metadata.ipv6_src_is_link_local | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
-----------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 1 -- ingress promised 1 and remain 1 and req 0 -- egress promised 0 and remain 1 and req 0 -- as if deparsed False -- container_to_use phv62 -- fails False
  Group 4 8 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 4 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv74 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: ipv6_metadata.ipv6_src_is_link_local <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv74[1:1] for ipv6_metadata.ipv6_src_is_link_local[0:0]

---------------------------------------
Working on:
acl_metadata.racl_stats_index <12 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 19 (51 bits)
bits_will_need_to_parse = 12
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (12 bits)
  acl_metadata.racl_stats_index [11:0]
----------------------------------------------------------------------------------------------------
|              Name             | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------
| acl_metadata.racl_stats_index | 12 |   False   |  -  |  -   |     -     |   None   |     1      |
----------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 0 -- ingress promised 0 and remain 2 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 4 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: acl_metadata.racl_stats_index <12 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv58[21:10] for acl_metadata.racl_stats_index[11:0]

---------------------------------------
Working on:
ig_intr_md_for_tm.qid <5 bits ingress imeta W>
max_split = 1, adj = False
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 21 (49 bits)
bits_will_need_to_parse = 5
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (5 bits)
  ig_intr_md_for_tm.qid [4:0]
--------------------------------------------------------------------------------------------
|          Name         | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------
| ig_intr_md_for_tm.qid | 5  |   False   |  -  |  -   |     -     |    1     |     1      |
--------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv7 -- fails False
  Group 2 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv37 -- fails False
  Group 3 32 bits -- avail 2 and promised 1 -- ingress promised 1 and remain 1 and req 1 -- egress promised 0 and remain 1 and req 0 -- as if deparsed True -- container_to_use phv62 -- fails False
  Group 4 8 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 4 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed True -- container_to_use phv72 -- fails False
  Group 6 8 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv105 -- fails False
  Group 7 8 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv124 -- fails False
  Group 8 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv141 -- fails False
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv201 -- fails False
  Group 13 16 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed True -- container_to_use phv219 -- fails False
Metadata instance: ig_intr_md_for_tm.qid <5 bits ingress imeta W>
>>req_alignment = None
>>allowed_container_start_bits = [0, 1, 2, 3]
>>req_container = None
  case 2: looking at allowed start bits [0, 1, 2, 3]
    final start_bit = 3
  (1) msb_offset = 8
***Allocating phv72[7:3] for ig_intr_md_for_tm.qid[4:0]

---------------------------------------
Working on:
ig_intr_md_for_tm.ingress_cos <3 bits ingress imeta W>
max_split = 1, adj = False
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 20 (46 bits)
bits_will_need_to_parse = 3
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (3 bits)
  ig_intr_md_for_tm.ingress_cos [2:0]
----------------------------------------------------------------------------------------------------
|              Name             | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------
| ig_intr_md_for_tm.ingress_cos | 3  |   False   |  -  |  -   |     -     |    1     |     1      |
----------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv7 -- fails False
  Group 2 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv37 -- fails False
  Group 3 32 bits -- avail 2 and promised 1 -- ingress promised 1 and remain 1 and req 1 -- egress promised 0 and remain 1 and req 0 -- as if deparsed True -- container_to_use phv62 -- fails False
  Group 4 8 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 4 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv73 -- fails False
  Group 6 8 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv105 -- fails False
  Group 7 8 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv124 -- fails False
  Group 8 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv141 -- fails False
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv201 -- fails False
  Group 13 16 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed True -- container_to_use phv219 -- fails False
Metadata instance: ig_intr_md_for_tm.ingress_cos <3 bits ingress imeta W>
>>req_alignment = None
>>allowed_container_start_bits = [0, 1, 2, 3, 4, 5]
>>req_container = None
  case 2: looking at allowed start bits [0, 1, 2, 3, 4, 5]
    final start_bit = 5
  (1) msb_offset = 8
***Allocating phv73[7:5] for ig_intr_md_for_tm.ingress_cos[2:0]

---------------------------------------
Working on:
l3_metadata.lkp_ip_type <2 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 15 (16 bits)
bits_will_need_to_parse = 2
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (2 bits)
  l3_metadata.lkp_ip_type [1:0]
----------------------------------------------------------------------------------------------
|           Name          | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------
| l3_metadata.lkp_ip_type | 2  |   False   |  -  |  -   |     -     |   None   |     1      |
----------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 0 -- ingress promised 0 and remain 2 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 4 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv70 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: l3_metadata.lkp_ip_type <2 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv70[3:2] for l3_metadata.lkp_ip_type[1:0]

---------------------------------------
Working on:
ig_intr_md_for_tm.copy_to_cpu <1 bits ingress imeta W>
max_split = 1, adj = False
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 18 (43 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  ig_intr_md_for_tm.copy_to_cpu [0:0]
----------------------------------------------------------------------------------------------------
|              Name             | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------
| ig_intr_md_for_tm.copy_to_cpu | 1  |   False   |  -  |  -   |     -     |    1     |     1      |
----------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv7 -- fails False
  Group 2 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv37 -- fails False
  Group 3 32 bits -- avail 2 and promised 1 -- ingress promised 1 and remain 1 and req 1 -- egress promised 0 and remain 1 and req 0 -- as if deparsed True -- container_to_use phv62 -- fails False
  Group 4 8 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 4 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv70 -- fails False
  Group 6 8 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv105 -- fails False
  Group 7 8 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv124 -- fails False
  Group 8 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv141 -- fails False
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv201 -- fails False
  Group 13 16 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed True -- container_to_use phv219 -- fails False
Metadata instance: ig_intr_md_for_tm.copy_to_cpu <1 bits ingress imeta W>
>>req_alignment = None
>>allowed_container_start_bits = [0, 1, 2, 3, 4, 5, 6, 7]
>>req_container = None
  case 2: looking at allowed start bits [0, 1, 2, 3, 4, 5, 6, 7]
    final start_bit = 7
  (1) msb_offset = 8
***Allocating phv105[7:7] for ig_intr_md_for_tm.copy_to_cpu[0:0]

---------------------------------------
Working on:
l3_metadata.lkp_ip_llmc <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 13 (14 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  l3_metadata.lkp_ip_llmc [0:0]
----------------------------------------------------------------------------------------------
|           Name          | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------
| l3_metadata.lkp_ip_llmc | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
----------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 0 -- ingress promised 0 and remain 2 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 4 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv70 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: l3_metadata.lkp_ip_llmc <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv70[4:4] for l3_metadata.lkp_ip_llmc[0:0]

---------------------------------------
Working on:
l3_metadata.lkp_ip_mc <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 12 (13 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  l3_metadata.lkp_ip_mc [0:0]
--------------------------------------------------------------------------------------------
|          Name         | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------
| l3_metadata.lkp_ip_mc | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
--------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 0 -- ingress promised 0 and remain 2 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 4 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv70 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: l3_metadata.lkp_ip_mc <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv70[5:5] for l3_metadata.lkp_ip_mc[0:0]

---------------------------------------
Working on:
l3_metadata.l3_copy <1 bits ingress meta R>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 15 (40 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  l3_metadata.l3_copy [0:0]
------------------------------------------------------------------------------------------
|         Name        | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------
| l3_metadata.l3_copy | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 0 -- ingress promised 0 and remain 2 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 4 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv70 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv206 -- fails False
  Group 13 16 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv223 -- fails False
Metadata instance: l3_metadata.l3_copy <1 bits ingress meta R>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv70[6:6] for l3_metadata.l3_copy[0:0]

---------------------------------------
Working on:
acl_metadata.racl_deny <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 11 (36 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  acl_metadata.racl_deny [0:0]
---------------------------------------------------------------------------------------------
|          Name          | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------
| acl_metadata.racl_deny | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
---------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 0 -- ingress promised 0 and remain 2 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 4 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv70 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: acl_metadata.racl_deny <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv70[7:7] for acl_metadata.racl_deny[0:0]

---------------------------------------
Working on:
acl_metadata.racl_redirect <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 10 (35 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  acl_metadata.racl_redirect [0:0]
-------------------------------------------------------------------------------------------------
|            Name            | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------
| acl_metadata.racl_redirect | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
-------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 0 -- ingress promised 0 and remain 2 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 4 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv74 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: acl_metadata.racl_redirect <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv74[2:2] for acl_metadata.racl_redirect[0:0]

---------------------------------------
Working on:
multicast_metadata.mcast_rpf_fail <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 11 (36 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  multicast_metadata.mcast_rpf_fail [0:0]
--------------------------------------------------------------------------------------------------------
|                Name               | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------------------
| multicast_metadata.mcast_rpf_fail | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
--------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 0 -- ingress promised 0 and remain 2 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 4 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv74 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv223 -- fails False
Metadata instance: multicast_metadata.mcast_rpf_fail <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv74[3:3] for multicast_metadata.mcast_rpf_fail[0:0]

---------------------------------------
Working on:
multicast_metadata.flood_to_mrouters <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 10 (35 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  multicast_metadata.flood_to_mrouters [0:0]
-----------------------------------------------------------------------------------------------------------
|                 Name                 | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------------
| multicast_metadata.flood_to_mrouters | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
-----------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 0 -- ingress promised 0 and remain 2 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 4 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv74 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv223 -- fails False
Metadata instance: multicast_metadata.flood_to_mrouters <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv74[4:4] for multicast_metadata.flood_to_mrouters[0:0]

---------------------------------------
Working on:
nexthop_metadata.nexthop_glean <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 10 (35 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  nexthop_metadata.nexthop_glean [0:0]
-----------------------------------------------------------------------------------------------------
|              Name              | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------------
| nexthop_metadata.nexthop_glean | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
-----------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv7 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv37 -- fails False
  Group 3 32 bits -- avail 2 and promised 0 -- ingress promised 0 and remain 2 and req 0 -- egress promised 0 and remain 2 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 4 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv74 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv109 -- fails False
  Group 7 8 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv124 -- fails False
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv175 -- fails False
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv201 -- fails False
  Group 13 16 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv223 -- fails False
Metadata instance: nexthop_metadata.nexthop_glean <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
  (2b) msb_offset = 8
***Allocating phv109[7:7] for nexthop_metadata.nexthop_glean[0:0]

---------------------------------------
Working on:
ig_intr_md_for_tm.level2_exclusion_id <9 bits ingress imeta W>
max_split = 1, adj = False
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 8 (10 bits)
bits_will_need_to_parse = 9
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (9 bits)
  ig_intr_md_for_tm.level2_exclusion_id [8:0]
------------------------------------------------------------------------------------------------------------
|                  Name                 | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------------------------
| ig_intr_md_for_tm.level2_exclusion_id | 9  |   False   |  -  |  -   |    [8]    |    1     |     1      |
------------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [[8]]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 3 32 bits -- avail 2 and promised 1 -- ingress promised 1 and remain 1 and req 1 -- egress promised 0 and remain 1 and req 0 -- as if deparsed True -- container_to_use phv62 -- fails False
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 4 and req 0 -- as if deparsed True -- container_to_use None -- fails True
Metadata instance: ig_intr_md_for_tm.level2_exclusion_id <9 bits ingress imeta W>
>>req_alignment = None
>>allowed_container_start_bits = [0]
>>req_container = None
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 9
>> HEY!:  Adjusted msb_offset!
***Allocating phv62[8:0] for ig_intr_md_for_tm.level2_exclusion_id[8:0]

---------------------------------------
Working on:
l2_metadata.port_learning_enabled <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 7 (9 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  l2_metadata.port_learning_enabled [0:0]
--------------------------------------------------------------------------------------------------------
|                Name               | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------------------
| l2_metadata.port_learning_enabled | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
--------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 1 and promised 0 -- ingress promised 0 and remain 1 and req 0 -- egress promised 0 and remain 1 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 4 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv75 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: l2_metadata.port_learning_enabled <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv75[1:1] for l2_metadata.port_learning_enabled[0:0]

---------------------------------------
Working on:
l3_metadata.fib_hit <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 4 (20 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  l3_metadata.fib_hit [0:0]
------------------------------------------------------------------------------------------
|         Name        | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
------------------------------------------------------------------------------------------
| l3_metadata.fib_hit | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 1 and promised 0 -- ingress promised 0 and remain 1 and req 0 -- egress promised 0 and remain 1 and req 0 -- as if deparsed True -- container_to_use phv62 -- fails False
  Group 4 8 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv76 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: l3_metadata.fib_hit <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv62[9:9] for l3_metadata.fib_hit[0:0]

---------------------------------------
Working on:
l3_metadata.fib_hit_myip <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 4 (20 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  l3_metadata.fib_hit_myip [0:0]
-----------------------------------------------------------------------------------------------
|           Name           | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------
| l3_metadata.fib_hit_myip | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
-----------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 1 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv63 -- fails False
  Group 4 8 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 3 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv76 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: l3_metadata.fib_hit_myip <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv76[0:0] for l3_metadata.fib_hit_myip[0:0]

---------------------------------------
Working on:
meter_metadata.storm_control_color <2 bits ingress meta R W>
max_split = 1, adj = False
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 3 (4 bits)
bits_will_need_to_parse = 2
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (2 bits)
  meter_metadata.storm_control_color [1:0]
---------------------------------------------------------------------------------------------------------
|                Name                | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------------
| meter_metadata.storm_control_color | 2  |   False   |  -  |  -   |     -     |    1     |     1      |
---------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 1 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv63 -- fails False
  Group 4 8 bits -- avail 3 and promised 1 -- ingress promised 1 and remain 2 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv77 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: meter_metadata.storm_control_color <2 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv77[1:0] for meter_metadata.storm_control_color[1:0]

---------------------------------------
Working on:
multicast_metadata.mcast_mode <2 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 2 (17 bits)
bits_will_need_to_parse = 2
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (2 bits)
  multicast_metadata.mcast_mode [1:0]
----------------------------------------------------------------------------------------------------
|              Name             | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------
| multicast_metadata.mcast_mode | 2  |   False   |  -  |  -   |     -     |   None   |     1      |
----------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 1 and promised 0 -- ingress promised 0 and remain 1 and req 0 -- egress promised 0 and remain 1 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 2 and promised 0 -- ingress promised 0 and remain 2 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv74 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv223 -- fails False
Metadata instance: multicast_metadata.mcast_mode <2 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv74[6:5] for multicast_metadata.mcast_mode[1:0]

---------------------------------------
Working on:
multicast_metadata.mcast_route_hit <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 2 (17 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  multicast_metadata.mcast_route_hit [0:0]
---------------------------------------------------------------------------------------------------------
|                Name                | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------------
| multicast_metadata.mcast_route_hit | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
---------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 1 and promised 0 -- ingress promised 0 and remain 1 and req 0 -- egress promised 0 and remain 1 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 2 and promised 0 -- ingress promised 0 and remain 2 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv75 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv223 -- fails False
Metadata instance: multicast_metadata.mcast_route_hit <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv75[2:2] for multicast_metadata.mcast_route_hit[0:0]

---------------------------------------
Working on:
multicast_metadata.mcast_route_s_g_hit <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 2 (17 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  multicast_metadata.mcast_route_s_g_hit [0:0]
-------------------------------------------------------------------------------------------------------------
|                  Name                  | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------------------
| multicast_metadata.mcast_route_s_g_hit | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
-------------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv14 -- fails False
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 1 and promised 0 -- ingress promised 0 and remain 1 and req 0 -- egress promised 0 and remain 1 and req 0 -- as if deparsed True -- container_to_use phv58 -- fails False
  Group 4 8 bits -- avail 2 and promised 0 -- ingress promised 0 and remain 2 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv76 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 4 and req 0 -- as if deparsed False -- container_to_use phv223 -- fails False
Metadata instance: multicast_metadata.mcast_route_s_g_hit <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv76[1:1] for multicast_metadata.mcast_route_s_g_hit[0:0]

---------------------------------------
Working on:
ig_intr_md_for_tm.rid <16 bits ingress imeta W>
max_split = 1, adj = False
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 1 (1 bits)
bits_will_need_to_parse = 16
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (16 bits)
  ig_intr_md_for_tm.rid [15:0]
--------------------------------------------------------------------------------------------
|          Name         | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------
| ig_intr_md_for_tm.rid | 16 |   False   |  -  |  -   |    [8]    |    1     |     1      |
--------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [[8]]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 3 32 bits -- avail 1 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv63 -- fails False
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 1 -- egress promised 0 and remain 4 and req 0 -- as if deparsed True -- container_to_use None -- fails True
Metadata instance: ig_intr_md_for_tm.rid <16 bits ingress imeta W>
>>req_alignment = None
>>allowed_container_start_bits = [0]
>>req_container = None
  case 2: looking at allowed start bits [0]
    final start_bit = 0
  (1) msb_offset = 16
>> HEY!:  Adjusted msb_offset!
***Allocating phv63[15:0] for ig_intr_md_for_tm.rid[15:0]

---------------------------------------
Working on:
l3_metadata.urpf_check_fail <1 bits ingress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 0 (0 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  l3_metadata.urpf_check_fail [0:0]
--------------------------------------------------------------------------------------------------
|             Name            | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------------
| l3_metadata.urpf_check_fail | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
--------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed True -- container_to_use phv63 -- fails False
  Group 4 8 bits -- avail 2 and promised 1 -- ingress promised 1 and remain 1 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv78 -- fails False
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 1 and remain 0 and req 0 -- egress promised 0 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: l3_metadata.urpf_check_fail <1 bits ingress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv63[16:16] for l3_metadata.urpf_check_fail[0:0]

---------------------------------------
Working on:
egress_metadata.smac_idx <9 bits egress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 7 (45 bits)
bits_will_need_to_parse = 9
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (9 bits)
  egress_metadata.smac_idx [8:0]
-----------------------------------------------------------------------------------------------
|           Name           | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-----------------------------------------------------------------------------------------------
| egress_metadata.smac_idx | 9  |   False   |  -  |  -   |     -     |   None   |     1      |
-----------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 4 8 bits -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv156 -- fails False
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: egress_metadata.smac_idx <9 bits egress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
  (2b) msb_offset = 16
***Allocating phv156[15:7] for egress_metadata.smac_idx[8:0]

---------------------------------------
Working on:
l3_metadata.mtu_index <8 bits egress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 6 (37 bits)
bits_will_need_to_parse = 8
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (8 bits)
  l3_metadata.mtu_index [7:0]
--------------------------------------------------------------------------------------------
|          Name         | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------
| l3_metadata.mtu_index | 8  |   False   |  -  |  -   |     -     |   None   |     1      |
--------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 4 8 bits -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 4 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 3 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: l3_metadata.mtu_index <8 bits egress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv212[7:0] for l3_metadata.mtu_index[7:0]

---------------------------------------
Working on:
tunnel_metadata.tunnel_index <14 bits egress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 6 (31 bits)
bits_will_need_to_parse = 14
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (14 bits)
  tunnel_metadata.tunnel_index [13:0]
---------------------------------------------------------------------------------------------------
|             Name             | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------
| tunnel_metadata.tunnel_index | 14 |   False   |  -  |  -   |     -     |   None   |     1      |
---------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 4 8 bits -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 3 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 2 and req 0 -- as if deparsed False -- container_to_use phv213 -- fails False
Metadata instance: tunnel_metadata.tunnel_index <14 bits egress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv213[13:0] for tunnel_metadata.tunnel_index[13:0]

---------------------------------------
Working on:
tunnel_metadata.tunnel_dmac_index <12 bits egress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 5 (19 bits)
bits_will_need_to_parse = 12
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 16
Parse state 0 (12 bits)
  tunnel_metadata.tunnel_dmac_index [11:0]
--------------------------------------------------------------------------------------------------------
|                Name               | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------------------
| tunnel_metadata.tunnel_dmac_index | 12 |   False   |  -  |  -   |     -     |   None   |     1      |
--------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 4 8 bits -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 2 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 1 and req 0 -- as if deparsed False -- container_to_use phv214 -- fails False
Metadata instance: tunnel_metadata.tunnel_dmac_index <12 bits egress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv214[11:0] for tunnel_metadata.tunnel_dmac_index[11:0]

---------------------------------------
Working on:
tunnel_metadata.tunnel_smac_index <8 bits egress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 1 (1 bits)
   max_share = 4 (11 bits)
bits_will_need_to_parse = 8
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (8 bits)
  tunnel_metadata.tunnel_smac_index [7:0]
--------------------------------------------------------------------------------------------------------
|                Name               | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
--------------------------------------------------------------------------------------------------------
| tunnel_metadata.tunnel_smac_index | 8  |   False   |  -  |  -   |     -     |   None   |     1      |
--------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv55 -- fails False
  Group 4 8 bits -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv102 -- fails False
  Group 7 8 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv117 -- fails False
  Group 8 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 1 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv215 -- fails False
Metadata instance: tunnel_metadata.tunnel_smac_index <8 bits egress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
  (3) msb_offset = 8
***Allocating phv102[7:0] for tunnel_metadata.tunnel_smac_index[7:0]

---------------------------------------
Working on:
tunnel_metadata.egress_tunnel_type <5 bits egress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 3 (6 bits)
bits_will_need_to_parse = 5
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (5 bits)
  tunnel_metadata.egress_tunnel_type [4:0]
---------------------------------------------------------------------------------------------------------
|                Name                | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
---------------------------------------------------------------------------------------------------------
| tunnel_metadata.egress_tunnel_type | 5  |   False   |  -  |  -   |     -     |   None   |     1      |
---------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 4 8 bits -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 1 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 1 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: tunnel_metadata.egress_tunnel_type <5 bits egress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv212[12:8] for tunnel_metadata.egress_tunnel_type[4:0]

---------------------------------------
Working on:
tunnel_metadata.egress_header_count <4 bits egress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 2 (2 bits)
bits_will_need_to_parse = 4
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (4 bits)
  tunnel_metadata.egress_header_count [3:0]
----------------------------------------------------------------------------------------------------------
|                 Name                | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
----------------------------------------------------------------------------------------------------------
| tunnel_metadata.egress_header_count | 4  |   False   |  -  |  -   |     -     |   None   |     1      |
----------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 4 8 bits -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 1 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 1 and req 0 -- as if deparsed False -- container_to_use phv214 -- fails False
Metadata instance: tunnel_metadata.egress_header_count <4 bits egress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv214[15:12] for tunnel_metadata.egress_header_count[3:0]

---------------------------------------
Working on:
multicast_metadata.inner_replica <1 bits egress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 1 (1 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  multicast_metadata.inner_replica [0:0]
-------------------------------------------------------------------------------------------------------
|               Name               | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------------
| multicast_metadata.inner_replica | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
-------------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 4 8 bits -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 0 and req 0 -- as if deparsed False -- container_to_use phv158 -- fails False
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 1 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 1 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: multicast_metadata.inner_replica <1 bits egress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv158[15:15] for multicast_metadata.inner_replica[0:0]

---------------------------------------
Working on:
multicast_metadata.replica <1 bits egress meta R W>
max_split = None, adj = None
Of remaining metadata fields to allocate
   max_overlay = 0 (0 bits)
   max_share = 0 (0 bits)
bits_will_need_to_parse = 1
unused_metadata_container_bits = 0
min_parse_states = 1
bits_per_state = 8
Parse state 0 (1 bits)
  multicast_metadata.replica [0:0]
-------------------------------------------------------------------------------------------------
|            Name            | BW | Tagalong? | Req | Pref | Not Allow | MaxSplit | Group Size |
-------------------------------------------------------------------------------------------------
| multicast_metadata.replica | 1  |   False   |  -  |  -   |     -     |   None   |     1      |
-------------------------------------------------------------------------------------------------

  req packing: [None]
  disallowed packing: [None]
  Group 0 32 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 1 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 2 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 3 32 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 4 8 bits -- avail 2 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 5 8 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 6 8 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 7 8 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 8 16 bits -- avail 0 and promised None -- ingress promised None and remain None and req None -- egress promised None and remain None and req None -- as if deparsed False -- container_to_use None -- fails True
  Group 9 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 10 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 11 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 12 16 bits -- avail 0 and promised 1 -- ingress promised 0 and remain 0 and req 0 -- egress promised 1 and remain 0 and req 0 -- as if deparsed False -- container_to_use None -- fails True
  Group 13 16 bits -- avail 1 and promised 0 -- ingress promised 0 and remain 0 and req 0 -- egress promised 0 and remain 1 and req 0 -- as if deparsed False -- container_to_use phv212 -- fails False
Metadata instance: multicast_metadata.replica <1 bits egress meta R W>
>>req_alignment = None
>>allowed_container_start_bits = None
>>req_container = None
***Allocating phv212[13:13] for multicast_metadata.replica[0:0]
>> Free metadata allocation took 16.60 seconds
>>Event 'pa_meta_init' at time 1573972425.54
   Took 19.30 seconds

-----------------------------------------------
  Adding metadata initialization
-----------------------------------------------

--------------------------------------------
Liveness ranges for container phv1
  phv1[31:16] = tunnel_metadata.tunnel_dst_index[15:0] -- 8 to 12
  phv1[15:0] = l3_metadata.nexthop_index[15:0] -- 7 to 12
  phv1[31:0] = ig_intr_md.ingress_mac_tstamp[31:0] -- -1 to 0
Initialization may be required for:
  l3_metadata.nexthop_index[15:0] -- 7 to 12
  tunnel_metadata.tunnel_dst_index[15:0] -- 8 to 12

Checking if l3_metadata.nexthop_index needs initialization.  Non exclusive with fields ['ig_intr_md.ingress_mac_tstamp'].
Looking at dominator: fwd_result
  looking at non excl field ig_intr_md.ingress_mac_tstamp
   leaf usage switch_config_params and R
      fwd_result cannot reach switch_config_params
  Candidate dominator: fwd_result because not above last usage of previous field
Looking at dominator: nexthop
  looking at non excl field ig_intr_md.ingress_mac_tstamp
   leaf usage switch_config_params and R
      nexthop cannot reach switch_config_params
  Candidate dominator: nexthop because not above last usage of previous field
Looking at dominator: _condition_39
  looking at non excl field ig_intr_md.ingress_mac_tstamp
   leaf usage switch_config_params and R
      _condition_39 cannot reach switch_config_params
  Candidate dominator: _condition_39 because not above last usage of previous field
Looking at dominator: ecmp_group
  looking at non excl field ig_intr_md.ingress_mac_tstamp
   leaf usage switch_config_params and R
      ecmp_group cannot reach switch_config_params
  Candidate dominator: ecmp_group because not above last usage of previous field
Looking at dominators_reach_previous_usage: fwd_result
Looking at dominators_reach_previous_usage: nexthop
Looking at dominators_reach_previous_usage: _condition_39
Looking at dominators_reach_previous_usage: ecmp_group
------
Initial usages for l3_metadata.nexthop_index.
  fwd_result: W
  nexthop: R
  ecmp_group: RW
Dominators for l3_metadata.nexthop_index: 4
  fwd_result
  nexthop
  _condition_39
  ecmp_group
Trimmed dominators for l3_metadata.nexthop_index (those that are not dominated by other dominators): 2
  fwd_result can reach ['_condition_39'] and who can reach me []
  _condition_39 can reach [] and who can reach me ['fwd_result']
fi = l3_metadata.nexthop_index (ingress), fi_earliest_use = 7
  squashed group dominator _condition_36 in stage 7
    Possible nodes to do clearing: (16)
      _condition_36 (stage 7)
      storm_control_stats (stage 7)
      acl_stats (stage 7)
      ingress_bd_stats (stage 7)
      compute_other_hashes (stage 7)
      _condition_34 (stage 6)
      _condition_12 (stage 3)
      _condition_11 (stage 2)
      _condition_10 (stage 2)
      _condition_7 (stage 1)
      _condition_6 (stage 1)
      _condition_5 (stage 1)
      port_vlan_to_ifindex_mapping (stage 0)
      _condition_4 (stage 0)
      validate_outer_ethernet (stage 0)
      switch_config_params (stage 0)
previous stages: 11
   _condition_34  (stage 6)
   _condition_12  (stage 3)
   _condition_11  (stage 2)
   _condition_10  (stage 2)
   _condition_7  (stage 1)
   _condition_6  (stage 1)
   _condition_5  (stage 1)
   port_vlan_to_ifindex_mapping  (stage 0)
   _condition_4  (stage 0)
   validate_outer_ethernet  (stage 0)
   switch_config_params  (stage 0)
current_stage stages: 5
   _condition_36  (stage 7)
   storm_control_stats  (stage 7)
   acl_stats  (stage 7)
   ingress_bd_stats  (stage 7)
   compute_other_hashes  (stage 7)
Call to _delay_init(_condition_34)
  _condition_34 is condition
    looking at _condition_34's out edge _condition_34 -- False (f=0.50) -- _condition_35
Call to _delay_init(_condition_35)
  _condition_35 is condition
    looking at _condition_35's out edge _condition_35 -- False (f=0.25) -- compute_non_ip_hashes
Call to _delay_init(compute_non_ip_hashes)
  compute_non_ip_hashes is match
    looking at compute_non_ip_hashes's out edge compute_non_ip_hashes -- compute_lkp_non_ip_hash (f=0.12) -- compute_other_hashes
      reaches node where usage is ['unused', 'W', 'R', 'RW']
        compute_other_hashes compute_non_ip_hashes -- compute_lkp_non_ip_hash (f=0.12) -- compute_other_hashes
    looking at compute_non_ip_hashes's out edge compute_non_ip_hashes -- --default-- (f=0.12) -- compute_other_hashes
      reaches node where usage is ['unused', 'W', 'R', 'RW']
        compute_other_hashes compute_non_ip_hashes -- --default-- (f=0.12) -- compute_other_hashes
      tbl _condition_35 can descend on edge edge _condition_35 -- False (f=0.25) -- compute_non_ip_hashes
        compute_other_hashes compute_non_ip_hashes -- compute_lkp_non_ip_hash (f=0.12) -- compute_other_hashes
        compute_other_hashes compute_non_ip_hashes -- --default-- (f=0.12) -- compute_other_hashes
    looking at _condition_35's out edge _condition_35 -- True (f=0.25) -- compute_ipv6_hashes
Call to _delay_init(compute_ipv6_hashes)
  compute_ipv6_hashes is match
    looking at compute_ipv6_hashes's out edge compute_ipv6_hashes -- compute_lkp_ipv6_hash (f=0.12) -- compute_other_hashes
      reaches node where usage is ['unused', 'W', 'R', 'RW']
        compute_other_hashes compute_ipv6_hashes -- compute_lkp_ipv6_hash (f=0.12) -- compute_other_hashes
    looking at compute_ipv6_hashes's out edge compute_ipv6_hashes -- --default-- (f=0.12) -- compute_other_hashes
      reaches node where usage is ['unused', 'W', 'R', 'RW']
        compute_other_hashes compute_ipv6_hashes -- --default-- (f=0.12) -- compute_other_hashes
      tbl _condition_35 can descend on edge edge _condition_35 -- True (f=0.25) -- compute_ipv6_hashes
        compute_other_hashes compute_ipv6_hashes -- compute_lkp_ipv6_hash (f=0.12) -- compute_other_hashes
        compute_other_hashes compute_ipv6_hashes -- --default-- (f=0.12) -- compute_other_hashes
      tbl _condition_34 can descend on edge edge _condition_34 -- False (f=0.50) -- _condition_35
        compute_other_hashes compute_non_ip_hashes -- compute_lkp_non_ip_hash (f=0.12) -- compute_other_hashes
        compute_other_hashes compute_non_ip_hashes -- --default-- (f=0.12) -- compute_other_hashes
        compute_other_hashes compute_ipv6_hashes -- compute_lkp_ipv6_hash (f=0.12) -- compute_other_hashes
        compute_other_hashes compute_ipv6_hashes -- --default-- (f=0.12) -- compute_other_hashes
    looking at _condition_34's out edge _condition_34 -- True (f=0.50) -- compute_ipv4_hashes
Call to _delay_init(compute_ipv4_hashes)
  compute_ipv4_hashes is match
    looking at compute_ipv4_hashes's out edge compute_ipv4_hashes -- compute_lkp_ipv4_hash (f=0.25) -- compute_other_hashes
      reaches node where usage is ['unused', 'W', 'R', 'RW']
        compute_other_hashes compute_ipv4_hashes -- compute_lkp_ipv4_hash (f=0.25) -- compute_other_hashes
    looking at compute_ipv4_hashes's out edge compute_ipv4_hashes -- --default-- (f=0.25) -- compute_other_hashes
      reaches node where usage is ['unused', 'W', 'R', 'RW']
        compute_other_hashes compute_ipv4_hashes -- --default-- (f=0.25) -- compute_other_hashes
      tbl _condition_34 can descend on edge edge _condition_34 -- True (f=0.50) -- compute_ipv4_hashes
        compute_other_hashes compute_ipv4_hashes -- compute_lkp_ipv4_hash (f=0.25) -- compute_other_hashes
        compute_other_hashes compute_ipv4_hashes -- --default-- (f=0.25) -- compute_other_hashes
Group dominators for l3_metadata.nexthop_index (common parents): 1
  _condition_36
Squashed group dominators for l3_metadata.nexthop_index (common parents): 1
  _condition_36
Ascend group dominators for l3_metadata.nexthop_index: 1
  _condition_34
Squashed ascend group dominators for l3_metadata.nexthop_index: 1
  _condition_34

Delayed init points (experimental): 6
  compute_other_hashes on edge compute_non_ip_hashes -- compute_lkp_non_ip_hash (f=0.12) -- compute_other_hashes
  compute_other_hashes on edge compute_non_ip_hashes -- --default-- (f=0.12) -- compute_other_hashes
  compute_other_hashes on edge compute_ipv6_hashes -- compute_lkp_ipv6_hash (f=0.12) -- compute_other_hashes
  compute_other_hashes on edge compute_ipv6_hashes -- --default-- (f=0.12) -- compute_other_hashes
  compute_other_hashes on edge compute_ipv4_hashes -- compute_lkp_ipv4_hash (f=0.25) -- compute_other_hashes
  compute_other_hashes on edge compute_ipv4_hashes -- --default-- (f=0.25) -- compute_other_hashes
New squashed group dominators for l3_metadata.nexthop_index (experimental): 1
  compute_other_hashes
Table to Inbound edges to initialize field l3_metadata.nexthop_index: 1
  _condition_34
    _condition_12 -- False (f=0.50) -- _condition_34
    _condition_31 -- False (f=0.06) -- _condition_34
    _condition_33 -- False (f=0.03) -- _condition_34
    ipv6_multicast_route_star_g -- multicast_route_star_g_miss (f=0.00) -- _condition_34
    ipv6_multicast_route_star_g -- multicast_route_sm_star_g_hit (f=0.00) -- _condition_34
    ipv6_multicast_route_star_g -- multicast_route_bidir_star_g_hit (f=0.00) -- _condition_34
    ipv6_multicast_route_star_g -- --default-- (f=0.00) -- _condition_34
    ipv6_multicast_route -- multicast_route_s_g_hit (f=0.01) -- _condition_34
    ipv6_multicast_route -- --default-- (f=0.01) -- _condition_34
    _condition_30 -- False (f=0.06) -- _condition_34
    ipv4_multicast_route_star_g -- multicast_route_star_g_miss (f=0.01) -- _condition_34
    ipv4_multicast_route_star_g -- multicast_route_sm_star_g_hit (f=0.01) -- _condition_34
    ipv4_multicast_route_star_g -- multicast_route_bidir_star_g_hit (f=0.01) -- _condition_34
    ipv4_multicast_route_star_g -- --default-- (f=0.01) -- _condition_34
    ipv4_multicast_route -- multicast_route_s_g_hit (f=0.02) -- _condition_34
    ipv4_multicast_route -- --default-- (f=0.02) -- _condition_34
    _condition_22 -- False (f=0.04) -- _condition_34
    _condition_27 -- False (f=0.02) -- _condition_34
    urpf_bd -- nop (f=0.01) -- _condition_34
    urpf_bd -- urpf_bd_miss (f=0.01) -- _condition_34
    urpf_bd -- --default-- (f=0.01) -- _condition_34
    rmac -- rmac_miss (f=0.08) -- _condition_34
    rmac -- --default-- (f=0.08) -- _condition_34
--------
Init at table for field l3_metadata.nexthop_index: 1
  _condition_34
Init at table at specific edge for field l3_metadata.nexthop_index: 0

Init at table: 1
  _condition_34
  Since dominator table _condition_34 is a gateway, need to clear l3_metadata.nexthop_index in table that precedes or inject a new table to clear the field to be 0.
    Possible nodes to do clearing: (10)
      _condition_12
      _condition_11
      _condition_10
      _condition_7
      _condition_6
      _condition_5
      port_vlan_to_ifindex_mapping
      _condition_4
      validate_outer_ethernet
      switch_config_params
    Clear field l3_metadata.nexthop_index at table port_vlan_to_ifindex_mapping.
>>Adding instruction: clear tbl=port_vlan_to_ifindex_mapping action=set_ingress_interface_properties, phv1[15:0] for field l3_metadata.nexthop_index[15:0]
>>Adding instruction: clear tbl=port_vlan_to_ifindex_mapping action=nop, phv1[15:0] for field l3_metadata.nexthop_index[15:0]

Checking if tunnel_metadata.tunnel_dst_index needs initialization.  Non exclusive with fields ['ig_intr_md.ingress_mac_tstamp'].
Looking at dominator: nexthop
  looking at non excl field ig_intr_md.ingress_mac_tstamp
   leaf usage switch_config_params and R
      nexthop cannot reach switch_config_params
  Candidate dominator: nexthop because not above last usage of previous field
Looking at dominator: ecmp_group
  looking at non excl field ig_intr_md.ingress_mac_tstamp
   leaf usage switch_config_params and R
      ecmp_group cannot reach switch_config_params
  Candidate dominator: ecmp_group because not above last usage of previous field
Looking at dominators_reach_previous_usage: nexthop
Looking at dominators_reach_previous_usage: ecmp_group
------
Initial usages for tunnel_metadata.tunnel_dst_index.
  nexthop: W
  ecmp_group: W
Dominators for tunnel_metadata.tunnel_dst_index: 2
  nexthop
  ecmp_group
Trimmed dominators for tunnel_metadata.tunnel_dst_index (those that are not dominated by other dominators): 2
  nexthop can reach [] and who can reach me []
  ecmp_group can reach [] and who can reach me []
fi = tunnel_metadata.tunnel_dst_index (ingress), fi_earliest_use = 8
  squashed group dominator nexthop in stage 8
    Possible nodes to do clearing: (18)
      nexthop (stage 8)
      _condition_39 (stage 8)
      _condition_36 (stage 7)
      storm_control_stats (stage 7)
      acl_stats (stage 7)
      ingress_bd_stats (stage 7)
      compute_other_hashes (stage 7)
      _condition_34 (stage 6)
      _condition_12 (stage 3)
      _condition_11 (stage 2)
      _condition_10 (stage 2)
      _condition_7 (stage 1)
      _condition_6 (stage 1)
      _condition_5 (stage 1)
      port_vlan_to_ifindex_mapping (stage 0)
      _condition_4 (stage 0)
      validate_outer_ethernet (stage 0)
      switch_config_params (stage 0)
previous stages: 16
   _condition_36  (stage 7)
   storm_control_stats  (stage 7)
   acl_stats  (stage 7)
   ingress_bd_stats  (stage 7)
   compute_other_hashes  (stage 7)
   _condition_34  (stage 6)
   _condition_12  (stage 3)
   _condition_11  (stage 2)
   _condition_10  (stage 2)
   _condition_7  (stage 1)
   _condition_6  (stage 1)
   _condition_5  (stage 1)
   port_vlan_to_ifindex_mapping  (stage 0)
   _condition_4  (stage 0)
   validate_outer_ethernet  (stage 0)
   switch_config_params  (stage 0)
current_stage stages: 2
   nexthop  (stage 8)
   _condition_39  (stage 8)
  squashed group dominator ecmp_group in stage 8
    Possible nodes to do clearing: (18)
      ecmp_group (stage 8)
      _condition_39 (stage 8)
      _condition_36 (stage 7)
      storm_control_stats (stage 7)
      acl_stats (stage 7)
      ingress_bd_stats (stage 7)
      compute_other_hashes (stage 7)
      _condition_34 (stage 6)
      _condition_12 (stage 3)
      _condition_11 (stage 2)
      _condition_10 (stage 2)
      _condition_7 (stage 1)
      _condition_6 (stage 1)
      _condition_5 (stage 1)
      port_vlan_to_ifindex_mapping (stage 0)
      _condition_4 (stage 0)
      validate_outer_ethernet (stage 0)
      switch_config_params (stage 0)
previous stages: 16
   _condition_36  (stage 7)
   storm_control_stats  (stage 7)
   acl_stats  (stage 7)
   ingress_bd_stats  (stage 7)
   compute_other_hashes  (stage 7)
   _condition_34  (stage 6)
   _condition_12  (stage 3)
   _condition_11  (stage 2)
   _condition_10  (stage 2)
   _condition_7  (stage 1)
   _condition_6  (stage 1)
   _condition_5  (stage 1)
   port_vlan_to_ifindex_mapping  (stage 0)
   _condition_4  (stage 0)
   validate_outer_ethernet  (stage 0)
   switch_config_params  (stage 0)
current_stage stages: 2
   ecmp_group  (stage 8)
   _condition_39  (stage 8)
Call to _delay_init(_condition_36)
  _condition_36 is condition
    looking at _condition_36's out edge _condition_36 -- False (f=0.50) -- _condition_39
Call to _delay_init(_condition_39)
  _condition_39 is condition
    looking at _condition_39's out edge _condition_39 -- False (f=0.50) -- nexthop
Call to _delay_init(nexthop)
  my usage is W, so init here
      tbl _condition_39 can descend on edge edge _condition_39 -- False (f=0.50) -- nexthop
        nexthop ---all---
    looking at _condition_39's out edge _condition_39 -- True (f=0.50) -- ecmp_group
Call to _delay_init(ecmp_group)
  my usage is W, so init here
      tbl _condition_39 can descend on edge edge _condition_39 -- True (f=0.50) -- ecmp_group
        ecmp_group ---all---
      tbl _condition_36 can descend on edge edge _condition_36 -- False (f=0.50) -- _condition_39
        nexthop ---all---
        ecmp_group ---all---
    looking at _condition_36's out edge _condition_36 -- True (f=0.50) -- fwd_result
Call to _delay_init(fwd_result)
  fwd_result is match
    looking at fwd_result's out edge fwd_result -- nop (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_l2_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_fib_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_cpu_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_acl_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_racl_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_rmac_non_ip_drop (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_route (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_rpf_fail_bridge (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_rpf_fail_flood_to_mrouters (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_bridge (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_miss_flood (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_miss_flood_to_mrouters (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_drop (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- --default-- (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
      tbl _condition_36 can descend on edge edge _condition_36 -- True (f=0.50) -- fwd_result
Group dominators for tunnel_metadata.tunnel_dst_index (common parents): 2
  nexthop
  ecmp_group
Squashed group dominators for tunnel_metadata.tunnel_dst_index (common parents): 2
  nexthop
  ecmp_group
Ascend group dominators for tunnel_metadata.tunnel_dst_index: 1
  _condition_36
Squashed ascend group dominators for tunnel_metadata.tunnel_dst_index: 1
  _condition_36

Delayed init points (experimental): 2
  nexthop on edge ---all---
  ecmp_group on edge ---all---
New squashed group dominators for tunnel_metadata.tunnel_dst_index (experimental): 2
  nexthop
  ecmp_group
Table to Inbound edges to initialize field tunnel_metadata.tunnel_dst_index: 1
  _condition_36
    storm_control_stats -- nop (f=0.50) -- _condition_36
    storm_control_stats -- --default-- (f=0.50) -- _condition_36
--------
Init at table for field tunnel_metadata.tunnel_dst_index: 1
  _condition_36
Init at table at specific edge for field tunnel_metadata.tunnel_dst_index: 0

Init at table: 1
  _condition_36
  Since dominator table _condition_36 is a gateway, need to clear tunnel_metadata.tunnel_dst_index in table that precedes or inject a new table to clear the field to be 0.
    Possible nodes to do clearing: (15)
      storm_control_stats
      acl_stats
      ingress_bd_stats
      compute_other_hashes
      _condition_34
      _condition_12
      _condition_11
      _condition_10
      _condition_7
      _condition_6
      _condition_5
      port_vlan_to_ifindex_mapping
      _condition_4
      validate_outer_ethernet
      switch_config_params
    Clear field tunnel_metadata.tunnel_dst_index at table storm_control_stats.
>>Adding instruction: clear tbl=storm_control_stats action=nop, phv1[31:16] for field tunnel_metadata.tunnel_dst_index[15:0]

--------------------------------------------
Liveness ranges for container phv39
  phv39[31:0] = ipv6_metadata.lkp_ipv6_sa[95:64] -- -1 to 6
  phv39[31:0] = ipv4_metadata.lkp_ipv4_sa[31:0] -- -1 to 6

--------------------------------------------
Liveness ranges for container phv45
  phv45[31:0] = ipv6_metadata.lkp_ipv6_da[127:96] -- -1 to 6
  phv45[31:0] = ipv4_metadata.lkp_ipv4_da[31:0] -- -1 to 6

--------------------------------------------
Liveness ranges for container phv72
  phv72[7:0] = acl_metadata.ingress_src_port_range_id[7:0] -- 3 to 5
  phv72[7:3] = ig_intr_md_for_tm.qid[4:0] -- 9 to 12
Initialization may be required for:
  ig_intr_md_for_tm.qid[4:0] -- 9 to 12

Checking if ig_intr_md_for_tm.qid needs initialization.  Non exclusive with fields ['acl_metadata.ingress_src_port_range_id'].
Looking at dominator: __sink__
  looking at non excl field acl_metadata.ingress_src_port_range_id
   leaf usage ipv6_acl and R
      __sink__ cannot reach ipv6_acl
   leaf usage ip_acl and R
      __sink__ cannot reach ip_acl
   leaf usage ipv6_racl and R
      __sink__ cannot reach ipv6_racl
   leaf usage ipv4_racl and R
      __sink__ cannot reach ipv4_racl
   leaf usage ingress_l4_src_port and W
      __sink__ cannot reach ingress_l4_src_port
  Candidate dominator: __sink__ because not above last usage of previous field
Looking at dominator: _condition_44
  looking at non excl field acl_metadata.ingress_src_port_range_id
   leaf usage ipv6_acl and R
      _condition_44 cannot reach ipv6_acl
   leaf usage ip_acl and R
      _condition_44 cannot reach ip_acl
   leaf usage ipv6_racl and R
      _condition_44 cannot reach ipv6_racl
   leaf usage ipv4_racl and R
      _condition_44 cannot reach ipv4_racl
   leaf usage ingress_l4_src_port and W
      _condition_44 cannot reach ingress_l4_src_port
  Candidate dominator: _condition_44 because not above last usage of previous field
Looking at dominator: system_acl
  looking at non excl field acl_metadata.ingress_src_port_range_id
   leaf usage ipv6_acl and R
      system_acl cannot reach ipv6_acl
   leaf usage ip_acl and R
      system_acl cannot reach ip_acl
   leaf usage ipv6_racl and R
      system_acl cannot reach ipv6_racl
   leaf usage ipv4_racl and R
      system_acl cannot reach ipv4_racl
   leaf usage ingress_l4_src_port and W
      system_acl cannot reach ingress_l4_src_port
  Candidate dominator: system_acl because not above last usage of previous field
Looking at dominators_reach_previous_usage: __sink__
Looking at dominators_reach_previous_usage: _condition_44
Looking at dominators_reach_previous_usage: system_acl
------
Initial usages for ig_intr_md_for_tm.qid.
  Deparser: R
  system_acl: W
Dominators for ig_intr_md_for_tm.qid: 3
  __sink__
  _condition_44
  system_acl
Trimmed dominators for ig_intr_md_for_tm.qid (those that are not dominated by other dominators): 1
  _condition_44 can reach [] and who can reach me []
fi = ig_intr_md_for_tm.qid (ingress), fi_earliest_use = 9
  squashed group dominator _condition_44 in stage 9
    Possible nodes to do clearing: (11)
      _condition_44 (stage 9)
      racl_stats (stage 9)
      _condition_43 (stage 9)
      _condition_41 (stage 9)
      _condition_39 (stage 8)
      _condition_36 (stage 7)
      storm_control_stats (stage 7)
      acl_stats (stage 7)
      ingress_bd_stats (stage 7)
      compute_other_hashes (stage 7)
      _condition_34 (stage 6)
previous stages: 7
   _condition_39  (stage 8)
   _condition_36  (stage 7)
   storm_control_stats  (stage 7)
   acl_stats  (stage 7)
   ingress_bd_stats  (stage 7)
   compute_other_hashes  (stage 7)
   _condition_34  (stage 6)
current_stage stages: 4
   _condition_44  (stage 9)
   racl_stats  (stage 9)
   _condition_43  (stage 9)
   _condition_41  (stage 9)
Call to _delay_init(_condition_39)
  _condition_39 is condition
    looking at _condition_39's out edge _condition_39 -- False (f=0.50) -- nexthop
Call to _delay_init(nexthop)
  nexthop is match
    looking at nexthop's out edge nexthop -- nop (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- nop (f=0.07) -- _condition_41
    looking at nexthop's out edge nexthop -- set_nexthop_details (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- set_nexthop_details (f=0.07) -- _condition_41
    looking at nexthop's out edge nexthop -- set_nexthop_details_with_tunnel (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- set_nexthop_details_with_tunnel (f=0.07) -- _condition_41
    looking at nexthop's out edge nexthop -- set_nexthop_details_for_post_routed_flood (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- set_nexthop_details_for_post_routed_flood (f=0.07) -- _condition_41
    looking at nexthop's out edge nexthop -- set_nexthop_details_for_glean (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- set_nexthop_details_for_glean (f=0.07) -- _condition_41
    looking at nexthop's out edge nexthop -- set_nexthop_details_for_drop (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- set_nexthop_details_for_drop (f=0.07) -- _condition_41
    looking at nexthop's out edge nexthop -- --default-- (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- --default-- (f=0.07) -- _condition_41
      tbl _condition_39 can descend on edge edge _condition_39 -- False (f=0.50) -- nexthop
        _condition_41 nexthop -- nop (f=0.07) -- _condition_41
        _condition_41 nexthop -- set_nexthop_details (f=0.07) -- _condition_41
        _condition_41 nexthop -- set_nexthop_details_with_tunnel (f=0.07) -- _condition_41
        _condition_41 nexthop -- set_nexthop_details_for_post_routed_flood (f=0.07) -- _condition_41
        _condition_41 nexthop -- set_nexthop_details_for_glean (f=0.07) -- _condition_41
        _condition_41 nexthop -- set_nexthop_details_for_drop (f=0.07) -- _condition_41
        _condition_41 nexthop -- --default-- (f=0.07) -- _condition_41
    looking at _condition_39's out edge _condition_39 -- True (f=0.50) -- ecmp_group
Call to _delay_init(ecmp_group)
  ecmp_group is match
    looking at ecmp_group's out edge ecmp_group -- nop (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 ecmp_group -- nop (f=0.10) -- _condition_41
    looking at ecmp_group's out edge ecmp_group -- set_ecmp_nexthop_details (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 ecmp_group -- set_ecmp_nexthop_details (f=0.10) -- _condition_41
    looking at ecmp_group's out edge ecmp_group -- set_ecmp_nexthop_details_with_tunnel (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 ecmp_group -- set_ecmp_nexthop_details_with_tunnel (f=0.10) -- _condition_41
    looking at ecmp_group's out edge ecmp_group -- set_ecmp_nexthop_details_for_post_routed_flood (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 ecmp_group -- set_ecmp_nexthop_details_for_post_routed_flood (f=0.10) -- _condition_41
    looking at ecmp_group's out edge ecmp_group -- --default-- (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 ecmp_group -- --default-- (f=0.10) -- _condition_41
      tbl _condition_39 can descend on edge edge _condition_39 -- True (f=0.50) -- ecmp_group
        _condition_41 ecmp_group -- nop (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- set_ecmp_nexthop_details (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- set_ecmp_nexthop_details_with_tunnel (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- set_ecmp_nexthop_details_for_post_routed_flood (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- --default-- (f=0.10) -- _condition_41
Group dominators for ig_intr_md_for_tm.qid (common parents): 1
  _condition_44
Squashed group dominators for ig_intr_md_for_tm.qid (common parents): 1
  _condition_44
Ascend group dominators for ig_intr_md_for_tm.qid: 1
  _condition_39
Squashed ascend group dominators for ig_intr_md_for_tm.qid: 1
  _condition_39

Delayed init points (experimental): 12
  _condition_41 on edge nexthop -- nop (f=0.07) -- _condition_41
  _condition_41 on edge nexthop -- set_nexthop_details (f=0.07) -- _condition_41
  _condition_41 on edge nexthop -- set_nexthop_details_with_tunnel (f=0.07) -- _condition_41
  _condition_41 on edge nexthop -- set_nexthop_details_for_post_routed_flood (f=0.07) -- _condition_41
  _condition_41 on edge nexthop -- set_nexthop_details_for_glean (f=0.07) -- _condition_41
  _condition_41 on edge nexthop -- set_nexthop_details_for_drop (f=0.07) -- _condition_41
  _condition_41 on edge nexthop -- --default-- (f=0.07) -- _condition_41
  _condition_41 on edge ecmp_group -- nop (f=0.10) -- _condition_41
  _condition_41 on edge ecmp_group -- set_ecmp_nexthop_details (f=0.10) -- _condition_41
  _condition_41 on edge ecmp_group -- set_ecmp_nexthop_details_with_tunnel (f=0.10) -- _condition_41
  _condition_41 on edge ecmp_group -- set_ecmp_nexthop_details_for_post_routed_flood (f=0.10) -- _condition_41
  _condition_41 on edge ecmp_group -- --default-- (f=0.10) -- _condition_41
New squashed group dominators for ig_intr_md_for_tm.qid (experimental): 1
  _condition_41
Table to Inbound edges to initialize field ig_intr_md_for_tm.qid: 1
  _condition_39
    _condition_36 -- False (f=0.50) -- _condition_39
    fwd_result -- nop (f=0.03) -- _condition_39
    fwd_result -- set_l2_redirect (f=0.03) -- _condition_39
    fwd_result -- set_fib_redirect (f=0.03) -- _condition_39
    fwd_result -- set_cpu_redirect (f=0.03) -- _condition_39
    fwd_result -- set_acl_redirect (f=0.03) -- _condition_39
    fwd_result -- set_racl_redirect (f=0.03) -- _condition_39
    fwd_result -- set_rmac_non_ip_drop (f=0.03) -- _condition_39
    fwd_result -- set_multicast_route (f=0.03) -- _condition_39
    fwd_result -- set_multicast_rpf_fail_bridge (f=0.03) -- _condition_39
    fwd_result -- set_multicast_rpf_fail_flood_to_mrouters (f=0.03) -- _condition_39
    fwd_result -- set_multicast_bridge (f=0.03) -- _condition_39
    fwd_result -- set_multicast_miss_flood (f=0.03) -- _condition_39
    fwd_result -- set_multicast_miss_flood_to_mrouters (f=0.03) -- _condition_39
    fwd_result -- set_multicast_drop (f=0.03) -- _condition_39
    fwd_result -- --default-- (f=0.03) -- _condition_39
--------
Init at table for field ig_intr_md_for_tm.qid: 1
  _condition_39
Init at table at specific edge for field ig_intr_md_for_tm.qid: 0

Init at table: 1
  _condition_39
  Since dominator table _condition_39 is a gateway, need to clear ig_intr_md_for_tm.qid in table that precedes or inject a new table to clear the field to be 0.
    Possible nodes to do clearing: (6)
      _condition_36
      storm_control_stats
      acl_stats
      ingress_bd_stats
      compute_other_hashes
      _condition_34
    Clear field ig_intr_md_for_tm.qid at table storm_control_stats.
>>Adding instruction: clear tbl=storm_control_stats action=nop, phv72[7:3] for field ig_intr_md_for_tm.qid[4:0]

--------------------------------------------
Liveness ranges for container phv73
  phv73[7:0] = acl_metadata.ingress_dst_port_range_id[7:0] -- 3 to 5
  phv73[7:5] = ig_intr_md_for_tm.ingress_cos[2:0] -- 9 to 12
Initialization may be required for:
  ig_intr_md_for_tm.ingress_cos[2:0] -- 9 to 12

Checking if ig_intr_md_for_tm.ingress_cos needs initialization.  Non exclusive with fields ['acl_metadata.ingress_dst_port_range_id'].
Looking at dominator: __sink__
  looking at non excl field acl_metadata.ingress_dst_port_range_id
   leaf usage ipv6_acl and R
      __sink__ cannot reach ipv6_acl
   leaf usage ip_acl and R
      __sink__ cannot reach ip_acl
   leaf usage ipv6_racl and R
      __sink__ cannot reach ipv6_racl
   leaf usage ipv4_racl and R
      __sink__ cannot reach ipv4_racl
   leaf usage ingress_l4_dst_port and W
      __sink__ cannot reach ingress_l4_dst_port
  Candidate dominator: __sink__ because not above last usage of previous field
Looking at dominator: _condition_44
  looking at non excl field acl_metadata.ingress_dst_port_range_id
   leaf usage ipv6_acl and R
      _condition_44 cannot reach ipv6_acl
   leaf usage ip_acl and R
      _condition_44 cannot reach ip_acl
   leaf usage ipv6_racl and R
      _condition_44 cannot reach ipv6_racl
   leaf usage ipv4_racl and R
      _condition_44 cannot reach ipv4_racl
   leaf usage ingress_l4_dst_port and W
      _condition_44 cannot reach ingress_l4_dst_port
  Candidate dominator: _condition_44 because not above last usage of previous field
Looking at dominator: system_acl
  looking at non excl field acl_metadata.ingress_dst_port_range_id
   leaf usage ipv6_acl and R
      system_acl cannot reach ipv6_acl
   leaf usage ip_acl and R
      system_acl cannot reach ip_acl
   leaf usage ipv6_racl and R
      system_acl cannot reach ipv6_racl
   leaf usage ipv4_racl and R
      system_acl cannot reach ipv4_racl
   leaf usage ingress_l4_dst_port and W
      system_acl cannot reach ingress_l4_dst_port
  Candidate dominator: system_acl because not above last usage of previous field
Looking at dominators_reach_previous_usage: __sink__
Looking at dominators_reach_previous_usage: _condition_44
Looking at dominators_reach_previous_usage: system_acl
------
Initial usages for ig_intr_md_for_tm.ingress_cos.
  Deparser: R
  system_acl: W
Dominators for ig_intr_md_for_tm.ingress_cos: 3
  __sink__
  _condition_44
  system_acl
Trimmed dominators for ig_intr_md_for_tm.ingress_cos (those that are not dominated by other dominators): 1
  _condition_44 can reach [] and who can reach me []
fi = ig_intr_md_for_tm.ingress_cos (ingress), fi_earliest_use = 9
  squashed group dominator _condition_44 in stage 9
    Possible nodes to do clearing: (11)
      _condition_44 (stage 9)
      racl_stats (stage 9)
      _condition_43 (stage 9)
      _condition_41 (stage 9)
      _condition_39 (stage 8)
      _condition_36 (stage 7)
      storm_control_stats (stage 7)
      acl_stats (stage 7)
      ingress_bd_stats (stage 7)
      compute_other_hashes (stage 7)
      _condition_34 (stage 6)
previous stages: 7
   _condition_39  (stage 8)
   _condition_36  (stage 7)
   storm_control_stats  (stage 7)
   acl_stats  (stage 7)
   ingress_bd_stats  (stage 7)
   compute_other_hashes  (stage 7)
   _condition_34  (stage 6)
current_stage stages: 4
   _condition_44  (stage 9)
   racl_stats  (stage 9)
   _condition_43  (stage 9)
   _condition_41  (stage 9)
Call to _delay_init(_condition_39)
  _condition_39 is condition
    looking at _condition_39's out edge _condition_39 -- False (f=0.50) -- nexthop
Call to _delay_init(nexthop)
  nexthop is match
    looking at nexthop's out edge nexthop -- nop (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- nop (f=0.07) -- _condition_41
    looking at nexthop's out edge nexthop -- set_nexthop_details (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- set_nexthop_details (f=0.07) -- _condition_41
    looking at nexthop's out edge nexthop -- set_nexthop_details_with_tunnel (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- set_nexthop_details_with_tunnel (f=0.07) -- _condition_41
    looking at nexthop's out edge nexthop -- set_nexthop_details_for_post_routed_flood (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- set_nexthop_details_for_post_routed_flood (f=0.07) -- _condition_41
    looking at nexthop's out edge nexthop -- set_nexthop_details_for_glean (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- set_nexthop_details_for_glean (f=0.07) -- _condition_41
    looking at nexthop's out edge nexthop -- set_nexthop_details_for_drop (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- set_nexthop_details_for_drop (f=0.07) -- _condition_41
    looking at nexthop's out edge nexthop -- --default-- (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- --default-- (f=0.07) -- _condition_41
      tbl _condition_39 can descend on edge edge _condition_39 -- False (f=0.50) -- nexthop
        _condition_41 nexthop -- nop (f=0.07) -- _condition_41
        _condition_41 nexthop -- set_nexthop_details (f=0.07) -- _condition_41
        _condition_41 nexthop -- set_nexthop_details_with_tunnel (f=0.07) -- _condition_41
        _condition_41 nexthop -- set_nexthop_details_for_post_routed_flood (f=0.07) -- _condition_41
        _condition_41 nexthop -- set_nexthop_details_for_glean (f=0.07) -- _condition_41
        _condition_41 nexthop -- set_nexthop_details_for_drop (f=0.07) -- _condition_41
        _condition_41 nexthop -- --default-- (f=0.07) -- _condition_41
    looking at _condition_39's out edge _condition_39 -- True (f=0.50) -- ecmp_group
Call to _delay_init(ecmp_group)
  ecmp_group is match
    looking at ecmp_group's out edge ecmp_group -- nop (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 ecmp_group -- nop (f=0.10) -- _condition_41
    looking at ecmp_group's out edge ecmp_group -- set_ecmp_nexthop_details (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 ecmp_group -- set_ecmp_nexthop_details (f=0.10) -- _condition_41
    looking at ecmp_group's out edge ecmp_group -- set_ecmp_nexthop_details_with_tunnel (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 ecmp_group -- set_ecmp_nexthop_details_with_tunnel (f=0.10) -- _condition_41
    looking at ecmp_group's out edge ecmp_group -- set_ecmp_nexthop_details_for_post_routed_flood (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 ecmp_group -- set_ecmp_nexthop_details_for_post_routed_flood (f=0.10) -- _condition_41
    looking at ecmp_group's out edge ecmp_group -- --default-- (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 ecmp_group -- --default-- (f=0.10) -- _condition_41
      tbl _condition_39 can descend on edge edge _condition_39 -- True (f=0.50) -- ecmp_group
        _condition_41 ecmp_group -- nop (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- set_ecmp_nexthop_details (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- set_ecmp_nexthop_details_with_tunnel (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- set_ecmp_nexthop_details_for_post_routed_flood (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- --default-- (f=0.10) -- _condition_41
Group dominators for ig_intr_md_for_tm.ingress_cos (common parents): 1
  _condition_44
Squashed group dominators for ig_intr_md_for_tm.ingress_cos (common parents): 1
  _condition_44
Ascend group dominators for ig_intr_md_for_tm.ingress_cos: 1
  _condition_39
Squashed ascend group dominators for ig_intr_md_for_tm.ingress_cos: 1
  _condition_39

Delayed init points (experimental): 12
  _condition_41 on edge nexthop -- nop (f=0.07) -- _condition_41
  _condition_41 on edge nexthop -- set_nexthop_details (f=0.07) -- _condition_41
  _condition_41 on edge nexthop -- set_nexthop_details_with_tunnel (f=0.07) -- _condition_41
  _condition_41 on edge nexthop -- set_nexthop_details_for_post_routed_flood (f=0.07) -- _condition_41
  _condition_41 on edge nexthop -- set_nexthop_details_for_glean (f=0.07) -- _condition_41
  _condition_41 on edge nexthop -- set_nexthop_details_for_drop (f=0.07) -- _condition_41
  _condition_41 on edge nexthop -- --default-- (f=0.07) -- _condition_41
  _condition_41 on edge ecmp_group -- nop (f=0.10) -- _condition_41
  _condition_41 on edge ecmp_group -- set_ecmp_nexthop_details (f=0.10) -- _condition_41
  _condition_41 on edge ecmp_group -- set_ecmp_nexthop_details_with_tunnel (f=0.10) -- _condition_41
  _condition_41 on edge ecmp_group -- set_ecmp_nexthop_details_for_post_routed_flood (f=0.10) -- _condition_41
  _condition_41 on edge ecmp_group -- --default-- (f=0.10) -- _condition_41
New squashed group dominators for ig_intr_md_for_tm.ingress_cos (experimental): 1
  _condition_41
Table to Inbound edges to initialize field ig_intr_md_for_tm.ingress_cos: 1
  _condition_39
    _condition_36 -- False (f=0.50) -- _condition_39
    fwd_result -- nop (f=0.03) -- _condition_39
    fwd_result -- set_l2_redirect (f=0.03) -- _condition_39
    fwd_result -- set_fib_redirect (f=0.03) -- _condition_39
    fwd_result -- set_cpu_redirect (f=0.03) -- _condition_39
    fwd_result -- set_acl_redirect (f=0.03) -- _condition_39
    fwd_result -- set_racl_redirect (f=0.03) -- _condition_39
    fwd_result -- set_rmac_non_ip_drop (f=0.03) -- _condition_39
    fwd_result -- set_multicast_route (f=0.03) -- _condition_39
    fwd_result -- set_multicast_rpf_fail_bridge (f=0.03) -- _condition_39
    fwd_result -- set_multicast_rpf_fail_flood_to_mrouters (f=0.03) -- _condition_39
    fwd_result -- set_multicast_bridge (f=0.03) -- _condition_39
    fwd_result -- set_multicast_miss_flood (f=0.03) -- _condition_39
    fwd_result -- set_multicast_miss_flood_to_mrouters (f=0.03) -- _condition_39
    fwd_result -- set_multicast_drop (f=0.03) -- _condition_39
    fwd_result -- --default-- (f=0.03) -- _condition_39
--------
Init at table for field ig_intr_md_for_tm.ingress_cos: 1
  _condition_39
Init at table at specific edge for field ig_intr_md_for_tm.ingress_cos: 0

Init at table: 1
  _condition_39
  Since dominator table _condition_39 is a gateway, need to clear ig_intr_md_for_tm.ingress_cos in table that precedes or inject a new table to clear the field to be 0.
    Possible nodes to do clearing: (6)
      _condition_36
      storm_control_stats
      acl_stats
      ingress_bd_stats
      compute_other_hashes
      _condition_34
    Clear field ig_intr_md_for_tm.ingress_cos at table storm_control_stats.
>>Adding instruction: clear tbl=storm_control_stats action=nop, phv73[7:5] for field ig_intr_md_for_tm.ingress_cos[2:0]

--------------------------------------------
Liveness ranges for container phv82
  phv82[7:5] = -pad-3-[2:0] -- None to None
  phv82[4:0] = tunnel_metadata.ingress_tunnel_type[4:0] -- -1 to 3
  phv82[7:5] = eg_intr_md_for_oport.drop_ctl[2:0] -- 8 to 12
Initialization may be required for:
  eg_intr_md_for_oport.drop_ctl[2:0] -- 8 to 12

Checking if eg_intr_md_for_oport.drop_ctl needs initialization.  Non exclusive with fields [].
Looking at dominator: __sink__
  Candidate dominator: __sink__ because not above last usage of previous field
Looking at dominator: _condition_67
  Candidate dominator: _condition_67 because not above last usage of previous field
Looking at dominator: egress_system_acl
  Candidate dominator: egress_system_acl because not above last usage of previous field
------
Initial usages for eg_intr_md_for_oport.drop_ctl.
  Deparser: R
  egress_system_acl: W
Dominators for eg_intr_md_for_oport.drop_ctl: 3
  __sink__
  _condition_67
  egress_system_acl
Trimmed dominators for eg_intr_md_for_oport.drop_ctl (those that are not dominated by other dominators): 0
fi = eg_intr_md_for_oport.drop_ctl (egress), fi_earliest_use = 8
Group dominators for eg_intr_md_for_oport.drop_ctl (common parents): 0
Squashed group dominators for eg_intr_md_for_oport.drop_ctl (common parents): 0
Ascend group dominators for eg_intr_md_for_oport.drop_ctl: 0
Squashed ascend group dominators for eg_intr_md_for_oport.drop_ctl: 0

Delayed init points (experimental): 0
New squashed group dominators for eg_intr_md_for_oport.drop_ctl (experimental): 0
Table to Inbound edges to initialize field eg_intr_md_for_oport.drop_ctl: 0
--------
Init at table for field eg_intr_md_for_oport.drop_ctl: 0
Init at table at specific edge for field eg_intr_md_for_oport.drop_ctl: 0

Init at table: 0

--------------------------------------------
Liveness ranges for container phv95
  phv95[7:0] = eg_intr_md.egress_rid[15:8] -- -1 to 1
  phv95[7:0] = tunnel_metadata.inner_ip_proto[7:0] -- 5 to 6
Initialization may be required for:
  tunnel_metadata.inner_ip_proto[7:0] -- 5 to 6

Checking if tunnel_metadata.inner_ip_proto needs initialization.  Non exclusive with fields ['eg_intr_md.egress_rid'].
Looking at dominator: tunnel_encap_process_inner
  looking at non excl field eg_intr_md.egress_rid
   leaf usage None and W
   leaf usage _condition_54 and R
      tunnel_encap_process_inner cannot reach _condition_54
   leaf usage replica_type and R
      tunnel_encap_process_inner cannot reach replica_type
  Candidate dominator: tunnel_encap_process_inner because not above last usage of previous field
Looking at dominators_reach_previous_usage: tunnel_encap_process_inner
------
Initial usages for tunnel_metadata.inner_ip_proto.
  tunnel_encap_process_inner: W
Dominators for tunnel_metadata.inner_ip_proto: 1
  tunnel_encap_process_inner
Trimmed dominators for tunnel_metadata.inner_ip_proto (those that are not dominated by other dominators): 1
  tunnel_encap_process_inner can reach [] and who can reach me []
fi = tunnel_metadata.inner_ip_proto (egress), fi_earliest_use = 5
  squashed group dominator tunnel_encap_process_inner in stage 5
    Possible nodes to do clearing: (5)
      tunnel_encap_process_inner (stage 5)
      _condition_65 (stage 5)
      _condition_64 (stage 5)
      _condition_61 (stage 4)
      egress_port_mapping (stage 2)
previous stages: 2
   _condition_61  (stage 4)
   egress_port_mapping  (stage 2)
current_stage stages: 3
   tunnel_encap_process_inner  (stage 5)
   _condition_65  (stage 5)
   _condition_64  (stage 5)
Call to _delay_init(_condition_61)
  _condition_61 is condition
    looking at _condition_61's out edge _condition_61 -- False (f=0.12) -- _condition_64
Call to _delay_init(_condition_64)
  _condition_64 is condition
    looking at _condition_64's out edge _condition_64 -- False (f=0.12) -- mtu
Call to _delay_init(mtu)
  can reach other usages of field tunnel_metadata.inner_ip_proto
      di_tbl _condition_64 cannot descend on edge edge _condition_64 -- False (f=0.12) -- mtu
      di_tbl _condition_61 cannot descend on edge edge _condition_61 -- False (f=0.12) -- _condition_64
Group dominators for tunnel_metadata.inner_ip_proto (common parents): 1
  tunnel_encap_process_inner
Squashed group dominators for tunnel_metadata.inner_ip_proto (common parents): 1
  tunnel_encap_process_inner
Ascend group dominators for tunnel_metadata.inner_ip_proto: 1
  _condition_61
Squashed ascend group dominators for tunnel_metadata.inner_ip_proto: 1
  _condition_61

Delayed init points (experimental): 1
  _condition_61 on edge None
New squashed group dominators for tunnel_metadata.inner_ip_proto (experimental): 1
  _condition_61
Table to Inbound edges to initialize field tunnel_metadata.inner_ip_proto: 1
  _condition_61
    egress_port_mapping -- egress_port_type_cpu (f=0.08) -- _condition_61
    _condition_59 -- False (f=0.04) -- _condition_61
    rewrite -- nop (f=0.00) -- _condition_61
    rewrite -- set_l2_rewrite (f=0.00) -- _condition_61
    rewrite -- set_l2_rewrite_with_tunnel (f=0.00) -- _condition_61
    rewrite -- set_l3_rewrite_with_tunnel (f=0.00) -- _condition_61
    rewrite -- set_l3_rewrite_with_tunnel_vnid (f=0.00) -- _condition_61
    rewrite -- set_l3_rewrite_with_tunnel_and_ingress_vrf (f=0.00) -- _condition_61
    rewrite -- set_l3_rewrite (f=0.00) -- _condition_61
    rewrite -- set_mpls_push_rewrite_l2 (f=0.00) -- _condition_61
    rewrite -- set_mpls_swap_push_rewrite_l3 (f=0.00) -- _condition_61
    rewrite -- set_mpls_push_rewrite_l3 (f=0.00) -- _condition_61
    rewrite -- --default-- (f=0.00) -- _condition_61
    egress_port_mapping -- --default-- (f=0.08) -- _condition_61
--------
Init at table for field tunnel_metadata.inner_ip_proto: 1
  _condition_61
Init at table at specific edge for field tunnel_metadata.inner_ip_proto: 0

Init at table: 1
  _condition_61
  Since dominator table _condition_61 is a gateway, need to clear tunnel_metadata.inner_ip_proto in table that precedes or inject a new table to clear the field to be 0.
    Possible nodes to do clearing: (1)
      egress_port_mapping
    Clear field tunnel_metadata.inner_ip_proto at table egress_port_mapping.
>>Adding instruction: clear tbl=egress_port_mapping action=egress_port_type_cpu, phv95[7:0] for field tunnel_metadata.inner_ip_proto[7:0]
>>Adding instruction: clear tbl=egress_port_mapping action=egress_port_type_normal, phv95[7:0] for field tunnel_metadata.inner_ip_proto[7:0]

--------------------------------------------
Liveness ranges for container phv102
  phv102[7:0] = egress_metadata.mac_da[47:40] -- 3 to 4
  phv102[7:0] = tunnel_metadata.tunnel_smac_index[7:0] -- 6 to 7
Initialization may be required for:
  tunnel_metadata.tunnel_smac_index[7:0] -- 6 to 7

Checking if tunnel_metadata.tunnel_smac_index needs initialization.  Non exclusive with fields ['egress_metadata.mac_da'].
Looking at dominator: egress_outer_bd_map
  looking at non excl field egress_metadata.mac_da
   leaf usage rewrite and W
      egress_outer_bd_map cannot reach rewrite
   leaf usage l3_rewrite and R
      egress_outer_bd_map cannot reach l3_rewrite
  Candidate dominator: egress_outer_bd_map because not above last usage of previous field
Looking at dominators_reach_previous_usage: egress_outer_bd_map
------
Initial usages for tunnel_metadata.tunnel_smac_index.
  egress_outer_bd_map: W
Dominators for tunnel_metadata.tunnel_smac_index: 1
  egress_outer_bd_map
Trimmed dominators for tunnel_metadata.tunnel_smac_index (those that are not dominated by other dominators): 1
  egress_outer_bd_map can reach [] and who can reach me []
fi = tunnel_metadata.tunnel_smac_index (egress), fi_earliest_use = 6
  squashed group dominator egress_outer_bd_map in stage 6
    Possible nodes to do clearing: (5)
      egress_outer_bd_map (stage 6)
      tunnel_encap_process_outer (stage 6)
      tunnel_encap_process_inner (stage 5)
      _condition_65 (stage 5)
      _condition_64 (stage 5)
previous stages: 3
   tunnel_encap_process_inner  (stage 5)
   _condition_65  (stage 5)
   _condition_64  (stage 5)
current_stage stages: 2
   egress_outer_bd_map  (stage 6)
   tunnel_encap_process_outer  (stage 6)
Call to _delay_init(tunnel_encap_process_inner)
  tunnel_encap_process_inner is match
    looking at tunnel_encap_process_inner's out edge tunnel_encap_process_inner -- inner_ipv4_udp_rewrite (f=0.01) -- tunnel_encap_process_outer
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at tunnel_encap_process_inner's out edge tunnel_encap_process_inner -- inner_ipv4_tcp_rewrite (f=0.01) -- tunnel_encap_process_outer
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at tunnel_encap_process_inner's out edge tunnel_encap_process_inner -- inner_ipv4_icmp_rewrite (f=0.01) -- tunnel_encap_process_outer
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at tunnel_encap_process_inner's out edge tunnel_encap_process_inner -- inner_ipv4_unknown_rewrite (f=0.01) -- tunnel_encap_process_outer
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at tunnel_encap_process_inner's out edge tunnel_encap_process_inner -- inner_ipv6_udp_rewrite (f=0.01) -- tunnel_encap_process_outer
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at tunnel_encap_process_inner's out edge tunnel_encap_process_inner -- inner_ipv6_tcp_rewrite (f=0.01) -- tunnel_encap_process_outer
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at tunnel_encap_process_inner's out edge tunnel_encap_process_inner -- inner_ipv6_icmp_rewrite (f=0.01) -- tunnel_encap_process_outer
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at tunnel_encap_process_inner's out edge tunnel_encap_process_inner -- inner_ipv6_unknown_rewrite (f=0.01) -- tunnel_encap_process_outer
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at tunnel_encap_process_inner's out edge tunnel_encap_process_inner -- inner_non_ip_rewrite (f=0.01) -- tunnel_encap_process_outer
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at tunnel_encap_process_inner's out edge tunnel_encap_process_inner -- --default-- (f=0.01) -- tunnel_encap_process_outer
      reaches node where usage is ['unused', 'W'], so no need to init
Group dominators for tunnel_metadata.tunnel_smac_index (common parents): 1
  egress_outer_bd_map
Squashed group dominators for tunnel_metadata.tunnel_smac_index (common parents): 1
  egress_outer_bd_map
Ascend group dominators for tunnel_metadata.tunnel_smac_index: 1
  tunnel_encap_process_inner
Squashed ascend group dominators for tunnel_metadata.tunnel_smac_index: 1
  tunnel_encap_process_inner

Delayed init points (experimental): 0
New squashed group dominators for tunnel_metadata.tunnel_smac_index (experimental): 0
Table to Inbound edges to initialize field tunnel_metadata.tunnel_smac_index: 1
  tunnel_encap_process_inner
    _condition_65 -- False (f=0.06) -- tunnel_encap_process_inner
    egress_vni -- set_egress_tunnel_vni (f=0.03) -- tunnel_encap_process_inner
    egress_vni -- --default-- (f=0.03) -- tunnel_encap_process_inner
--------
Init at table for field tunnel_metadata.tunnel_smac_index: 1
  tunnel_encap_process_inner
Init at table at specific edge for field tunnel_metadata.tunnel_smac_index: 0

Init at table: 1
  tunnel_encap_process_inner
  Delaying initialization of field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] from action inner_ipv4_udp_rewrite in table tunnel_encap_process_inner to initial usage tables.
  Add primitive to write 0 for field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] in action nop of table egress_outer_bd_map with 0.
  Field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] is already written in action set_egress_outer_bd_properties of table egress_outer_bd_map.
  Delaying initialization of field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] from action inner_ipv4_tcp_rewrite in table tunnel_encap_process_inner to initial usage tables.
  Add primitive to write 0 for field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] in action nop of table egress_outer_bd_map with 0.
  Field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] is already written in action set_egress_outer_bd_properties of table egress_outer_bd_map.
  Delaying initialization of field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] from action inner_ipv4_icmp_rewrite in table tunnel_encap_process_inner to initial usage tables.
  Add primitive to write 0 for field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] in action nop of table egress_outer_bd_map with 0.
  Field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] is already written in action set_egress_outer_bd_properties of table egress_outer_bd_map.
  Delaying initialization of field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] from action inner_ipv4_unknown_rewrite in table tunnel_encap_process_inner to initial usage tables.
  Add primitive to write 0 for field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] in action nop of table egress_outer_bd_map with 0.
  Field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] is already written in action set_egress_outer_bd_properties of table egress_outer_bd_map.
  Delaying initialization of field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] from action inner_ipv6_udp_rewrite in table tunnel_encap_process_inner to initial usage tables.
  Add primitive to write 0 for field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] in action nop of table egress_outer_bd_map with 0.
  Field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] is already written in action set_egress_outer_bd_properties of table egress_outer_bd_map.
  Delaying initialization of field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] from action inner_ipv6_tcp_rewrite in table tunnel_encap_process_inner to initial usage tables.
  Add primitive to write 0 for field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] in action nop of table egress_outer_bd_map with 0.
  Field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] is already written in action set_egress_outer_bd_properties of table egress_outer_bd_map.
  Delaying initialization of field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] from action inner_ipv6_icmp_rewrite in table tunnel_encap_process_inner to initial usage tables.
  Add primitive to write 0 for field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] in action nop of table egress_outer_bd_map with 0.
  Field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] is already written in action set_egress_outer_bd_properties of table egress_outer_bd_map.
  Delaying initialization of field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] from action inner_ipv6_unknown_rewrite in table tunnel_encap_process_inner to initial usage tables.
  Add primitive to write 0 for field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] in action nop of table egress_outer_bd_map with 0.
  Field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] is already written in action set_egress_outer_bd_properties of table egress_outer_bd_map.
  Delaying initialization of field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] from action inner_non_ip_rewrite in table tunnel_encap_process_inner to initial usage tables.
  Add primitive to write 0 for field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] in action nop of table egress_outer_bd_map with 0.
  Field tunnel_metadata.tunnel_smac_index[7:0] in phv[7:0] is already written in action set_egress_outer_bd_properties of table egress_outer_bd_map.
>>Adding instruction: clear tbl=egress_outer_bd_map action=nop, phv102[7:0] for field tunnel_metadata.tunnel_smac_index[7:0]
>>Adding instruction: clear tbl=egress_outer_bd_map action=nop, phv102[7:0] for field tunnel_metadata.tunnel_smac_index[7:0]
>>Adding instruction: clear tbl=egress_outer_bd_map action=nop, phv102[7:0] for field tunnel_metadata.tunnel_smac_index[7:0]
>>Adding instruction: clear tbl=egress_outer_bd_map action=nop, phv102[7:0] for field tunnel_metadata.tunnel_smac_index[7:0]
>>Adding instruction: clear tbl=egress_outer_bd_map action=nop, phv102[7:0] for field tunnel_metadata.tunnel_smac_index[7:0]
>>Adding instruction: clear tbl=egress_outer_bd_map action=nop, phv102[7:0] for field tunnel_metadata.tunnel_smac_index[7:0]
>>Adding instruction: clear tbl=egress_outer_bd_map action=nop, phv102[7:0] for field tunnel_metadata.tunnel_smac_index[7:0]
>>Adding instruction: clear tbl=egress_outer_bd_map action=nop, phv102[7:0] for field tunnel_metadata.tunnel_smac_index[7:0]
>>Adding instruction: clear tbl=egress_outer_bd_map action=nop, phv102[7:0] for field tunnel_metadata.tunnel_smac_index[7:0]

--------------------------------------------
Liveness ranges for container phv105
  phv105[7:0] = l3_metadata.lkp_ip_proto[7:0] -- -1 to 6
  phv105[7:7] = ig_intr_md_for_tm.copy_to_cpu[0:0] -- 9 to 12
Initialization may be required for:
  ig_intr_md_for_tm.copy_to_cpu[0:0] -- 9 to 12

Checking if ig_intr_md_for_tm.copy_to_cpu needs initialization.  Non exclusive with fields ['l3_metadata.lkp_ip_proto'].
Looking at dominator: __sink__
  looking at non excl field l3_metadata.lkp_ip_proto
   leaf usage compute_ipv6_hashes and R
      __sink__ cannot reach compute_ipv6_hashes
   leaf usage compute_ipv4_hashes and R
      __sink__ cannot reach compute_ipv4_hashes
   leaf usage adjust_lkp_fields and W
      __sink__ cannot reach adjust_lkp_fields
   leaf usage tunnel_lookup_miss and W
      __sink__ cannot reach tunnel_lookup_miss
   leaf usage ipv6_acl and R
      __sink__ cannot reach ipv6_acl
   leaf usage ip_acl and R
      __sink__ cannot reach ip_acl
   leaf usage ipv6_racl and R
      __sink__ cannot reach ipv6_racl
   leaf usage ipv4_racl and R
      __sink__ cannot reach ipv4_racl
   leaf usage None and W
  Candidate dominator: __sink__ because not above last usage of previous field
Looking at dominator: _condition_44
  looking at non excl field l3_metadata.lkp_ip_proto
   leaf usage compute_ipv6_hashes and R
      _condition_44 cannot reach compute_ipv6_hashes
   leaf usage compute_ipv4_hashes and R
      _condition_44 cannot reach compute_ipv4_hashes
   leaf usage adjust_lkp_fields and W
      _condition_44 cannot reach adjust_lkp_fields
   leaf usage tunnel_lookup_miss and W
      _condition_44 cannot reach tunnel_lookup_miss
   leaf usage ipv6_acl and R
      _condition_44 cannot reach ipv6_acl
   leaf usage ip_acl and R
      _condition_44 cannot reach ip_acl
   leaf usage ipv6_racl and R
      _condition_44 cannot reach ipv6_racl
   leaf usage ipv4_racl and R
      _condition_44 cannot reach ipv4_racl
   leaf usage None and W
  Candidate dominator: _condition_44 because not above last usage of previous field
Looking at dominator: system_acl
  looking at non excl field l3_metadata.lkp_ip_proto
   leaf usage compute_ipv6_hashes and R
      system_acl cannot reach compute_ipv6_hashes
   leaf usage compute_ipv4_hashes and R
      system_acl cannot reach compute_ipv4_hashes
   leaf usage adjust_lkp_fields and W
      system_acl cannot reach adjust_lkp_fields
   leaf usage tunnel_lookup_miss and W
      system_acl cannot reach tunnel_lookup_miss
   leaf usage ipv6_acl and R
      system_acl cannot reach ipv6_acl
   leaf usage ip_acl and R
      system_acl cannot reach ip_acl
   leaf usage ipv6_racl and R
      system_acl cannot reach ipv6_racl
   leaf usage ipv4_racl and R
      system_acl cannot reach ipv4_racl
   leaf usage None and W
  Candidate dominator: system_acl because not above last usage of previous field
Looking at dominators_reach_previous_usage: __sink__
Looking at dominators_reach_previous_usage: _condition_44
Looking at dominators_reach_previous_usage: system_acl
------
Initial usages for ig_intr_md_for_tm.copy_to_cpu.
  Deparser: R
  system_acl: W
Dominators for ig_intr_md_for_tm.copy_to_cpu: 3
  __sink__
  _condition_44
  system_acl
Trimmed dominators for ig_intr_md_for_tm.copy_to_cpu (those that are not dominated by other dominators): 1
  _condition_44 can reach [] and who can reach me []
fi = ig_intr_md_for_tm.copy_to_cpu (ingress), fi_earliest_use = 9
  squashed group dominator _condition_44 in stage 9
    Possible nodes to do clearing: (10)
      _condition_44 (stage 9)
      racl_stats (stage 9)
      _condition_43 (stage 9)
      _condition_41 (stage 9)
      _condition_39 (stage 8)
      _condition_36 (stage 7)
      storm_control_stats (stage 7)
      acl_stats (stage 7)
      ingress_bd_stats (stage 7)
      compute_other_hashes (stage 7)
previous stages: 6
   _condition_39  (stage 8)
   _condition_36  (stage 7)
   storm_control_stats  (stage 7)
   acl_stats  (stage 7)
   ingress_bd_stats  (stage 7)
   compute_other_hashes  (stage 7)
current_stage stages: 4
   _condition_44  (stage 9)
   racl_stats  (stage 9)
   _condition_43  (stage 9)
   _condition_41  (stage 9)
Call to _delay_init(_condition_39)
  _condition_39 is condition
    looking at _condition_39's out edge _condition_39 -- False (f=0.50) -- nexthop
Call to _delay_init(nexthop)
  nexthop is match
    looking at nexthop's out edge nexthop -- nop (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- nop (f=0.07) -- _condition_41
    looking at nexthop's out edge nexthop -- set_nexthop_details (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- set_nexthop_details (f=0.07) -- _condition_41
    looking at nexthop's out edge nexthop -- set_nexthop_details_with_tunnel (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- set_nexthop_details_with_tunnel (f=0.07) -- _condition_41
    looking at nexthop's out edge nexthop -- set_nexthop_details_for_post_routed_flood (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- set_nexthop_details_for_post_routed_flood (f=0.07) -- _condition_41
    looking at nexthop's out edge nexthop -- set_nexthop_details_for_glean (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- set_nexthop_details_for_glean (f=0.07) -- _condition_41
    looking at nexthop's out edge nexthop -- set_nexthop_details_for_drop (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- set_nexthop_details_for_drop (f=0.07) -- _condition_41
    looking at nexthop's out edge nexthop -- --default-- (f=0.07) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 nexthop -- --default-- (f=0.07) -- _condition_41
      tbl _condition_39 can descend on edge edge _condition_39 -- False (f=0.50) -- nexthop
        _condition_41 nexthop -- nop (f=0.07) -- _condition_41
        _condition_41 nexthop -- set_nexthop_details (f=0.07) -- _condition_41
        _condition_41 nexthop -- set_nexthop_details_with_tunnel (f=0.07) -- _condition_41
        _condition_41 nexthop -- set_nexthop_details_for_post_routed_flood (f=0.07) -- _condition_41
        _condition_41 nexthop -- set_nexthop_details_for_glean (f=0.07) -- _condition_41
        _condition_41 nexthop -- set_nexthop_details_for_drop (f=0.07) -- _condition_41
        _condition_41 nexthop -- --default-- (f=0.07) -- _condition_41
    looking at _condition_39's out edge _condition_39 -- True (f=0.50) -- ecmp_group
Call to _delay_init(ecmp_group)
  ecmp_group is match
    looking at ecmp_group's out edge ecmp_group -- nop (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 ecmp_group -- nop (f=0.10) -- _condition_41
    looking at ecmp_group's out edge ecmp_group -- set_ecmp_nexthop_details (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 ecmp_group -- set_ecmp_nexthop_details (f=0.10) -- _condition_41
    looking at ecmp_group's out edge ecmp_group -- set_ecmp_nexthop_details_with_tunnel (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 ecmp_group -- set_ecmp_nexthop_details_with_tunnel (f=0.10) -- _condition_41
    looking at ecmp_group's out edge ecmp_group -- set_ecmp_nexthop_details_for_post_routed_flood (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 ecmp_group -- set_ecmp_nexthop_details_for_post_routed_flood (f=0.10) -- _condition_41
    looking at ecmp_group's out edge ecmp_group -- --default-- (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R', 'W']
        _condition_41 ecmp_group -- --default-- (f=0.10) -- _condition_41
      tbl _condition_39 can descend on edge edge _condition_39 -- True (f=0.50) -- ecmp_group
        _condition_41 ecmp_group -- nop (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- set_ecmp_nexthop_details (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- set_ecmp_nexthop_details_with_tunnel (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- set_ecmp_nexthop_details_for_post_routed_flood (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- --default-- (f=0.10) -- _condition_41
Group dominators for ig_intr_md_for_tm.copy_to_cpu (common parents): 1
  _condition_44
Squashed group dominators for ig_intr_md_for_tm.copy_to_cpu (common parents): 1
  _condition_44
Ascend group dominators for ig_intr_md_for_tm.copy_to_cpu: 1
  _condition_39
Squashed ascend group dominators for ig_intr_md_for_tm.copy_to_cpu: 1
  _condition_39

Delayed init points (experimental): 12
  _condition_41 on edge nexthop -- nop (f=0.07) -- _condition_41
  _condition_41 on edge nexthop -- set_nexthop_details (f=0.07) -- _condition_41
  _condition_41 on edge nexthop -- set_nexthop_details_with_tunnel (f=0.07) -- _condition_41
  _condition_41 on edge nexthop -- set_nexthop_details_for_post_routed_flood (f=0.07) -- _condition_41
  _condition_41 on edge nexthop -- set_nexthop_details_for_glean (f=0.07) -- _condition_41
  _condition_41 on edge nexthop -- set_nexthop_details_for_drop (f=0.07) -- _condition_41
  _condition_41 on edge nexthop -- --default-- (f=0.07) -- _condition_41
  _condition_41 on edge ecmp_group -- nop (f=0.10) -- _condition_41
  _condition_41 on edge ecmp_group -- set_ecmp_nexthop_details (f=0.10) -- _condition_41
  _condition_41 on edge ecmp_group -- set_ecmp_nexthop_details_with_tunnel (f=0.10) -- _condition_41
  _condition_41 on edge ecmp_group -- set_ecmp_nexthop_details_for_post_routed_flood (f=0.10) -- _condition_41
  _condition_41 on edge ecmp_group -- --default-- (f=0.10) -- _condition_41
New squashed group dominators for ig_intr_md_for_tm.copy_to_cpu (experimental): 1
  _condition_41
Table to Inbound edges to initialize field ig_intr_md_for_tm.copy_to_cpu: 1
  _condition_39
    _condition_36 -- False (f=0.50) -- _condition_39
    fwd_result -- nop (f=0.03) -- _condition_39
    fwd_result -- set_l2_redirect (f=0.03) -- _condition_39
    fwd_result -- set_fib_redirect (f=0.03) -- _condition_39
    fwd_result -- set_cpu_redirect (f=0.03) -- _condition_39
    fwd_result -- set_acl_redirect (f=0.03) -- _condition_39
    fwd_result -- set_racl_redirect (f=0.03) -- _condition_39
    fwd_result -- set_rmac_non_ip_drop (f=0.03) -- _condition_39
    fwd_result -- set_multicast_route (f=0.03) -- _condition_39
    fwd_result -- set_multicast_rpf_fail_bridge (f=0.03) -- _condition_39
    fwd_result -- set_multicast_rpf_fail_flood_to_mrouters (f=0.03) -- _condition_39
    fwd_result -- set_multicast_bridge (f=0.03) -- _condition_39
    fwd_result -- set_multicast_miss_flood (f=0.03) -- _condition_39
    fwd_result -- set_multicast_miss_flood_to_mrouters (f=0.03) -- _condition_39
    fwd_result -- set_multicast_drop (f=0.03) -- _condition_39
    fwd_result -- --default-- (f=0.03) -- _condition_39
--------
Init at table for field ig_intr_md_for_tm.copy_to_cpu: 1
  _condition_39
Init at table at specific edge for field ig_intr_md_for_tm.copy_to_cpu: 0

Init at table: 1
  _condition_39
  Since dominator table _condition_39 is a gateway, need to clear ig_intr_md_for_tm.copy_to_cpu in table that precedes or inject a new table to clear the field to be 0.
    Possible nodes to do clearing: (5)
      _condition_36
      storm_control_stats
      acl_stats
      ingress_bd_stats
      compute_other_hashes
    Clear field ig_intr_md_for_tm.copy_to_cpu at table storm_control_stats.
>>Adding instruction: clear tbl=storm_control_stats action=nop, phv105[7:7] for field ig_intr_md_for_tm.copy_to_cpu[0:0]

--------------------------------------------
Liveness ranges for container phv109
  phv109[7:0] = l2_metadata.lkp_mac_da[47:40] -- -1 to 6
  phv109[7:7] = nexthop_metadata.nexthop_glean[0:0] -- 8 to 9
Initialization may be required for:
  nexthop_metadata.nexthop_glean[0:0] -- 8 to 9

Checking if nexthop_metadata.nexthop_glean needs initialization.  Non exclusive with fields ['l2_metadata.lkp_mac_da'].
Looking at dominator: nexthop
  looking at non excl field l2_metadata.lkp_mac_da
   leaf usage compute_non_ip_hashes and R
      nexthop cannot reach compute_non_ip_hashes
   leaf usage rmac and R
      nexthop cannot reach rmac
   leaf usage adjust_lkp_fields and W
      nexthop cannot reach adjust_lkp_fields
   leaf usage tunnel and W
      nexthop cannot reach tunnel
   leaf usage tunnel_lookup_miss and W
      nexthop cannot reach tunnel_lookup_miss
   leaf usage mac_acl and R
      nexthop cannot reach mac_acl
   leaf usage dmac and R
      nexthop cannot reach dmac
   leaf usage validate_packet and R
      nexthop cannot reach validate_packet
   leaf usage None and W
  Candidate dominator: nexthop because not above last usage of previous field
Looking at dominator: system_acl
  looking at non excl field l2_metadata.lkp_mac_da
   leaf usage compute_non_ip_hashes and R
      system_acl cannot reach compute_non_ip_hashes
   leaf usage rmac and R
      system_acl cannot reach rmac
   leaf usage adjust_lkp_fields and W
      system_acl cannot reach adjust_lkp_fields
   leaf usage tunnel and W
      system_acl cannot reach tunnel
   leaf usage tunnel_lookup_miss and W
      system_acl cannot reach tunnel_lookup_miss
   leaf usage mac_acl and R
      system_acl cannot reach mac_acl
   leaf usage dmac and R
      system_acl cannot reach dmac
   leaf usage validate_packet and R
      system_acl cannot reach validate_packet
   leaf usage None and W
  Candidate dominator: system_acl because not above last usage of previous field
Looking at dominator: _condition_45
  looking at non excl field l2_metadata.lkp_mac_da
   leaf usage compute_non_ip_hashes and R
      _condition_45 cannot reach compute_non_ip_hashes
   leaf usage rmac and R
      _condition_45 cannot reach rmac
   leaf usage adjust_lkp_fields and W
      _condition_45 cannot reach adjust_lkp_fields
   leaf usage tunnel and W
      _condition_45 cannot reach tunnel
   leaf usage tunnel_lookup_miss and W
      _condition_45 cannot reach tunnel_lookup_miss
   leaf usage mac_acl and R
      _condition_45 cannot reach mac_acl
   leaf usage dmac and R
      _condition_45 cannot reach dmac
   leaf usage validate_packet and R
      _condition_45 cannot reach validate_packet
   leaf usage None and W
  Candidate dominator: _condition_45 because not above last usage of previous field
Looking at dominators_reach_previous_usage: nexthop
Looking at dominators_reach_previous_usage: system_acl
Looking at dominators_reach_previous_usage: _condition_45
------
Initial usages for nexthop_metadata.nexthop_glean.
  nexthop: W
  system_acl: R
Dominators for nexthop_metadata.nexthop_glean: 3
  nexthop
  system_acl
  _condition_45
Trimmed dominators for nexthop_metadata.nexthop_glean (those that are not dominated by other dominators): 2
  nexthop can reach ['_condition_45'] and who can reach me []
  _condition_45 can reach [] and who can reach me ['nexthop']
fi = nexthop_metadata.nexthop_glean (ingress), fi_earliest_use = 8
  squashed group dominator _condition_39 in stage 8
    Possible nodes to do clearing: (6)
      _condition_39 (stage 8)
      _condition_36 (stage 7)
      storm_control_stats (stage 7)
      acl_stats (stage 7)
      ingress_bd_stats (stage 7)
      compute_other_hashes (stage 7)
previous stages: 5
   _condition_36  (stage 7)
   storm_control_stats  (stage 7)
   acl_stats  (stage 7)
   ingress_bd_stats  (stage 7)
   compute_other_hashes  (stage 7)
current_stage stages: 1
   _condition_39  (stage 8)
Call to _delay_init(_condition_36)
  _condition_36 is condition
    looking at _condition_36's out edge _condition_36 -- False (f=0.50) -- _condition_39
Call to _delay_init(_condition_39)
  _condition_39 is condition
    looking at _condition_39's out edge _condition_39 -- False (f=0.50) -- nexthop
Call to _delay_init(nexthop)
  my usage is W, so init here
      tbl _condition_39 can descend on edge edge _condition_39 -- False (f=0.50) -- nexthop
        nexthop ---all---
    looking at _condition_39's out edge _condition_39 -- True (f=0.50) -- ecmp_group
Call to _delay_init(ecmp_group)
  ecmp_group is match
    looking at ecmp_group's out edge ecmp_group -- nop (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R']
        _condition_41 ecmp_group -- nop (f=0.10) -- _condition_41
    looking at ecmp_group's out edge ecmp_group -- set_ecmp_nexthop_details (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R']
        _condition_41 ecmp_group -- set_ecmp_nexthop_details (f=0.10) -- _condition_41
    looking at ecmp_group's out edge ecmp_group -- set_ecmp_nexthop_details_with_tunnel (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R']
        _condition_41 ecmp_group -- set_ecmp_nexthop_details_with_tunnel (f=0.10) -- _condition_41
    looking at ecmp_group's out edge ecmp_group -- set_ecmp_nexthop_details_for_post_routed_flood (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R']
        _condition_41 ecmp_group -- set_ecmp_nexthop_details_for_post_routed_flood (f=0.10) -- _condition_41
    looking at ecmp_group's out edge ecmp_group -- --default-- (f=0.10) -- _condition_41
      reaches node where usage is ['unused', 'R']
        _condition_41 ecmp_group -- --default-- (f=0.10) -- _condition_41
      tbl _condition_39 can descend on edge edge _condition_39 -- True (f=0.50) -- ecmp_group
        _condition_41 ecmp_group -- nop (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- set_ecmp_nexthop_details (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- set_ecmp_nexthop_details_with_tunnel (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- set_ecmp_nexthop_details_for_post_routed_flood (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- --default-- (f=0.10) -- _condition_41
      tbl _condition_36 can descend on edge edge _condition_36 -- False (f=0.50) -- _condition_39
        nexthop ---all---
        _condition_41 ecmp_group -- nop (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- set_ecmp_nexthop_details (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- set_ecmp_nexthop_details_with_tunnel (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- set_ecmp_nexthop_details_for_post_routed_flood (f=0.10) -- _condition_41
        _condition_41 ecmp_group -- --default-- (f=0.10) -- _condition_41
    looking at _condition_36's out edge _condition_36 -- True (f=0.50) -- fwd_result
Call to _delay_init(fwd_result)
  fwd_result is match
    looking at fwd_result's out edge fwd_result -- nop (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W', 'R']
        _condition_39 fwd_result -- nop (f=0.03) -- _condition_39
    looking at fwd_result's out edge fwd_result -- set_l2_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W', 'R']
        _condition_39 fwd_result -- set_l2_redirect (f=0.03) -- _condition_39
    looking at fwd_result's out edge fwd_result -- set_fib_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W', 'R']
        _condition_39 fwd_result -- set_fib_redirect (f=0.03) -- _condition_39
    looking at fwd_result's out edge fwd_result -- set_cpu_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W', 'R']
        _condition_39 fwd_result -- set_cpu_redirect (f=0.03) -- _condition_39
    looking at fwd_result's out edge fwd_result -- set_acl_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W', 'R']
        _condition_39 fwd_result -- set_acl_redirect (f=0.03) -- _condition_39
    looking at fwd_result's out edge fwd_result -- set_racl_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W', 'R']
        _condition_39 fwd_result -- set_racl_redirect (f=0.03) -- _condition_39
    looking at fwd_result's out edge fwd_result -- set_rmac_non_ip_drop (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W', 'R']
        _condition_39 fwd_result -- set_rmac_non_ip_drop (f=0.03) -- _condition_39
    looking at fwd_result's out edge fwd_result -- set_multicast_route (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W', 'R']
        _condition_39 fwd_result -- set_multicast_route (f=0.03) -- _condition_39
    looking at fwd_result's out edge fwd_result -- set_multicast_rpf_fail_bridge (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W', 'R']
        _condition_39 fwd_result -- set_multicast_rpf_fail_bridge (f=0.03) -- _condition_39
    looking at fwd_result's out edge fwd_result -- set_multicast_rpf_fail_flood_to_mrouters (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W', 'R']
        _condition_39 fwd_result -- set_multicast_rpf_fail_flood_to_mrouters (f=0.03) -- _condition_39
    looking at fwd_result's out edge fwd_result -- set_multicast_bridge (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W', 'R']
        _condition_39 fwd_result -- set_multicast_bridge (f=0.03) -- _condition_39
    looking at fwd_result's out edge fwd_result -- set_multicast_miss_flood (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W', 'R']
        _condition_39 fwd_result -- set_multicast_miss_flood (f=0.03) -- _condition_39
    looking at fwd_result's out edge fwd_result -- set_multicast_miss_flood_to_mrouters (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W', 'R']
        _condition_39 fwd_result -- set_multicast_miss_flood_to_mrouters (f=0.03) -- _condition_39
    looking at fwd_result's out edge fwd_result -- set_multicast_drop (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W', 'R']
        _condition_39 fwd_result -- set_multicast_drop (f=0.03) -- _condition_39
    looking at fwd_result's out edge fwd_result -- --default-- (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W', 'R']
        _condition_39 fwd_result -- --default-- (f=0.03) -- _condition_39
      tbl _condition_36 can descend on edge edge _condition_36 -- True (f=0.50) -- fwd_result
        _condition_39 fwd_result -- nop (f=0.03) -- _condition_39
        _condition_39 fwd_result -- set_l2_redirect (f=0.03) -- _condition_39
        _condition_39 fwd_result -- set_fib_redirect (f=0.03) -- _condition_39
        _condition_39 fwd_result -- set_cpu_redirect (f=0.03) -- _condition_39
        _condition_39 fwd_result -- set_acl_redirect (f=0.03) -- _condition_39
        _condition_39 fwd_result -- set_racl_redirect (f=0.03) -- _condition_39
        _condition_39 fwd_result -- set_rmac_non_ip_drop (f=0.03) -- _condition_39
        _condition_39 fwd_result -- set_multicast_route (f=0.03) -- _condition_39
        _condition_39 fwd_result -- set_multicast_rpf_fail_bridge (f=0.03) -- _condition_39
        _condition_39 fwd_result -- set_multicast_rpf_fail_flood_to_mrouters (f=0.03) -- _condition_39
        _condition_39 fwd_result -- set_multicast_bridge (f=0.03) -- _condition_39
        _condition_39 fwd_result -- set_multicast_miss_flood (f=0.03) -- _condition_39
        _condition_39 fwd_result -- set_multicast_miss_flood_to_mrouters (f=0.03) -- _condition_39
        _condition_39 fwd_result -- set_multicast_drop (f=0.03) -- _condition_39
        _condition_39 fwd_result -- --default-- (f=0.03) -- _condition_39
Group dominators for nexthop_metadata.nexthop_glean (common parents): 1
  _condition_39
Squashed group dominators for nexthop_metadata.nexthop_glean (common parents): 1
  _condition_39
Ascend group dominators for nexthop_metadata.nexthop_glean: 1
  _condition_36
Squashed ascend group dominators for nexthop_metadata.nexthop_glean: 1
  _condition_36

Delayed init points (experimental): 21
  nexthop on edge ---all---
  _condition_41 on edge ecmp_group -- nop (f=0.10) -- _condition_41
  _condition_41 on edge ecmp_group -- set_ecmp_nexthop_details (f=0.10) -- _condition_41
  _condition_41 on edge ecmp_group -- set_ecmp_nexthop_details_with_tunnel (f=0.10) -- _condition_41
  _condition_41 on edge ecmp_group -- set_ecmp_nexthop_details_for_post_routed_flood (f=0.10) -- _condition_41
  _condition_41 on edge ecmp_group -- --default-- (f=0.10) -- _condition_41
  _condition_39 on edge fwd_result -- nop (f=0.03) -- _condition_39
  _condition_39 on edge fwd_result -- set_l2_redirect (f=0.03) -- _condition_39
  _condition_39 on edge fwd_result -- set_fib_redirect (f=0.03) -- _condition_39
  _condition_39 on edge fwd_result -- set_cpu_redirect (f=0.03) -- _condition_39
  _condition_39 on edge fwd_result -- set_acl_redirect (f=0.03) -- _condition_39
  _condition_39 on edge fwd_result -- set_racl_redirect (f=0.03) -- _condition_39
  _condition_39 on edge fwd_result -- set_rmac_non_ip_drop (f=0.03) -- _condition_39
  _condition_39 on edge fwd_result -- set_multicast_route (f=0.03) -- _condition_39
  _condition_39 on edge fwd_result -- set_multicast_rpf_fail_bridge (f=0.03) -- _condition_39
  _condition_39 on edge fwd_result -- set_multicast_rpf_fail_flood_to_mrouters (f=0.03) -- _condition_39
  _condition_39 on edge fwd_result -- set_multicast_bridge (f=0.03) -- _condition_39
  _condition_39 on edge fwd_result -- set_multicast_miss_flood (f=0.03) -- _condition_39
  _condition_39 on edge fwd_result -- set_multicast_miss_flood_to_mrouters (f=0.03) -- _condition_39
  _condition_39 on edge fwd_result -- set_multicast_drop (f=0.03) -- _condition_39
  _condition_39 on edge fwd_result -- --default-- (f=0.03) -- _condition_39
New squashed group dominators for nexthop_metadata.nexthop_glean (experimental): 3
  nexthop
  _condition_41
  _condition_39
Table to Inbound edges to initialize field nexthop_metadata.nexthop_glean: 1
  _condition_36
    storm_control_stats -- nop (f=0.50) -- _condition_36
    storm_control_stats -- --default-- (f=0.50) -- _condition_36
--------
Init at table for field nexthop_metadata.nexthop_glean: 1
  _condition_36
Init at table at specific edge for field nexthop_metadata.nexthop_glean: 0

Init at table: 1
  _condition_36
  Since dominator table _condition_36 is a gateway, need to clear nexthop_metadata.nexthop_glean in table that precedes or inject a new table to clear the field to be 0.
    Possible nodes to do clearing: (4)
      storm_control_stats
      acl_stats
      ingress_bd_stats
      compute_other_hashes
    Clear field nexthop_metadata.nexthop_glean at table storm_control_stats.
>>Adding instruction: clear tbl=storm_control_stats action=nop, phv109[7:7] for field nexthop_metadata.nexthop_glean[0:0]

--------------------------------------------
Liveness ranges for container phv121
  phv121[7:0] = l3_metadata.lkp_tcp_flags[7:0] -- -1 to 5
  phv121[7:7] = ig_intr_md_for_tm.disable_ucast_cutthru[0:0] -- 8 to 12
Initialization may be required for:
  ig_intr_md_for_tm.disable_ucast_cutthru[0:0] -- 8 to 12

Checking if ig_intr_md_for_tm.disable_ucast_cutthru needs initialization.  Non exclusive with fields ['l3_metadata.lkp_tcp_flags'].
Looking at dominator: nexthop
  looking at non excl field l3_metadata.lkp_tcp_flags
   leaf usage adjust_lkp_fields and W
      nexthop cannot reach adjust_lkp_fields
   leaf usage tunnel_lookup_miss and W
      nexthop cannot reach tunnel_lookup_miss
   leaf usage ipv6_acl and R
      nexthop cannot reach ipv6_acl
   leaf usage ip_acl and R
      nexthop cannot reach ip_acl
   leaf usage ipv6_racl and R
      nexthop cannot reach ipv6_racl
   leaf usage ipv4_racl and R
      nexthop cannot reach ipv4_racl
   leaf usage None and W
  Candidate dominator: nexthop because not above last usage of previous field
Looking at dominator: ecmp_group
  looking at non excl field l3_metadata.lkp_tcp_flags
   leaf usage adjust_lkp_fields and W
      ecmp_group cannot reach adjust_lkp_fields
   leaf usage tunnel_lookup_miss and W
      ecmp_group cannot reach tunnel_lookup_miss
   leaf usage ipv6_acl and R
      ecmp_group cannot reach ipv6_acl
   leaf usage ip_acl and R
      ecmp_group cannot reach ip_acl
   leaf usage ipv6_racl and R
      ecmp_group cannot reach ipv6_racl
   leaf usage ipv4_racl and R
      ecmp_group cannot reach ipv4_racl
   leaf usage None and W
  Candidate dominator: ecmp_group because not above last usage of previous field
Looking at dominators_reach_previous_usage: nexthop
Looking at dominators_reach_previous_usage: ecmp_group
------
Initial usages for ig_intr_md_for_tm.disable_ucast_cutthru.
  nexthop: W
  ecmp_group: W
Dominators for ig_intr_md_for_tm.disable_ucast_cutthru: 2
  nexthop
  ecmp_group
Trimmed dominators for ig_intr_md_for_tm.disable_ucast_cutthru (those that are not dominated by other dominators): 2
  nexthop can reach [] and who can reach me []
  ecmp_group can reach [] and who can reach me []
fi = ig_intr_md_for_tm.disable_ucast_cutthru (ingress), fi_earliest_use = 8
  squashed group dominator nexthop in stage 8
    Possible nodes to do clearing: (8)
      nexthop (stage 8)
      _condition_39 (stage 8)
      _condition_36 (stage 7)
      storm_control_stats (stage 7)
      acl_stats (stage 7)
      ingress_bd_stats (stage 7)
      compute_other_hashes (stage 7)
      _condition_34 (stage 6)
previous stages: 6
   _condition_36  (stage 7)
   storm_control_stats  (stage 7)
   acl_stats  (stage 7)
   ingress_bd_stats  (stage 7)
   compute_other_hashes  (stage 7)
   _condition_34  (stage 6)
current_stage stages: 2
   nexthop  (stage 8)
   _condition_39  (stage 8)
  squashed group dominator ecmp_group in stage 8
    Possible nodes to do clearing: (8)
      ecmp_group (stage 8)
      _condition_39 (stage 8)
      _condition_36 (stage 7)
      storm_control_stats (stage 7)
      acl_stats (stage 7)
      ingress_bd_stats (stage 7)
      compute_other_hashes (stage 7)
      _condition_34 (stage 6)
previous stages: 6
   _condition_36  (stage 7)
   storm_control_stats  (stage 7)
   acl_stats  (stage 7)
   ingress_bd_stats  (stage 7)
   compute_other_hashes  (stage 7)
   _condition_34  (stage 6)
current_stage stages: 2
   ecmp_group  (stage 8)
   _condition_39  (stage 8)
Call to _delay_init(_condition_36)
  _condition_36 is condition
    looking at _condition_36's out edge _condition_36 -- False (f=0.50) -- _condition_39
Call to _delay_init(_condition_39)
  _condition_39 is condition
    looking at _condition_39's out edge _condition_39 -- False (f=0.50) -- nexthop
Call to _delay_init(nexthop)
  my usage is W, so init here
      tbl _condition_39 can descend on edge edge _condition_39 -- False (f=0.50) -- nexthop
        nexthop ---all---
    looking at _condition_39's out edge _condition_39 -- True (f=0.50) -- ecmp_group
Call to _delay_init(ecmp_group)
  my usage is W, so init here
      tbl _condition_39 can descend on edge edge _condition_39 -- True (f=0.50) -- ecmp_group
        ecmp_group ---all---
      tbl _condition_36 can descend on edge edge _condition_36 -- False (f=0.50) -- _condition_39
        nexthop ---all---
        ecmp_group ---all---
    looking at _condition_36's out edge _condition_36 -- True (f=0.50) -- fwd_result
Call to _delay_init(fwd_result)
  fwd_result is match
    looking at fwd_result's out edge fwd_result -- nop (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_l2_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_fib_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_cpu_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_acl_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_racl_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_rmac_non_ip_drop (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_route (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_rpf_fail_bridge (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_rpf_fail_flood_to_mrouters (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_bridge (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_miss_flood (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_miss_flood_to_mrouters (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_drop (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- --default-- (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
      tbl _condition_36 can descend on edge edge _condition_36 -- True (f=0.50) -- fwd_result
Group dominators for ig_intr_md_for_tm.disable_ucast_cutthru (common parents): 2
  nexthop
  ecmp_group
Squashed group dominators for ig_intr_md_for_tm.disable_ucast_cutthru (common parents): 2
  nexthop
  ecmp_group
Ascend group dominators for ig_intr_md_for_tm.disable_ucast_cutthru: 1
  _condition_36
Squashed ascend group dominators for ig_intr_md_for_tm.disable_ucast_cutthru: 1
  _condition_36

Delayed init points (experimental): 2
  nexthop on edge ---all---
  ecmp_group on edge ---all---
New squashed group dominators for ig_intr_md_for_tm.disable_ucast_cutthru (experimental): 2
  nexthop
  ecmp_group
Table to Inbound edges to initialize field ig_intr_md_for_tm.disable_ucast_cutthru: 1
  _condition_36
    storm_control_stats -- nop (f=0.50) -- _condition_36
    storm_control_stats -- --default-- (f=0.50) -- _condition_36
--------
Init at table for field ig_intr_md_for_tm.disable_ucast_cutthru: 1
  _condition_36
Init at table at specific edge for field ig_intr_md_for_tm.disable_ucast_cutthru: 0

Init at table: 1
  _condition_36
  Since dominator table _condition_36 is a gateway, need to clear ig_intr_md_for_tm.disable_ucast_cutthru in table that precedes or inject a new table to clear the field to be 0.
    Possible nodes to do clearing: (5)
      storm_control_stats
      acl_stats
      ingress_bd_stats
      compute_other_hashes
      _condition_34
    Clear field ig_intr_md_for_tm.disable_ucast_cutthru at table storm_control_stats.
>>Adding instruction: clear tbl=storm_control_stats action=nop, phv121[7:7] for field ig_intr_md_for_tm.disable_ucast_cutthru[0:0]

--------------------------------------------
Liveness ranges for container phv124
  phv124[7:0] = tunnel_metadata.tunnel_vni[23:16] -- -1 to 2
  phv124[7:7] = l3_metadata.urpf_hit[0:0] -- 5 to 6
Initialization may be required for:
  l3_metadata.urpf_hit[0:0] -- 5 to 6

Checking if l3_metadata.urpf_hit needs initialization.  Non exclusive with fields ['tunnel_metadata.tunnel_vni'].
Looking at dominator: _condition_27
  looking at non excl field tunnel_metadata.tunnel_vni
   leaf usage tunnel and R
      _condition_27 cannot reach tunnel
   leaf usage ipv6_dest_vtep and W
      _condition_27 cannot reach ipv6_dest_vtep
   leaf usage ipv4_dest_vtep and W
      _condition_27 cannot reach ipv4_dest_vtep
   leaf usage validate_mpls_packet and W
      _condition_27 cannot reach validate_mpls_packet
   leaf usage None and W
  Candidate dominator: _condition_27 because not above last usage of previous field
Looking at dominator: _condition_23
  looking at non excl field tunnel_metadata.tunnel_vni
   leaf usage tunnel and R
      _condition_23 cannot reach tunnel
   leaf usage ipv6_dest_vtep and W
      _condition_23 cannot reach ipv6_dest_vtep
   leaf usage ipv4_dest_vtep and W
      _condition_23 cannot reach ipv4_dest_vtep
   leaf usage validate_mpls_packet and W
      _condition_23 cannot reach validate_mpls_packet
   leaf usage None and W
  Candidate dominator: _condition_23 because not above last usage of previous field
Looking at dominator: ipv4_urpf
  looking at non excl field tunnel_metadata.tunnel_vni
   leaf usage tunnel and R
      ipv4_urpf cannot reach tunnel
   leaf usage ipv6_dest_vtep and W
      ipv4_urpf cannot reach ipv6_dest_vtep
   leaf usage ipv4_dest_vtep and W
      ipv4_urpf cannot reach ipv4_dest_vtep
   leaf usage validate_mpls_packet and W
      ipv4_urpf cannot reach validate_mpls_packet
   leaf usage None and W
  Candidate dominator: ipv4_urpf because not above last usage of previous field
Looking at dominator: ipv6_urpf
  looking at non excl field tunnel_metadata.tunnel_vni
   leaf usage tunnel and R
      ipv6_urpf cannot reach tunnel
   leaf usage ipv6_dest_vtep and W
      ipv6_urpf cannot reach ipv6_dest_vtep
   leaf usage ipv4_dest_vtep and W
      ipv6_urpf cannot reach ipv4_dest_vtep
   leaf usage validate_mpls_packet and W
      ipv6_urpf cannot reach validate_mpls_packet
   leaf usage None and W
  Candidate dominator: ipv6_urpf because not above last usage of previous field
Looking at dominators_reach_previous_usage: _condition_27
Looking at dominators_reach_previous_usage: _condition_23
Looking at dominators_reach_previous_usage: ipv4_urpf
Looking at dominators_reach_previous_usage: ipv6_urpf
------
Initial usages for l3_metadata.urpf_hit.
  _condition_27: R
  ipv4_urpf: W
  ipv6_urpf: W
Dominators for l3_metadata.urpf_hit: 4
  _condition_27
  _condition_23
  ipv4_urpf
  ipv6_urpf
Trimmed dominators for l3_metadata.urpf_hit (those that are not dominated by other dominators): 1
  _condition_23 can reach [] and who can reach me []
fi = l3_metadata.urpf_hit (ingress), fi_earliest_use = 5
  squashed group dominator _condition_23 in stage 5
    Possible nodes to do clearing: (12)
      _condition_23 (stage 5)
      _condition_22 (stage 4)
      rmac (stage 4)
      _condition_21 (stage 4)
      _condition_16 (stage 3)
      _condition_15 (stage 3)
      _condition_14 (stage 3)
      ingress_l4_dst_port (stage 3)
      ingress_l4_src_port (stage 3)
      _condition_13 (stage 3)
      _condition_12 (stage 3)
      _condition_11 (stage 2)
previous stages: 11
   _condition_22  (stage 4)
   rmac  (stage 4)
   _condition_21  (stage 4)
   _condition_16  (stage 3)
   _condition_15  (stage 3)
   _condition_14  (stage 3)
   ingress_l4_dst_port  (stage 3)
   ingress_l4_src_port  (stage 3)
   _condition_13  (stage 3)
   _condition_12  (stage 3)
   _condition_11  (stage 2)
current_stage stages: 1
   _condition_23  (stage 5)
Call to _delay_init(_condition_22)
  _condition_22 is condition
    looking at _condition_22's out edge _condition_22 -- False (f=0.04) -- _condition_34
Call to _delay_init(_condition_34)
  can reach other usages of field l3_metadata.urpf_hit
      di_tbl _condition_22 cannot descend on edge edge _condition_22 -- False (f=0.04) -- _condition_34
Group dominators for l3_metadata.urpf_hit (common parents): 1
  _condition_23
Squashed group dominators for l3_metadata.urpf_hit (common parents): 1
  _condition_23
Ascend group dominators for l3_metadata.urpf_hit: 1
  _condition_22
Squashed ascend group dominators for l3_metadata.urpf_hit: 1
  _condition_22

Delayed init points (experimental): 1
  _condition_22 on edge None
New squashed group dominators for l3_metadata.urpf_hit (experimental): 1
  _condition_22
Table to Inbound edges to initialize field l3_metadata.urpf_hit: 1
  _condition_22
    rmac -- rmac_hit (f=0.08) -- _condition_22
--------
Init at table for field l3_metadata.urpf_hit: 1
  _condition_22
Init at table at specific edge for field l3_metadata.urpf_hit: 0

Init at table: 1
  _condition_22
  Since dominator table _condition_22 is a gateway, need to clear l3_metadata.urpf_hit in table that precedes or inject a new table to clear the field to be 0.
    Possible nodes to do clearing: (10)
      rmac
      _condition_21
      _condition_16
      _condition_15
      _condition_14
      ingress_l4_dst_port
      ingress_l4_src_port
      _condition_13
      _condition_12
      _condition_11
    Clear field l3_metadata.urpf_hit at table rmac.
>>Adding instruction: clear tbl=rmac action=rmac_hit, phv124[7:7] for field l3_metadata.urpf_hit[0:0]
>>Adding instruction: clear tbl=rmac action=rmac_miss, phv124[7:7] for field l3_metadata.urpf_hit[0:0]

--------------------------------------------
Liveness ranges for container phv130
  phv130[15:0] = fabric_metadata.reason_code[15:0] -- 3 to 12
  phv130[15:0] = ig_intr_md.ingress_mac_tstamp[47:32] -- -1 to 0
Initialization may be required for:
  fabric_metadata.reason_code[15:0] -- 3 to 12

Checking if fabric_metadata.reason_code needs initialization.  Non exclusive with fields ['ig_intr_md.ingress_mac_tstamp'].
Looking at dominator: mac_acl
  looking at non excl field ig_intr_md.ingress_mac_tstamp
   leaf usage switch_config_params and R
      mac_acl cannot reach switch_config_params
  Candidate dominator: mac_acl because not above last usage of previous field
Looking at dominator: ip_acl
  looking at non excl field ig_intr_md.ingress_mac_tstamp
   leaf usage switch_config_params and R
      ip_acl cannot reach switch_config_params
  Candidate dominator: ip_acl because not above last usage of previous field
Looking at dominator: ipv6_acl
  looking at non excl field ig_intr_md.ingress_mac_tstamp
   leaf usage switch_config_params and R
      ipv6_acl cannot reach switch_config_params
  Candidate dominator: ipv6_acl because not above last usage of previous field
Looking at dominator: __sink__
  looking at non excl field ig_intr_md.ingress_mac_tstamp
   leaf usage switch_config_params and R
      __sink__ cannot reach switch_config_params
  Candidate dominator: __sink__ because not above last usage of previous field
Looking at dominator: _condition_44
  looking at non excl field ig_intr_md.ingress_mac_tstamp
   leaf usage switch_config_params and R
      _condition_44 cannot reach switch_config_params
  Candidate dominator: _condition_44 because not above last usage of previous field
Looking at dominator: system_acl
  looking at non excl field ig_intr_md.ingress_mac_tstamp
   leaf usage switch_config_params and R
      system_acl cannot reach switch_config_params
  Candidate dominator: system_acl because not above last usage of previous field
Looking at dominator: _condition_45
  looking at non excl field ig_intr_md.ingress_mac_tstamp
   leaf usage switch_config_params and R
      _condition_45 cannot reach switch_config_params
  Candidate dominator: _condition_45 because not above last usage of previous field
Looking at dominators_reach_previous_usage: mac_acl
Looking at dominators_reach_previous_usage: ip_acl
Looking at dominators_reach_previous_usage: ipv6_acl
Looking at dominators_reach_previous_usage: __sink__
Looking at dominators_reach_previous_usage: _condition_44
Looking at dominators_reach_previous_usage: system_acl
Looking at dominators_reach_previous_usage: _condition_45
------
Initial usages for fabric_metadata.reason_code.
  mac_acl: W
  ip_acl: W
  ipv6_acl: W
  Deparser: R
  system_acl: RW
Dominators for fabric_metadata.reason_code: 7
  mac_acl
  ip_acl
  ipv6_acl
  __sink__
  _condition_44
  system_acl
  _condition_45
Trimmed dominators for fabric_metadata.reason_code (those that are not dominated by other dominators): 4
  mac_acl can reach ['_condition_44'] and who can reach me []
  ip_acl can reach ['_condition_44'] and who can reach me []
  ipv6_acl can reach ['_condition_44'] and who can reach me []
  _condition_44 can reach [] and who can reach me ['mac_acl', 'ip_acl', 'ipv6_acl']
fi = fabric_metadata.reason_code (ingress), fi_earliest_use = 3
  squashed group dominator _condition_12 in stage 3
    Possible nodes to do clearing: (10)
      _condition_12 (stage 3)
      _condition_11 (stage 2)
      _condition_10 (stage 2)
      _condition_7 (stage 1)
      _condition_6 (stage 1)
      _condition_5 (stage 1)
      port_vlan_to_ifindex_mapping (stage 0)
      _condition_4 (stage 0)
      validate_outer_ethernet (stage 0)
      switch_config_params (stage 0)
previous stages: 9
   _condition_11  (stage 2)
   _condition_10  (stage 2)
   _condition_7  (stage 1)
   _condition_6  (stage 1)
   _condition_5  (stage 1)
   port_vlan_to_ifindex_mapping  (stage 0)
   _condition_4  (stage 0)
   validate_outer_ethernet  (stage 0)
   switch_config_params  (stage 0)
current_stage stages: 1
   _condition_12  (stage 3)
Call to _delay_init(_condition_11)
  _condition_11 is condition
    looking at _condition_11's out edge _condition_11 -- False (f=0.50) -- _condition_12
Call to _delay_init(_condition_12)
  _condition_12 is condition
    looking at _condition_12's out edge _condition_12 -- False (f=0.50) -- _condition_34
Call to _delay_init(_condition_34)
  can reach other usages of field fabric_metadata.reason_code
      di_tbl _condition_12 cannot descend on edge edge _condition_12 -- False (f=0.50) -- _condition_34
      di_tbl _condition_11 cannot descend on edge edge _condition_11 -- False (f=0.50) -- _condition_12
Group dominators for fabric_metadata.reason_code (common parents): 1
  _condition_12
Squashed group dominators for fabric_metadata.reason_code (common parents): 1
  _condition_12
Ascend group dominators for fabric_metadata.reason_code: 1
  _condition_11
Squashed ascend group dominators for fabric_metadata.reason_code: 1
  _condition_11

Delayed init points (experimental): 1
  _condition_11 on edge None
New squashed group dominators for fabric_metadata.reason_code (experimental): 1
  _condition_11
Table to Inbound edges to initialize field fabric_metadata.reason_code: 1
  _condition_11
    adjust_lkp_fields -- non_ip_lkp (f=0.12) -- _condition_11
    adjust_lkp_fields -- ipv4_lkp (f=0.12) -- _condition_11
    adjust_lkp_fields -- ipv6_lkp (f=0.12) -- _condition_11
    adjust_lkp_fields -- --default-- (f=0.12) -- _condition_11
    tunnel_check -- nop (f=0.17) -- _condition_11
    tunnel -- nop (f=0.01) -- _condition_11
    tunnel_lookup_miss -- non_ip_lkp (f=0.00) -- _condition_11
    tunnel_lookup_miss -- ipv4_lkp (f=0.00) -- _condition_11
    tunnel_lookup_miss -- ipv6_lkp (f=0.00) -- _condition_11
    tunnel_lookup_miss -- --default-- (f=0.00) -- _condition_11
    tunnel -- terminate_tunnel_inner_non_ip (f=0.01) -- _condition_11
    tunnel -- terminate_tunnel_inner_ethernet_ipv4 (f=0.01) -- _condition_11
    tunnel -- terminate_tunnel_inner_ipv4 (f=0.01) -- _condition_11
    tunnel -- terminate_tunnel_inner_ethernet_ipv6 (f=0.01) -- _condition_11
    tunnel -- terminate_tunnel_inner_ipv6 (f=0.01) -- _condition_11
    tunnel -- terminate_eompls (f=0.01) -- _condition_11
    tunnel -- terminate_vpls (f=0.01) -- _condition_11
    tunnel -- terminate_ipv4_over_mpls (f=0.01) -- _condition_11
    tunnel -- terminate_ipv6_over_mpls (f=0.01) -- _condition_11
    tunnel -- terminate_pw (f=0.01) -- _condition_11
    tunnel -- forward_mpls (f=0.01) -- _condition_11
    tunnel -- --default-- (f=0.01) -- _condition_11
    tunnel_check -- --default-- (f=0.17) -- _condition_11
--------
Init at table for field fabric_metadata.reason_code: 1
  _condition_11
Init at table at specific edge for field fabric_metadata.reason_code: 0

Init at table: 1
  _condition_11
  Since dominator table _condition_11 is a gateway, need to clear fabric_metadata.reason_code in table that precedes or inject a new table to clear the field to be 0.
    Possible nodes to do clearing: (8)
      _condition_10
      _condition_7
      _condition_6
      _condition_5
      port_vlan_to_ifindex_mapping
      _condition_4
      validate_outer_ethernet
      switch_config_params
    Clear field fabric_metadata.reason_code at table port_vlan_to_ifindex_mapping.
>>Adding instruction: clear tbl=port_vlan_to_ifindex_mapping action=set_ingress_interface_properties, phv130[15:0] for field fabric_metadata.reason_code[15:0]
>>Adding instruction: clear tbl=port_vlan_to_ifindex_mapping action=nop, phv130[15:0] for field fabric_metadata.reason_code[15:0]

--------------------------------------------
Liveness ranges for container phv135
  phv135[15:0] = hash_metadata.entropy_hash[15:0] -- 7 to 12
  phv135[15:0] = ig_intr_md_from_parser_aux.ingress_parser_err[15:0] -- -1 to 0
Initialization may be required for:
  hash_metadata.entropy_hash[15:0] -- 7 to 12

Checking if hash_metadata.entropy_hash needs initialization.  Non exclusive with fields ['ig_intr_md_from_parser_aux.ingress_parser_err'].
Looking at dominator: compute_other_hashes
  looking at non excl field ig_intr_md_from_parser_aux.ingress_parser_err
   leaf usage validate_outer_ipv4_packet and R
      compute_other_hashes cannot reach validate_outer_ipv4_packet
   leaf usage None and W
  Candidate dominator: compute_other_hashes because not above last usage of previous field
Looking at dominators_reach_previous_usage: compute_other_hashes
------
Initial usages for hash_metadata.entropy_hash.
  compute_other_hashes: W
Dominators for hash_metadata.entropy_hash: 1
  compute_other_hashes
Trimmed dominators for hash_metadata.entropy_hash (those that are not dominated by other dominators): 1
  compute_other_hashes can reach [] and who can reach me []
fi = hash_metadata.entropy_hash (ingress), fi_earliest_use = 7
  squashed group dominator compute_other_hashes in stage 7
    Possible nodes to do clearing: (10)
      compute_other_hashes (stage 7)
      _condition_34 (stage 6)
      _condition_12 (stage 3)
      _condition_11 (stage 2)
      _condition_10 (stage 2)
      _condition_7 (stage 1)
      _condition_6 (stage 1)
      _condition_5 (stage 1)
      port_vlan_to_ifindex_mapping (stage 0)
      _condition_4 (stage 0)
previous stages: 9
   _condition_34  (stage 6)
   _condition_12  (stage 3)
   _condition_11  (stage 2)
   _condition_10  (stage 2)
   _condition_7  (stage 1)
   _condition_6  (stage 1)
   _condition_5  (stage 1)
   port_vlan_to_ifindex_mapping  (stage 0)
   _condition_4  (stage 0)
current_stage stages: 1
   compute_other_hashes  (stage 7)
Call to _delay_init(_condition_34)
  _condition_34 is condition
    looking at _condition_34's out edge _condition_34 -- False (f=0.50) -- _condition_35
Call to _delay_init(_condition_35)
  _condition_35 is condition
    looking at _condition_35's out edge _condition_35 -- False (f=0.25) -- compute_non_ip_hashes
Call to _delay_init(compute_non_ip_hashes)
  compute_non_ip_hashes is match
    looking at compute_non_ip_hashes's out edge compute_non_ip_hashes -- compute_lkp_non_ip_hash (f=0.12) -- compute_other_hashes
       right usage is W, so no need to init on this edge
    looking at compute_non_ip_hashes's out edge compute_non_ip_hashes -- --default-- (f=0.12) -- compute_other_hashes
       right usage is W, so no need to init on this edge
      tbl _condition_35 can descend on edge edge _condition_35 -- False (f=0.25) -- compute_non_ip_hashes
        compute_other_hashes ---all---
    looking at _condition_35's out edge _condition_35 -- True (f=0.25) -- compute_ipv6_hashes
Call to _delay_init(compute_ipv6_hashes)
  compute_ipv6_hashes is match
    looking at compute_ipv6_hashes's out edge compute_ipv6_hashes -- compute_lkp_ipv6_hash (f=0.12) -- compute_other_hashes
       right usage is W, so no need to init on this edge
    looking at compute_ipv6_hashes's out edge compute_ipv6_hashes -- --default-- (f=0.12) -- compute_other_hashes
       right usage is W, so no need to init on this edge
      tbl _condition_35 can descend on edge edge _condition_35 -- True (f=0.25) -- compute_ipv6_hashes
        compute_other_hashes ---all---
      tbl _condition_34 can descend on edge edge _condition_34 -- False (f=0.50) -- _condition_35
        compute_other_hashes ---all---
    looking at _condition_34's out edge _condition_34 -- True (f=0.50) -- compute_ipv4_hashes
Call to _delay_init(compute_ipv4_hashes)
  compute_ipv4_hashes is match
    looking at compute_ipv4_hashes's out edge compute_ipv4_hashes -- compute_lkp_ipv4_hash (f=0.25) -- compute_other_hashes
       right usage is W, so no need to init on this edge
    looking at compute_ipv4_hashes's out edge compute_ipv4_hashes -- --default-- (f=0.25) -- compute_other_hashes
       right usage is W, so no need to init on this edge
      tbl _condition_34 can descend on edge edge _condition_34 -- True (f=0.50) -- compute_ipv4_hashes
        compute_other_hashes ---all---
Group dominators for hash_metadata.entropy_hash (common parents): 1
  compute_other_hashes
Squashed group dominators for hash_metadata.entropy_hash (common parents): 1
  compute_other_hashes
Ascend group dominators for hash_metadata.entropy_hash: 1
  _condition_34
Squashed ascend group dominators for hash_metadata.entropy_hash: 1
  _condition_34

Delayed init points (experimental): 1
  compute_other_hashes on edge ---all---
New squashed group dominators for hash_metadata.entropy_hash (experimental): 1
  compute_other_hashes
Table to Inbound edges to initialize field hash_metadata.entropy_hash: 1
  _condition_34
    _condition_12 -- False (f=0.50) -- _condition_34
    _condition_31 -- False (f=0.06) -- _condition_34
    _condition_33 -- False (f=0.03) -- _condition_34
    ipv6_multicast_route_star_g -- multicast_route_star_g_miss (f=0.00) -- _condition_34
    ipv6_multicast_route_star_g -- multicast_route_sm_star_g_hit (f=0.00) -- _condition_34
    ipv6_multicast_route_star_g -- multicast_route_bidir_star_g_hit (f=0.00) -- _condition_34
    ipv6_multicast_route_star_g -- --default-- (f=0.00) -- _condition_34
    ipv6_multicast_route -- multicast_route_s_g_hit (f=0.01) -- _condition_34
    ipv6_multicast_route -- --default-- (f=0.01) -- _condition_34
    _condition_30 -- False (f=0.06) -- _condition_34
    ipv4_multicast_route_star_g -- multicast_route_star_g_miss (f=0.01) -- _condition_34
    ipv4_multicast_route_star_g -- multicast_route_sm_star_g_hit (f=0.01) -- _condition_34
    ipv4_multicast_route_star_g -- multicast_route_bidir_star_g_hit (f=0.01) -- _condition_34
    ipv4_multicast_route_star_g -- --default-- (f=0.01) -- _condition_34
    ipv4_multicast_route -- multicast_route_s_g_hit (f=0.02) -- _condition_34
    ipv4_multicast_route -- --default-- (f=0.02) -- _condition_34
    _condition_22 -- False (f=0.04) -- _condition_34
    _condition_27 -- False (f=0.02) -- _condition_34
    urpf_bd -- nop (f=0.01) -- _condition_34
    urpf_bd -- urpf_bd_miss (f=0.01) -- _condition_34
    urpf_bd -- --default-- (f=0.01) -- _condition_34
    rmac -- rmac_miss (f=0.08) -- _condition_34
    rmac -- --default-- (f=0.08) -- _condition_34
--------
Init at table for field hash_metadata.entropy_hash: 1
  _condition_34
Init at table at specific edge for field hash_metadata.entropy_hash: 0

Init at table: 1
  _condition_34
  Since dominator table _condition_34 is a gateway, need to clear hash_metadata.entropy_hash in table that precedes or inject a new table to clear the field to be 0.
    Possible nodes to do clearing: (8)
      _condition_12
      _condition_11
      _condition_10
      _condition_7
      _condition_6
      _condition_5
      port_vlan_to_ifindex_mapping
      _condition_4
    Clear field hash_metadata.entropy_hash at table port_vlan_to_ifindex_mapping.
>>Adding instruction: clear tbl=port_vlan_to_ifindex_mapping action=set_ingress_interface_properties, phv135[15:0] for field hash_metadata.entropy_hash[15:0]
>>Adding instruction: clear tbl=port_vlan_to_ifindex_mapping action=nop, phv135[15:0] for field hash_metadata.entropy_hash[15:0]

--------------------------------------------
Liveness ranges for container phv140
  phv140[13:0] = tunnel_metadata.vtep_ifindex[13:0] -- 1 to 2
  phv140[15:15] = tunnel_metadata.tunnel_if_check[0:0] -- 8 to 9
  phv140[15:14] = l3_metadata.urpf_mode[1:0] -- 5 to 6
Initialization may be required for:
  tunnel_metadata.tunnel_if_check[0:0] -- 8 to 9

Checking if tunnel_metadata.tunnel_if_check needs initialization.  Non exclusive with fields ['l3_metadata.urpf_mode'].
Looking at dominator: nexthop
  looking at non excl field l3_metadata.urpf_mode
   leaf usage _condition_27 and R
      nexthop cannot reach _condition_27
   leaf usage urpf_bd and R
      nexthop cannot reach urpf_bd
  Candidate dominator: nexthop because not above last usage of previous field
Looking at dominator: ecmp_group
  looking at non excl field l3_metadata.urpf_mode
   leaf usage _condition_27 and R
      ecmp_group cannot reach _condition_27
   leaf usage urpf_bd and R
      ecmp_group cannot reach urpf_bd
  Candidate dominator: ecmp_group because not above last usage of previous field
Looking at dominators_reach_previous_usage: nexthop
Looking at dominators_reach_previous_usage: ecmp_group
------
Initial usages for tunnel_metadata.tunnel_if_check.
  nexthop: W
  ecmp_group: W
Dominators for tunnel_metadata.tunnel_if_check: 2
  nexthop
  ecmp_group
Trimmed dominators for tunnel_metadata.tunnel_if_check (those that are not dominated by other dominators): 2
  nexthop can reach [] and who can reach me []
  ecmp_group can reach [] and who can reach me []
fi = tunnel_metadata.tunnel_if_check (ingress), fi_earliest_use = 8
  squashed group dominator nexthop in stage 8
    Possible nodes to do clearing: (8)
      nexthop (stage 8)
      _condition_39 (stage 8)
      _condition_36 (stage 7)
      storm_control_stats (stage 7)
      acl_stats (stage 7)
      ingress_bd_stats (stage 7)
      compute_other_hashes (stage 7)
      _condition_34 (stage 6)
previous stages: 6
   _condition_36  (stage 7)
   storm_control_stats  (stage 7)
   acl_stats  (stage 7)
   ingress_bd_stats  (stage 7)
   compute_other_hashes  (stage 7)
   _condition_34  (stage 6)
current_stage stages: 2
   nexthop  (stage 8)
   _condition_39  (stage 8)
  squashed group dominator ecmp_group in stage 8
    Possible nodes to do clearing: (8)
      ecmp_group (stage 8)
      _condition_39 (stage 8)
      _condition_36 (stage 7)
      storm_control_stats (stage 7)
      acl_stats (stage 7)
      ingress_bd_stats (stage 7)
      compute_other_hashes (stage 7)
      _condition_34 (stage 6)
previous stages: 6
   _condition_36  (stage 7)
   storm_control_stats  (stage 7)
   acl_stats  (stage 7)
   ingress_bd_stats  (stage 7)
   compute_other_hashes  (stage 7)
   _condition_34  (stage 6)
current_stage stages: 2
   ecmp_group  (stage 8)
   _condition_39  (stage 8)
Call to _delay_init(_condition_36)
  _condition_36 is condition
    looking at _condition_36's out edge _condition_36 -- False (f=0.50) -- _condition_39
Call to _delay_init(_condition_39)
  _condition_39 is condition
    looking at _condition_39's out edge _condition_39 -- False (f=0.50) -- nexthop
Call to _delay_init(nexthop)
  my usage is W, so init here
      tbl _condition_39 can descend on edge edge _condition_39 -- False (f=0.50) -- nexthop
        nexthop ---all---
    looking at _condition_39's out edge _condition_39 -- True (f=0.50) -- ecmp_group
Call to _delay_init(ecmp_group)
  my usage is W, so init here
      tbl _condition_39 can descend on edge edge _condition_39 -- True (f=0.50) -- ecmp_group
        ecmp_group ---all---
      tbl _condition_36 can descend on edge edge _condition_36 -- False (f=0.50) -- _condition_39
        nexthop ---all---
        ecmp_group ---all---
    looking at _condition_36's out edge _condition_36 -- True (f=0.50) -- fwd_result
Call to _delay_init(fwd_result)
  fwd_result is match
    looking at fwd_result's out edge fwd_result -- nop (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_l2_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_fib_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_cpu_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_acl_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_racl_redirect (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_rmac_non_ip_drop (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_route (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_rpf_fail_bridge (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_rpf_fail_flood_to_mrouters (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_bridge (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_miss_flood (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_miss_flood_to_mrouters (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- set_multicast_drop (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at fwd_result's out edge fwd_result -- --default-- (f=0.03) -- _condition_39
      reaches node where usage is ['unused', 'W'], so no need to init
      tbl _condition_36 can descend on edge edge _condition_36 -- True (f=0.50) -- fwd_result
Group dominators for tunnel_metadata.tunnel_if_check (common parents): 2
  nexthop
  ecmp_group
Squashed group dominators for tunnel_metadata.tunnel_if_check (common parents): 2
  nexthop
  ecmp_group
Ascend group dominators for tunnel_metadata.tunnel_if_check: 1
  _condition_36
Squashed ascend group dominators for tunnel_metadata.tunnel_if_check: 1
  _condition_36

Delayed init points (experimental): 2
  nexthop on edge ---all---
  ecmp_group on edge ---all---
New squashed group dominators for tunnel_metadata.tunnel_if_check (experimental): 2
  nexthop
  ecmp_group
Table to Inbound edges to initialize field tunnel_metadata.tunnel_if_check: 1
  _condition_36
    storm_control_stats -- nop (f=0.50) -- _condition_36
    storm_control_stats -- --default-- (f=0.50) -- _condition_36
--------
Init at table for field tunnel_metadata.tunnel_if_check: 1
  _condition_36
Init at table at specific edge for field tunnel_metadata.tunnel_if_check: 0

Init at table: 1
  _condition_36
  Since dominator table _condition_36 is a gateway, need to clear tunnel_metadata.tunnel_if_check in table that precedes or inject a new table to clear the field to be 0.
    Possible nodes to do clearing: (5)
      storm_control_stats
      acl_stats
      ingress_bd_stats
      compute_other_hashes
      _condition_34
    Clear field tunnel_metadata.tunnel_if_check at table storm_control_stats.
>>Adding instruction: clear tbl=storm_control_stats action=nop, phv140[15:15] for field tunnel_metadata.tunnel_if_check[0:0]

--------------------------------------------
Liveness ranges for container phv142
  phv142[15:0] = hash_metadata.hash1[15:0] -- 6 to 8
  phv142[15:6] = l3_metadata.rmac_group[9:0] -- 0 to 4
Initialization may be required for:
  hash_metadata.hash1[15:0] -- 6 to 8

Checking if hash_metadata.hash1 needs initialization.  Non exclusive with fields ['l3_metadata.rmac_group'].
Looking at dominator: compute_ipv4_hashes
  looking at non excl field l3_metadata.rmac_group
   leaf usage rmac and R
      compute_ipv4_hashes cannot reach rmac
   leaf usage tunnel and W
      compute_ipv4_hashes cannot reach tunnel
   leaf usage outer_rmac and R
      compute_ipv4_hashes cannot reach outer_rmac
   leaf usage port_vlan_to_bd_mapping and W
      compute_ipv4_hashes cannot reach port_vlan_to_bd_mapping
   leaf usage cpu_packet_transform and W
      compute_ipv4_hashes cannot reach cpu_packet_transform
  Candidate dominator: compute_ipv4_hashes because not above last usage of previous field
Looking at dominator: compute_non_ip_hashes
  looking at non excl field l3_metadata.rmac_group
   leaf usage rmac and R
      compute_non_ip_hashes cannot reach rmac
   leaf usage tunnel and W
      compute_non_ip_hashes cannot reach tunnel
   leaf usage outer_rmac and R
      compute_non_ip_hashes cannot reach outer_rmac
   leaf usage port_vlan_to_bd_mapping and W
      compute_non_ip_hashes cannot reach port_vlan_to_bd_mapping
   leaf usage cpu_packet_transform and W
      compute_non_ip_hashes cannot reach cpu_packet_transform
  Candidate dominator: compute_non_ip_hashes because not above last usage of previous field
Looking at dominator: compute_ipv6_hashes
  looking at non excl field l3_metadata.rmac_group
   leaf usage rmac and R
      compute_ipv6_hashes cannot reach rmac
   leaf usage tunnel and W
      compute_ipv6_hashes cannot reach tunnel
   leaf usage outer_rmac and R
      compute_ipv6_hashes cannot reach outer_rmac
   leaf usage port_vlan_to_bd_mapping and W
      compute_ipv6_hashes cannot reach port_vlan_to_bd_mapping
   leaf usage cpu_packet_transform and W
      compute_ipv6_hashes cannot reach cpu_packet_transform
  Candidate dominator: compute_ipv6_hashes because not above last usage of previous field
Looking at dominators_reach_previous_usage: compute_ipv4_hashes
Looking at dominators_reach_previous_usage: compute_non_ip_hashes
Looking at dominators_reach_previous_usage: compute_ipv6_hashes
------
Initial usages for hash_metadata.hash1.
  compute_ipv4_hashes: W
  compute_non_ip_hashes: W
  compute_ipv6_hashes: W
Dominators for hash_metadata.hash1: 3
  compute_ipv4_hashes
  compute_non_ip_hashes
  compute_ipv6_hashes
Trimmed dominators for hash_metadata.hash1 (those that are not dominated by other dominators): 3
  compute_ipv4_hashes can reach [] and who can reach me []
  compute_non_ip_hashes can reach [] and who can reach me []
  compute_ipv6_hashes can reach [] and who can reach me []
fi = hash_metadata.hash1 (ingress), fi_earliest_use = 6
  squashed group dominator compute_ipv4_hashes in stage 6
    Possible nodes to do clearing: (2)
      compute_ipv4_hashes (stage 6)
      _condition_34 (stage 6)
previous stages: 0
current_stage stages: 2
   compute_ipv4_hashes  (stage 6)
   _condition_34  (stage 6)
  squashed group dominator compute_non_ip_hashes in stage 6
    Possible nodes to do clearing: (3)
      compute_non_ip_hashes (stage 6)
      _condition_35 (stage 6)
      _condition_34 (stage 6)
previous stages: 0
current_stage stages: 3
   compute_non_ip_hashes  (stage 6)
   _condition_35  (stage 6)
   _condition_34  (stage 6)
  squashed group dominator compute_ipv6_hashes in stage 6
    Possible nodes to do clearing: (3)
      compute_ipv6_hashes (stage 6)
      _condition_35 (stage 6)
      _condition_34 (stage 6)
previous stages: 0
current_stage stages: 3
   compute_ipv6_hashes  (stage 6)
   _condition_35  (stage 6)
   _condition_34  (stage 6)
Call to _delay_init(compute_ipv4_hashes)
  my usage is W, so init here
Call to _delay_init(compute_non_ip_hashes)
  my usage is W, so init here
Call to _delay_init(compute_ipv6_hashes)
  my usage is W, so init here
Group dominators for hash_metadata.hash1 (common parents): 3
  compute_ipv4_hashes
  compute_non_ip_hashes
  compute_ipv6_hashes
Squashed group dominators for hash_metadata.hash1 (common parents): 3
  compute_ipv4_hashes
  compute_non_ip_hashes
  compute_ipv6_hashes
Ascend group dominators for hash_metadata.hash1: 3
  compute_ipv4_hashes
  compute_non_ip_hashes
  compute_ipv6_hashes
Squashed ascend group dominators for hash_metadata.hash1: 3
  compute_ipv4_hashes
  compute_non_ip_hashes
  compute_ipv6_hashes

Delayed init points (experimental): 3
  compute_ipv4_hashes on edge ---all---
  compute_non_ip_hashes on edge ---all---
  compute_ipv6_hashes on edge ---all---
New squashed group dominators for hash_metadata.hash1 (experimental): 3
  compute_ipv4_hashes
  compute_non_ip_hashes
  compute_ipv6_hashes
$$$ initialization at table compute_ipv4_hashes will expand the table layout due to adding a default action.
$$$ initialization at table compute_non_ip_hashes will expand the table layout due to adding a default action.
$$$ initialization at table compute_ipv6_hashes will expand the table layout due to adding a default action.
Table to Inbound edges to initialize field hash_metadata.hash1: 3
  compute_ipv4_hashes
    _condition_34 -- True (f=0.50) -- compute_ipv4_hashes
  compute_non_ip_hashes
    _condition_35 -- False (f=0.25) -- compute_non_ip_hashes
  compute_ipv6_hashes
    _condition_35 -- True (f=0.25) -- compute_ipv6_hashes
--------
Init at table for field hash_metadata.hash1: 3
  compute_ipv4_hashes
  compute_non_ip_hashes
  compute_ipv6_hashes
Init at table at specific edge for field hash_metadata.hash1: 0

Init at table: 3
  compute_ipv4_hashes
  Field hash_metadata.hash1[15:0] in phv[15:0] is already written in action compute_lkp_ipv4_hash of table compute_ipv4_hashes.
  Add primitive to write 0 for field hash_metadata.hash1[15:0] in phv[15:0] in newly created default action of table compute_ipv4_hashes with 0.
  compute_non_ip_hashes
  Field hash_metadata.hash1[15:0] in phv[15:0] is already written in action compute_lkp_non_ip_hash of table compute_non_ip_hashes.
  Add primitive to write 0 for field hash_metadata.hash1[15:0] in phv[15:0] in newly created default action of table compute_non_ip_hashes with 0.
  compute_ipv6_hashes
  Field hash_metadata.hash1[15:0] in phv[15:0] is already written in action compute_lkp_ipv6_hash of table compute_ipv6_hashes.
  Add primitive to write 0 for field hash_metadata.hash1[15:0] in phv[15:0] in newly created default action of table compute_ipv6_hashes with 0.
>>Adding instruction: clear tbl=compute_ipv4_hashes action=--create-default-action--, phv142[15:0] for field hash_metadata.hash1[15:0]
>>Adding instruction: clear tbl=compute_non_ip_hashes action=--create-default-action--, phv142[15:0] for field hash_metadata.hash1[15:0]
>>Adding instruction: clear tbl=compute_ipv6_hashes action=--create-default-action--, phv142[15:0] for field hash_metadata.hash1[15:0]

--------------------------------------------
Liveness ranges for container phv151
  phv151[15:15] = tunnel_metadata.tunnel_terminate[0:0] -- -1 to 2
  phv151[14:14] = l3_metadata.routed[0:0] -- -1 to 3
  phv151[13:0] = l3_metadata.vrf[13:0] -- -1 to 3
  phv151[7:7] = eg_intr_md_for_oport.capture_tstamp_on_tx[0:0] -- 7 to 12
Initialization may be required for:
  eg_intr_md_for_oport.capture_tstamp_on_tx[0:0] -- 7 to 12

Checking if eg_intr_md_for_oport.capture_tstamp_on_tx needs initialization.  Non exclusive with fields ['l3_metadata.vrf'].
Looking at dominator: capture_tstamp
  looking at non excl field l3_metadata.vrf
   leaf usage None and W
   leaf usage rewrite and R
      capture_tstamp cannot reach rewrite
  Candidate dominator: capture_tstamp because not above last usage of previous field
Looking at dominators_reach_previous_usage: capture_tstamp
------
Initial usages for eg_intr_md_for_oport.capture_tstamp_on_tx.
  capture_tstamp: W
Dominators for eg_intr_md_for_oport.capture_tstamp_on_tx: 1
  capture_tstamp
Trimmed dominators for eg_intr_md_for_oport.capture_tstamp_on_tx (those that are not dominated by other dominators): 1
  capture_tstamp can reach [] and who can reach me []
fi = eg_intr_md_for_oport.capture_tstamp_on_tx (egress), fi_earliest_use = 7
  squashed group dominator capture_tstamp in stage 7
    Possible nodes to do clearing: (1)
      capture_tstamp (stage 7)
previous stages: 0
current_stage stages: 1
   capture_tstamp  (stage 7)
Call to _delay_init(capture_tstamp)
  my usage is W, so init here
Group dominators for eg_intr_md_for_oport.capture_tstamp_on_tx (common parents): 1
  capture_tstamp
Squashed group dominators for eg_intr_md_for_oport.capture_tstamp_on_tx (common parents): 1
  capture_tstamp
Ascend group dominators for eg_intr_md_for_oport.capture_tstamp_on_tx: 1
  capture_tstamp
Squashed ascend group dominators for eg_intr_md_for_oport.capture_tstamp_on_tx: 1
  capture_tstamp

Delayed init points (experimental): 1
  capture_tstamp on edge ---all---
New squashed group dominators for eg_intr_md_for_oport.capture_tstamp_on_tx (experimental): 1
  capture_tstamp
Table to Inbound edges to initialize field eg_intr_md_for_oport.capture_tstamp_on_tx: 1
  capture_tstamp
    _condition_49 -- False (f=0.50) -- capture_tstamp
    _condition_50 -- False (f=0.25) -- capture_tstamp
    _condition_66 -- False (f=0.13) -- capture_tstamp
    egress_vlan_xlate -- set_egress_if_params_untagged (f=0.04) -- capture_tstamp
    egress_vlan_xlate -- set_egress_if_params_tagged (f=0.04) -- capture_tstamp
    egress_vlan_xlate -- --default-- (f=0.04) -- capture_tstamp
--------
Init at table for field eg_intr_md_for_oport.capture_tstamp_on_tx: 1
  capture_tstamp
Init at table at specific edge for field eg_intr_md_for_oport.capture_tstamp_on_tx: 0

Init at table: 1
  capture_tstamp
  Field eg_intr_md_for_oport.capture_tstamp_on_tx[0:0] in phv[7:7] is already written in action set_capture_tstamp of table capture_tstamp.

--------------------------------------------
Liveness ranges for container phv156
  phv156[13:0] = egress_metadata.same_bd_check[13:0] -- 0 to 1
  phv156[15:7] = egress_metadata.smac_idx[8:0] -- 4 to 5
Initialization may be required for:
  egress_metadata.smac_idx[8:0] -- 4 to 5

Checking if egress_metadata.smac_idx needs initialization.  Non exclusive with fields ['egress_metadata.same_bd_check'].
Looking at dominator: egress_bd_map
  looking at non excl field egress_metadata.same_bd_check
   leaf usage replica_type and R
      egress_bd_map cannot reach replica_type
   leaf usage rid and W
      egress_bd_map cannot reach rid
  Candidate dominator: egress_bd_map because not above last usage of previous field
Looking at dominators_reach_previous_usage: egress_bd_map
------
Initial usages for egress_metadata.smac_idx.
  egress_bd_map: W
Dominators for egress_metadata.smac_idx: 1
  egress_bd_map
Trimmed dominators for egress_metadata.smac_idx (those that are not dominated by other dominators): 1
  egress_bd_map can reach [] and who can reach me []
fi = egress_metadata.smac_idx (egress), fi_earliest_use = 4
  squashed group dominator egress_bd_map in stage 4
    Possible nodes to do clearing: (3)
      egress_bd_map (stage 4)
      _condition_61 (stage 4)
      egress_port_mapping (stage 2)
previous stages: 1
   egress_port_mapping  (stage 2)
current_stage stages: 2
   egress_bd_map  (stage 4)
   _condition_61  (stage 4)
Call to _delay_init(egress_port_mapping)
  egress_port_mapping is match
    looking at egress_port_mapping's out edge egress_port_mapping -- egress_port_type_cpu (f=0.08) -- _condition_61
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at egress_port_mapping's out edge egress_port_mapping -- egress_port_type_normal (f=0.08) -- _condition_56
      reaches node where usage is ['unused', 'W'], so no need to init
    looking at egress_port_mapping's out edge egress_port_mapping -- --default-- (f=0.08) -- _condition_61
      reaches node where usage is ['unused', 'W'], so no need to init
Group dominators for egress_metadata.smac_idx (common parents): 1
  egress_bd_map
Squashed group dominators for egress_metadata.smac_idx (common parents): 1
  egress_bd_map
Ascend group dominators for egress_metadata.smac_idx: 1
  egress_port_mapping
Squashed ascend group dominators for egress_metadata.smac_idx: 1
  egress_port_mapping

Delayed init points (experimental): 0
New squashed group dominators for egress_metadata.smac_idx (experimental): 0
$$$ initialization at table egress_port_mapping will expand the table layout due to adding a default action.
Table to Inbound edges to initialize field egress_metadata.smac_idx: 1
  egress_port_mapping
    _condition_54 -- False (f=0.13) -- egress_port_mapping
    replica_type -- nop (f=0.04) -- egress_port_mapping
    replica_type -- set_replica_copy_bridged (f=0.04) -- egress_port_mapping
    replica_type -- --default-- (f=0.04) -- egress_port_mapping
--------
Init at table for field egress_metadata.smac_idx: 1
  egress_port_mapping
Init at table at specific edge for field egress_metadata.smac_idx: 0

Init at table: 1
  egress_port_mapping
  Delaying initialization of field egress_metadata.smac_idx[8:0] in phv[15:7] from action egress_port_type_cpu in table egress_port_mapping to initial usage tables.
  Add primitive to write 0 for field egress_metadata.smac_idx[8:0] in phv[15:7] in action nop of table egress_bd_map with 0.
  Field egress_metadata.smac_idx[8:0] in phv[15:7] is already written in action set_egress_bd_properties of table egress_bd_map.
  Delaying initialization of field egress_metadata.smac_idx[8:0] in phv[15:7] from action egress_port_type_normal in table egress_port_mapping to initial usage tables.
  Add primitive to write 0 for field egress_metadata.smac_idx[8:0] in phv[15:7] in action nop of table egress_bd_map with 0.
  Field egress_metadata.smac_idx[8:0] in phv[15:7] is already written in action set_egress_bd_properties of table egress_bd_map.
>>Adding instruction: clear tbl=egress_bd_map action=nop, phv156[15:7] for field egress_metadata.smac_idx[8:0]
>>Adding instruction: clear tbl=egress_bd_map action=nop, phv156[15:7] for field egress_metadata.smac_idx[8:0]

--------------------------------------------
Liveness ranges for container phv175
  phv175[15:0] = tunnel_metadata.tunnel_vni[15:0] -- -1 to 2
  phv175[15:2] = l3_metadata.urpf_bd_group[13:0] -- 5 to 6
Initialization may be required for:
  l3_metadata.urpf_bd_group[13:0] -- 5 to 6

Checking if l3_metadata.urpf_bd_group needs initialization.  Non exclusive with fields ['tunnel_metadata.tunnel_vni'].
Looking at dominator: urpf_bd
  looking at non excl field tunnel_metadata.tunnel_vni
   leaf usage tunnel and R
      urpf_bd cannot reach tunnel
   leaf usage ipv6_dest_vtep and W
      urpf_bd cannot reach ipv6_dest_vtep
   leaf usage ipv4_dest_vtep and W
      urpf_bd cannot reach ipv4_dest_vtep
   leaf usage validate_mpls_packet and W
      urpf_bd cannot reach validate_mpls_packet
   leaf usage None and W
  Candidate dominator: urpf_bd because not above last usage of previous field
Looking at dominator: _condition_27
  looking at non excl field tunnel_metadata.tunnel_vni
   leaf usage tunnel and R
      _condition_27 cannot reach tunnel
   leaf usage ipv6_dest_vtep and W
      _condition_27 cannot reach ipv6_dest_vtep
   leaf usage ipv4_dest_vtep and W
      _condition_27 cannot reach ipv4_dest_vtep
   leaf usage validate_mpls_packet and W
      _condition_27 cannot reach validate_mpls_packet
   leaf usage None and W
  Candidate dominator: _condition_27 because not above last usage of previous field
Looking at dominator: ipv4_urpf
  looking at non excl field tunnel_metadata.tunnel_vni
   leaf usage tunnel and R
      ipv4_urpf cannot reach tunnel
   leaf usage ipv6_dest_vtep and W
      ipv4_urpf cannot reach ipv6_dest_vtep
   leaf usage ipv4_dest_vtep and W
      ipv4_urpf cannot reach ipv4_dest_vtep
   leaf usage validate_mpls_packet and W
      ipv4_urpf cannot reach validate_mpls_packet
   leaf usage None and W
  Candidate dominator: ipv4_urpf because not above last usage of previous field
Looking at dominator: ipv6_urpf
  looking at non excl field tunnel_metadata.tunnel_vni
   leaf usage tunnel and R
      ipv6_urpf cannot reach tunnel
   leaf usage ipv6_dest_vtep and W
      ipv6_urpf cannot reach ipv6_dest_vtep
   leaf usage ipv4_dest_vtep and W
      ipv6_urpf cannot reach ipv4_dest_vtep
   leaf usage validate_mpls_packet and W
      ipv6_urpf cannot reach validate_mpls_packet
   leaf usage None and W
  Candidate dominator: ipv6_urpf because not above last usage of previous field
Looking at dominators_reach_previous_usage: urpf_bd
Looking at dominators_reach_previous_usage: _condition_27
Looking at dominators_reach_previous_usage: ipv4_urpf
Looking at dominators_reach_previous_usage: ipv6_urpf
------
Initial usages for l3_metadata.urpf_bd_group.
  urpf_bd: R
  ipv4_urpf: W
  ipv6_urpf: W
Dominators for l3_metadata.urpf_bd_group: 4
  urpf_bd
  _condition_27
  ipv4_urpf
  ipv6_urpf
Trimmed dominators for l3_metadata.urpf_bd_group (those that are not dominated by other dominators): 3
  _condition_27 can reach [] and who can reach me ['ipv4_urpf', 'ipv6_urpf']
  ipv4_urpf can reach ['_condition_27'] and who can reach me []
  ipv6_urpf can reach ['_condition_27'] and who can reach me []
fi = l3_metadata.urpf_bd_group (ingress), fi_earliest_use = 5
  squashed group dominator _condition_23 in stage 5
    Possible nodes to do clearing: (12)
      _condition_23 (stage 5)
      _condition_22 (stage 4)
      rmac (stage 4)
      _condition_21 (stage 4)
      _condition_16 (stage 3)
      _condition_15 (stage 3)
      _condition_14 (stage 3)
      ingress_l4_dst_port (stage 3)
      ingress_l4_src_port (stage 3)
      _condition_13 (stage 3)
      _condition_12 (stage 3)
      _condition_11 (stage 2)
previous stages: 11
   _condition_22  (stage 4)
   rmac  (stage 4)
   _condition_21  (stage 4)
   _condition_16  (stage 3)
   _condition_15  (stage 3)
   _condition_14  (stage 3)
   ingress_l4_dst_port  (stage 3)
   ingress_l4_src_port  (stage 3)
   _condition_13  (stage 3)
   _condition_12  (stage 3)
   _condition_11  (stage 2)
current_stage stages: 1
   _condition_23  (stage 5)
Call to _delay_init(_condition_22)
  _condition_22 is condition
    looking at _condition_22's out edge _condition_22 -- False (f=0.04) -- _condition_34
Call to _delay_init(_condition_34)
  can reach other usages of field l3_metadata.urpf_bd_group
      di_tbl _condition_22 cannot descend on edge edge _condition_22 -- False (f=0.04) -- _condition_34
Group dominators for l3_metadata.urpf_bd_group (common parents): 1
  _condition_23
Squashed group dominators for l3_metadata.urpf_bd_group (common parents): 1
  _condition_23
Ascend group dominators for l3_metadata.urpf_bd_group: 1
  _condition_22
Squashed ascend group dominators for l3_metadata.urpf_bd_group: 1
  _condition_22

Delayed init points (experimental): 1
  _condition_22 on edge None
New squashed group dominators for l3_metadata.urpf_bd_group (experimental): 1
  _condition_22
Table to Inbound edges to initialize field l3_metadata.urpf_bd_group: 1
  _condition_22
    rmac -- rmac_hit (f=0.08) -- _condition_22
--------
Init at table for field l3_metadata.urpf_bd_group: 1
  _condition_22
Init at table at specific edge for field l3_metadata.urpf_bd_group: 0

Init at table: 1
  _condition_22
  Since dominator table _condition_22 is a gateway, need to clear l3_metadata.urpf_bd_group in table that precedes or inject a new table to clear the field to be 0.
    Possible nodes to do clearing: (10)
      rmac
      _condition_21
      _condition_16
      _condition_15
      _condition_14
      ingress_l4_dst_port
      ingress_l4_src_port
      _condition_13
      _condition_12
      _condition_11
    Clear field l3_metadata.urpf_bd_group at table rmac.
>>Adding instruction: clear tbl=rmac action=rmac_hit, phv175[15:2] for field l3_metadata.urpf_bd_group[13:0]
>>Adding instruction: clear tbl=rmac action=rmac_miss, phv175[15:2] for field l3_metadata.urpf_bd_group[13:0]

+------------------------+

Performing inject metadata initialization instructions: (0)
tbl_name_to_common_edge_groups: 0
all_edge: 0

Performing replace metadata initialization instructions: (0)

Performing remove metadata initialization instructions: (0)

Performing clear metadata initialization instructions: (12)
  table port_vlan_to_ifindex_mapping and action set_ingress_interface_properties
      phv1[15:0] = 0    (__md_ingress.__init_0[15:0])
      phv1[15:0] = 0    (__md_ingress.__init_1[15:0]) for default
      phv130[15:0] = 0    (__md_ingress.__init_2[15:0])
      phv130[15:0] = 0    (__md_ingress.__init_3[15:0]) for default
      phv135[15:0] = 0    (__md_ingress.__init_4[15:0])
      phv135[15:0] = 0    (__md_ingress.__init_5[15:0]) for default
  table port_vlan_to_ifindex_mapping and action nop
      phv1[15:0] = 0    (__md_ingress.__init_6[15:0])
      phv130[15:0] = 0    (__md_ingress.__init_7[15:0])
      phv135[15:0] = 0    (__md_ingress.__init_8[15:0])
  table storm_control_stats and action nop
      phv1[31:16] = 0    (__md_ingress.__init_9[15:0])
      phv1[31:16] = 0    (__md_ingress.__init_10[15:0]) for default
      phv72[7:3] = 0    (__md_ingress.__init_11[4:0])
      phv72[7:3] = 0    (__md_ingress.__init_12[4:0]) for default
      phv73[7:5] = 0    (__md_ingress.__init_13[2:0])
      phv73[7:5] = 0    (__md_ingress.__init_14[2:0]) for default
      phv105[7:7] = 0    (__md_ingress.__init_15[0:0])
      phv105[7:7] = 0    (__md_ingress.__init_16[0:0]) for default
      phv109[7:7] = 0    (__md_ingress.__init_17[0:0])
      phv109[7:7] = 0    (__md_ingress.__init_18[0:0]) for default
      phv121[7:7] = 0    (__md_ingress.__init_19[0:0])
      phv121[7:7] = 0    (__md_ingress.__init_20[0:0]) for default
      phv140[15:15] = 0    (__md_ingress.__init_21[0:0])
      phv140[15:15] = 0    (__md_ingress.__init_22[0:0]) for default
  table egress_port_mapping and action egress_port_type_cpu
      phv95[7:0] = 0    (__md_egress.__init_0[7:0])
      phv95[7:0] = 0    (__md_egress.__init_1[7:0]) for default
  table egress_port_mapping and action egress_port_type_normal
      phv95[7:0] = 0    (__md_egress.__init_2[7:0])
  table egress_outer_bd_map and action nop
      phv102[7:0] = 0    (__md_egress.__init_3[7:0])
      phv102[7:0] = 0    (__md_egress.__init_4[7:0]) for default
  table rmac and action rmac_hit
      phv124[7:7] = 0    (__md_ingress.__init_23[0:0])
      phv124[7:7] = 0    (__md_ingress.__init_24[0:0]) for default
      phv175[15:2] = 0    (__md_ingress.__init_25[13:0])
      phv175[15:2] = 0    (__md_ingress.__init_26[13:0]) for default
  table rmac and action rmac_miss
      phv124[7:7] = 0    (__md_ingress.__init_27[0:0])
      phv175[15:2] = 0    (__md_ingress.__init_28[13:0])
  table compute_ipv4_hashes and action --create-default-action--
      phv142[15:0] = 0    (__md_ingress.__init_29[15:0]) for default
  table compute_non_ip_hashes and action --create-default-action--
      phv142[15:0] = 0    (__md_ingress.__init_30[15:0]) for default
  table compute_ipv6_hashes and action --create-default-action--
      phv142[15:0] = 0    (__md_ingress.__init_31[15:0]) for default
  table egress_bd_map and action nop
      phv156[15:7] = 0    (__md_egress.__init_5[8:0])
      phv156[15:7] = 0    (__md_egress.__init_6[8:0]) for default

Performing invalidate metadata initialization instructions: (0)

 Total overlay containers examined for initialization: 17

-----------------------------------------------
  Checking constraints satisfied
-----------------------------------------------
  No constraints violated.
